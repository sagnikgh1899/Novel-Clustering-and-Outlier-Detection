{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class PriorityQueue:\n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "        self.keys = []\n",
    "\n",
    "    def add(self, key, value):\n",
    "        if len(self.values) == 0:\n",
    "            self.values.append(value)\n",
    "            self.keys.append(key)\n",
    "        else:\n",
    "            index = 0\n",
    "            while index < len(self.values) and key > self.keys[index]:\n",
    "                index += 1\n",
    "\n",
    "            if index == len(self.values):\n",
    "                self.values.append(value)\n",
    "                self.keys.append(key)\n",
    "            else:\n",
    "                self.values.insert(index, value)\n",
    "                self.keys.insert(index, key)\n",
    "\n",
    "    def peek(self):\n",
    "        return self.values[0]\n",
    "\n",
    "    def poll(self):\n",
    "        if len(self.values) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            self.keys.pop()\n",
    "            return self.values.pop(0)\n",
    "\n",
    "\n",
    "def calculate_distance(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise AttributeError('two tuples have mismatching lengths: %d and %d' % (len(a), len(b)))\n",
    "    diffs = [0] * len(a)\n",
    "    for index, (a_i, b_i) in enumerate(zip(a, b)):\n",
    "        diffs[index] = a_i - b_i\n",
    "    return math.sqrt(sum([diff_l ** 2 for diff_l in diffs]) / len(diffs))\n",
    "\n",
    "\n",
    "def gaussian_kernel_function(u, h):\n",
    "    return 1 / (2.5066282746310002 * h * math.exp(u * u / (2 * h * h)))\n",
    "\n",
    "\n",
    "def epanechnikov_kernel_function(u, h):\n",
    "    return 0.75 / h * (1 - u * u / (h * h))\n",
    "\n",
    "\n",
    "def calculate_mean(values):\n",
    "    return sum(values) / len(values)\n",
    "\n",
    "\n",
    "def calculate_z_score(target, values):\n",
    "    mean = calculate_mean(values=values)\n",
    "    std_dev = math.sqrt((1 / len(values)) * sum((value - mean) ** 2 for value in values))\n",
    "    return (target - mean) / std_dev\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 4) 0.0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# from Tools import sigmoid\n",
    "# from Tools import calculate_distance\n",
    "import math\n",
    "\n",
    "# region Global Variables\n",
    "window = 1000  # must be multiple of 4\n",
    "min_pts = 5\n",
    "skip = False\n",
    "half_window = int(window / 2)\n",
    "quarter_window = int(window / 4)\n",
    "dataset = []\n",
    "outliers = []\n",
    "\n",
    "\n",
    "# endregion\n",
    "\n",
    "\n",
    "class LOF:\n",
    "    @staticmethod\n",
    "    def calc_distance_euclidean(_vector1, _vector2):\n",
    "        # !!! all passed vector elements to this method must be float values !!!\n",
    "\n",
    "        # validate comparability\n",
    "        if len(_vector1) != len(_vector2):\n",
    "            raise AttributeError(\"Compared vectors have different number of arguments!\")\n",
    "\n",
    "        # init differences vector\n",
    "        _per_element_distances = [0] * len(_vector1)\n",
    "\n",
    "        # compute (each vector element) difference for RMSE (for euclidean distance)\n",
    "        for index, (_value1, _value2) in enumerate(zip(_vector1, _vector2)):\n",
    "            _per_element_distances[index] = _value1 - _value2\n",
    "    \n",
    "        # compute RMSE (root mean squared error)\n",
    "        return math.sqrt(sum([_val ** 2 for _val in _per_element_distances]) / len(_per_element_distances))\n",
    "    \n",
    "    @staticmethod\n",
    "    def calc_k_distance(_k, _vector, _dataset):\n",
    "        # TODO: consider caching for more efficient re-computation\n",
    "\n",
    "        _distances = {}\n",
    "        for _vector2 in _dataset:\n",
    "            _distance = LOF.calc_distance_euclidean(_vector1=_vector, _vector2=_vector2)\n",
    "            if _distance in _distances:\n",
    "                _distances[_distance].append(_vector2)\n",
    "            else:\n",
    "                _distances[_distance] = [_vector2]\n",
    "\n",
    "        _distances = list(sorted(_distances.items()))\n",
    "        _neighbours = []\n",
    "\n",
    "        for _distance in _distances[:_k]:\n",
    "            _neighbours += _distance[1]  # extract each neighbor\n",
    "\n",
    "        _k_distance = _distances[_k - 1][0] if len(_distances) >= _k else _distances[-1][0]\n",
    "\n",
    "        return _k_distance, _neighbours\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_k_reachability_distance(_k, _vector1, _vector2, _dataset):\n",
    "        (_k_distance, _neighbours) = LOF.calc_k_distance(_k, _vector2, _dataset)\n",
    "        return max(_k_distance, LOF.calc_distance_euclidean(_vector1=_vector1, _vector2=_vector2))\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_local_reachability_density(_k, _vector, _dataset):\n",
    "        (_k_distance, _neighbours) = LOF.calc_k_distance(_k=_k, _vector=_vector, _dataset=_dataset)\n",
    "        _reachability_distances = [0] * len(_neighbours)\n",
    "\n",
    "        for _index, _neighbour in enumerate(_neighbours):\n",
    "            _reachability_distances[_index] = LOF.calc_k_reachability_distance(_k=_k, _vector1=_vector, _vector2=_neighbour, _dataset=_dataset)\n",
    "\n",
    "        if sum(_reachability_distances) == 0:\n",
    "            # TODO: vector is identical with its neighbors, consider fixing this case!\n",
    "            # returning 'inf' to note that this vector has an issue\n",
    "            return float(\"inf\")\n",
    "        else:\n",
    "            return len(_neighbours) / sum(_reachability_distances)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_local_outlier_factor(_k, _vector, _dataset):\n",
    "        (_k_distance, _neighbours) = LOF.calc_k_distance(_k=_k, _vector=_vector, _dataset=_dataset)\n",
    "        _vector_lrd = LOF.calc_local_reachability_density(_k=_k, _vector=_vector, _dataset=_dataset)\n",
    "        _lrd_ratios = list([0] * len(_neighbours))\n",
    "\n",
    "        for _index, _neighbour in enumerate(_neighbours):\n",
    "            _tmp_dataset_without_neighbor = set(_dataset)\n",
    "            _tmp_dataset_without_neighbor.remove(_neighbour)\n",
    "            _neighbours_lrd = LOF.calc_local_reachability_density(_k=_k, _vector=_neighbour, _dataset=_tmp_dataset_without_neighbor)\n",
    "            _lrd_ratios[_index] = _neighbours_lrd / _vector_lrd\n",
    "\n",
    "        return sum(_lrd_ratios) / len(_neighbours)\n",
    "\n",
    "\n",
    "def get_k_distance_decisive(vector, dataset, Y):\n",
    "    D = [0] * half_window\n",
    "    for i in range(half_window):\n",
    "        D[i] = Y[i] * LOF.calc_distance_euclidean(vector, dataset[i])\n",
    "    D.sort()\n",
    "    return D[quarter_window - min_pts]\n",
    "\n",
    "\n",
    "def get_phi(y_n):\n",
    "    if y_n > 1:\n",
    "        return (y_n - 1) ** 2\n",
    "    elif y_n < 0:\n",
    "        y_n ** 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_C(n):\n",
    "    def find_KNN(target, k):\n",
    "        point = [{'point': c, 'distance': 0} for c in dataset]\n",
    "        for i in range(len(point)):\n",
    "            point[i]['distance'] = calculate_distance(a=target, b=point[i]['point'].get_centroid())\n",
    "        point.sort(key=lambda dist_key: dist_key['distance'])\n",
    "        return [point['point'] for point in point[:k]]\n",
    "\n",
    "    N = find_KNN()\n",
    "\n",
    "    S = [0] * len(dataset)\n",
    "    for n in range(len(dataset)):\n",
    "        for q in N[n]:\n",
    "            S[n] += math.exp(sigmoid(LOF(min_pts, q)))\n",
    "    avg_S = sum(S) / len(dataset)\n",
    "    C_indices, C = [], []\n",
    "    for i in range(len(dataset)):\n",
    "        if S[i] > avg_S:\n",
    "            for n in range(len(dataset)):\n",
    "                if get_k_distance_decisive():\n",
    "                    pass\n",
    "    return C_indices, C\n",
    "\n",
    "\n",
    "def get_LOD():\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def get_NDS(X, step, regularization, iterations):\n",
    "    Y = [0.5] * half_window\n",
    "    for iteration in range(iterations):\n",
    "        step *= 0.95\n",
    "        for n in range(half_window):\n",
    "            C_indices, C = get_C(n)\n",
    "            Y[n] = Y[n] - step * (\n",
    "                    sum([Y[i] for i in C_indices]) +\n",
    "                    get_k_distance_decisive(X[n], X, Y) / get_k_distance_decisive(X[n], C, Y) -\n",
    "                    math.exp(LOF(min_pts, X[n])) +  # TODO: Calculating LOF here doesn't make sense, it's just as bad as using classic LOF\n",
    "                    get_phi(Y[n]) +\n",
    "                    regularization * (sum([Y[i] for i in range(half_window)]) - quarter_window)\n",
    "            )\n",
    "    Z = []\n",
    "    for n in range(half_window):\n",
    "        if Y[n] == 1:\n",
    "            Z += [X[n]]\n",
    "    return Z\n",
    "\n",
    "\n",
    "def get_DILOF(_vector, threshold, step, regularization, iterations):\n",
    "    global dataset\n",
    "\n",
    "    lof_value = get_LOD()#_vector, 0, outliers, threshold)\n",
    "    print(_vector, lof_value)\n",
    "\n",
    "    if lof_value > 0:\n",
    "        dataset += [_vector]\n",
    "        if len(dataset) == window:\n",
    "            Z = get_NDS(dataset, step, regularization, iterations)\n",
    "            dataset = Z + dataset[window / 2:]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _vector = (1, 2, 3, 4)\n",
    "    get_DILOF(_vector, 1.0, 1.0, 1.0, 1)\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
