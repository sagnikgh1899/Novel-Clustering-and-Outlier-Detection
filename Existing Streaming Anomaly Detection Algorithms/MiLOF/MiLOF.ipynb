{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#data = np.delete(np.genfromtxt('Corners(F).csv',delimiter=','),obj=0,axis=0)\n",
    "data = np.genfromtxt('Vowel.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time1 = time.time()\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "data = np.delete(np.genfromtxt('optdigits_csv.csv',delimiter=','),obj=0,axis=0)\n",
    "print(len(data),\"\\t\", data.shape[1])\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "distortions = []\n",
    "K = range(1,15)\n",
    "#K = range(1,30)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(data)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "plt.figure()\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil = []\n",
    "#kmax = 10\n",
    "kmax = 15\n",
    "\n",
    "# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "for k in range(2, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    sil.append(silhouette_score(data, labels, metric = 'euclidean'))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(K, sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('The Silhouette Score Method showing the optimal k')\n",
    "plt.show()\n",
    "\n",
    "elapsed1 = time.time() - time1\n",
    "print(elapsed1, \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Weighted K-means Algorithm\n",
    "    \n",
    "from __future__ import division, print_function\n",
    "\n",
    "import random\n",
    "import sklearn.datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from colorama import Style\n",
    "\n",
    "\n",
    "def euclidean(a, b):\n",
    "    return np.linalg.norm(np.asarray(a) - np.asarray(b))\n",
    "\n",
    "\n",
    "class WKMeans():\n",
    "    def counted(f):\n",
    "        def wrapped(*args, **kwargs):\n",
    "            wrapped.calls += 1\n",
    "            return f(*args, **kwargs)\n",
    "        wrapped.calls = 0\n",
    "        return wrapped\n",
    "\n",
    "    def __init__(self, K, X=None, N=0, c=None, alpha=0, beta=0, dist=euclidean,\n",
    "                 max_runs=200, label='My Clustering', verbose=True, mu=None,\n",
    "                 max_diff=0.001):\n",
    "        self.K = K\n",
    "        if X is None:\n",
    "            if N == 0:\n",
    "                raise Exception(\"If no data is provided, \\\n",
    "                                 a parameter N (number of points) is needed\")\n",
    "            else:\n",
    "                self.N = N\n",
    "                self.X = self._init_gauss(N)\n",
    "        else:\n",
    "            self.X = X\n",
    "            self.N = len(X)\n",
    "        self.mu = mu\n",
    "        self.old_mu = None\n",
    "        if self.mu is None:\n",
    "            self.method = 'random'\n",
    "        else:\n",
    "            self.method = 'manual'\n",
    "        self.clusters = None\n",
    "        self.cluster_indices = np.asarray([None for i in self.X])\n",
    "        self.alpha = alpha\n",
    "        self.scaling_factor = np.ones((self.K)) / self.K\n",
    "        self.beta = beta\n",
    "        if c is None:\n",
    "            self.counts_per_data_point = [1 for x in self.X]\n",
    "        else:\n",
    "            self.counts_per_data_point = c\n",
    "        self.counts_per_cluster = [0 for x in range(self.K)]\n",
    "        self.max_runs = max_runs\n",
    "        self.runs = 0\n",
    "        self.dist = dist\n",
    "        self.max_diff = max_diff\n",
    "        self.label = label\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _init_gauss(self, N):\n",
    "        centers = [[0, 0], [1, 0], [0.5, np.sqrt(0.75)]]\n",
    "        cluster_std = [0.3, 0.3, 0.3]\n",
    "        n_samples = int(np.ceil(0.75 * N))\n",
    "\n",
    "        data, labels_true = \\\n",
    "            sklearn.datasets.make_blobs(n_samples=n_samples,\n",
    "                                        centers=centers,\n",
    "                                        cluster_std=cluster_std)\n",
    "\n",
    "        centers = [[0.5, np.sqrt(0.75)]]\n",
    "        cluster_std = [0.3]\n",
    "        # n_clusters = len(centers)\n",
    "        extra, labels_true = \\\n",
    "            sklearn.datasets.make_blobs(n_samples=int(0.25 * N),\n",
    "                                        centers=centers,\n",
    "                                        cluster_std=cluster_std)\n",
    "\n",
    "        data = np.concatenate((data, extra), axis=0)\n",
    "        return data\n",
    "\n",
    "    @counted\n",
    "    def plot_clusters(self, snapshot=0):\n",
    "        X = self.X\n",
    "        # fig = plt.figure(figsize=(5, 5))\n",
    "        # ax = plt.gca()\n",
    "        # palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "        if self.mu and self.clusters:\n",
    "            mu = self.mu\n",
    "            clus = self.clusters\n",
    "            K = self.K\n",
    "            for m, clu in enumerate(clus):\n",
    "                cs = cm.spectral(1. * m / K)\n",
    "                plt.plot(list(zip(*clus[m]))[0], list(zip(*clus[m]))[1], '.',\n",
    "                         markersize=8, color=cs, alpha=0.5)\n",
    "                plt.plot(mu[m][0], mu[m][1], 'o', marker='*',\n",
    "                         markersize=12, color=cs,  markeredgecolor='white',\n",
    "                         markeredgewidth=1.0)\n",
    "        else:\n",
    "            plt.plot(list(zip(*X))[0], list(zip(*X))[1], '.', alpha=0.5)\n",
    "\n",
    "        if self.method == '++':\n",
    "            title = 'K-means++'\n",
    "        elif self.method == 'random':\n",
    "            title = 'K-means with random initialization'\n",
    "        elif self.method == 'seed':\n",
    "            title = 'K-means seeded with pre-loaded clusters'\n",
    "        pars = r'$N=%s, K=%s, \\alpha=%s$' % (str(self.N), str(self.K),\n",
    "                                             str(self.alpha))\n",
    "        plt.title('\\n'.join([pars, title]), fontsize=16)\n",
    "\n",
    "        plt.savefig('kpp_N_%i_K_%i_alpha_%i_beta_%i_%i.png' % (self.N, self.K,\n",
    "                                                               self.alpha,\n",
    "                                                               self.beta,\n",
    "                                                               snapshot),\n",
    "                    bbox_inches='tight',\n",
    "                    dpi=200)\n",
    "\n",
    "    @counted\n",
    "    def _cluster_points(self):\n",
    "        clusters = [[] for i in range(self.K)]\n",
    "        counts_per_cluster = [0 for i in range(self.K)]\n",
    "\n",
    "        for index, x in enumerate(self.X):\n",
    "            bestmukey = min([(i[0],\n",
    "                              self.scaling_factor[i[0]] *\n",
    "                              self.dist(x, self.mu[i[0]]))\n",
    "                             for i in enumerate(self.mu)],\n",
    "                            key=lambda t: t[1])[0]\n",
    "            clusters[bestmukey].append(x)\n",
    "            counts_per_cluster[bestmukey] += self.counts_per_data_point[index]\n",
    "            self.cluster_indices[index] = bestmukey\n",
    "\n",
    "        clusters = [c for c in clusters if len(c)]\n",
    "        scaling_factor = np.asarray([self.scaling_factor[i] for i in range(self.K) if counts_per_cluster[i]])\n",
    "        counts_per_cluster = [num for num in counts_per_cluster if num]  \n",
    "        self.clusters = clusters\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.counts_per_cluster = counts_per_cluster\n",
    "        self.K = len(self.clusters)\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\tNumber of unique items per cluster: ' + Style.BRIGHT,\n",
    "                  end='')\n",
    "            print([len(x) for x in self.clusters], end='')\n",
    "            print(Style.RESET_ALL)\n",
    "            print('\\tNumber of items per cluster: ' + Style.BRIGHT,  end='')\n",
    "            for i, c in enumerate(self.counts_per_cluster):\n",
    "                if i == 0:\n",
    "                    print('[', end='')\n",
    "                print('%1.1f' % c, end='')\n",
    "                if i == len(self.counts_per_cluster) - 1:\n",
    "                    print(']', end='')\n",
    "                else:\n",
    "                    print(', ', end='')\n",
    "            print(Style.RESET_ALL)\n",
    "\n",
    "        scaling_factor = np.asarray([self.counts_per_cluster[index]**self.alpha\n",
    "                                     for index, cluster in\n",
    "                                     enumerate(self.clusters)])\n",
    "\n",
    "        scaling_factor = scaling_factor / np.sum(scaling_factor)\n",
    "\n",
    "        scaling_factor = (1 - self.beta) * scaling_factor + (self.beta) * self.scaling_factor\n",
    "\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\tScaling factors per cluster: ' + Style.BRIGHT,  end='')\n",
    "            for i, c in enumerate(self.scaling_factor):\n",
    "                if i == 0:\n",
    "                    print('[', end='')\n",
    "                print('%1.3f' % c, end='')\n",
    "                if i == len(self.scaling_factor) - 1:\n",
    "                    print(']', end='')\n",
    "                else:\n",
    "                    print(', ', end='')\n",
    "            print(Style.RESET_ALL)\n",
    "\n",
    "    def _reevaluate_centers(self):\n",
    "        new_mu = []\n",
    "        for k in self.clusters:\n",
    "            new_mu.append(np.mean(k, axis=0))\n",
    "\n",
    "        self.mu = new_mu\n",
    "\n",
    "    def _has_converged(self):\n",
    "        diff = 1000\n",
    "        if self.clusters:            \n",
    "            for clu in self.clusters:\n",
    "                if len(clu) is 0:\n",
    "                    raise ValueError('One or more clusters disappeared because'\n",
    "                                     ' all points rushed away to other'\n",
    "                                     ' cluster(s). Try increasing the'\n",
    "                                     ' stickiness parameter (beta).')\n",
    "            diff = 0\n",
    "            for i in range(self.K):\n",
    "                diff += self.dist(self.mu[i].tolist(), self.old_mu[i].tolist())\n",
    "            diff /= self.K\n",
    "\n",
    "            if self.verbose:\n",
    "                print('\\tDistance between previous and current centroids: ' +\n",
    "                      Style.BRIGHT + str(diff) + Style.RESET_ALL)\n",
    "\n",
    "        return diff < self.max_diff\n",
    "\n",
    "    def find_centers(self, method='random'):\n",
    "        self.method = method\n",
    "        X = self.X\n",
    "        K = self.K\n",
    "        self.old_mu = random.sample(list(X), K)\n",
    "\n",
    "        if method == 'random':\n",
    "            self.mu = random.sample(X, K)\n",
    "\n",
    "        while not self._has_converged() and self.runs < self.max_runs:\n",
    "            if self.verbose:\n",
    "                print(Style.BRIGHT + '\\nRun: ' + str(self.runs) + ', alpha: ' +\n",
    "                      str(self.alpha) + ', beta: ' +\n",
    "                      str(self.beta) + ', label: '\n",
    "                      + self.label + Style.RESET_ALL)\n",
    "            self.old_mu = self.mu\n",
    "            self._cluster_points()\n",
    "            self._reevaluate_centers()\n",
    "            self.runs += 1\n",
    "\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(Style.BRIGHT + '\\nThe End!' + Style.RESET_ALL)\n",
    "            print('\\tLabel: ' + Style.BRIGHT + self.label + Style.RESET_ALL)\n",
    "            print('\\tTotal runs:' + Style.BRIGHT, self.runs, Style.RESET_ALL)\n",
    "            print('\\tNumber of unique items per cluster: ' + Style.BRIGHT, end='')\n",
    "            print([len(x) for x in self.clusters], end='')\n",
    "            print(Style.RESET_ALL)\n",
    "            print('\\tNumber of items per cluster: ' + Style.BRIGHT,  end='')\n",
    "            for i, c in enumerate(self.counts_per_cluster):\n",
    "                if i == 0:\n",
    "                    print('[', end='')\n",
    "                print('%1.1f' % c, end='')\n",
    "                if i == len(self.counts_per_cluster) - 1:\n",
    "                    print(']', end='')\n",
    "                else:\n",
    "                    print(', ', end='')\n",
    "            print(Style.RESET_ALL)\n",
    "\n",
    "\n",
    "class KPlusPlus(WKMeans):\n",
    "    \n",
    "    def _dist_from_centers(self):\n",
    "        cent = self.mu\n",
    "        X = self.X\n",
    "        D2 = np.array([min([self.dist(x, c)**2 for c in cent]) for x in X])\n",
    "        self.D2 = D2\n",
    "\n",
    "    def _choose_next_center(self):\n",
    "        self.probs = self.D2 / self.D2.sum()\n",
    "        self.cumprobs = self.probs.cumsum()\n",
    "        r = random.random()\n",
    "        ind = np.where(self.cumprobs >= r)[0][0]\n",
    "        return(self.X[ind])\n",
    "\n",
    "    def init_centers(self):\n",
    "        self.mu = random.sample(list(self.X), 1)\n",
    "        while len(self.mu) < self.K:\n",
    "            self._dist_from_centers()\n",
    "            self.mu.append(self._choose_next_center())\n",
    "\n",
    "    def plot_init_centers(self):\n",
    "        X = self.X\n",
    "        # fig = plt.figure(figsize=(5, 5))\n",
    "        plt.xlim(-1, 1)\n",
    "        plt.ylim(-1, 1)\n",
    "        plt.plot(list(zip(*X))[0], list(zip(*X))[1], '.', alpha=0.5)\n",
    "        plt.plot(list(zip(*self.mu))[0], list(zip(*self.mu))[1], 'ro')\n",
    "        plt.savefig('kpp_init_N%s_K%s.png' % (str(self.N), str(self.K)),\n",
    "                    bbox_inches='tight', dpi=200)\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "# Str Kmeans algorithm\n",
    "\n",
    "import numpy as np \n",
    "from numpy import linalg as LA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "def StrmKMmeans(x, Centroids, Assign, cof_f, max_Cen, beta, DataType = 'Otherwise'):\n",
    "    m = len(x)\n",
    "\n",
    "    if not Centroids.size: \n",
    "        #print \"Entered the first time!\"\n",
    "        x = np.append(x, x)\n",
    "        x = np.append(x, 1)\n",
    "        Centroids = x\n",
    "        Centroids = x.reshape(1, len(x))\n",
    "        Assign = np.append(Assign, 0)\n",
    "\n",
    "    else: \n",
    "        Dist = np.sum((Centroids[:,:m] - x)**2, axis = 1)\n",
    "        delta_y = np.min(Dist)\n",
    "        idx_y = np.argmin(Dist)\n",
    "\n",
    "        if random()<delta_y/cof_f: # Create new facility for x\n",
    "            x = np.append(x, x)\n",
    "            x = np.append(x, 1)\n",
    "            Centroids = np.append( Centroids, [x], axis = 0 )\n",
    "            Assign = np.append(Assign, Centroids.shape[0]-1)\n",
    "        else: # Add x to an existing facility\n",
    "            Centroids[idx_y, m:2*m] = Centroids[idx_y, m:2*m] + x\n",
    "            Centroids[idx_y, 2*m] += 1\n",
    "            Assign =  np.append(Assign, idx_y)\n",
    "            \n",
    "    while Centroids.shape[0]>max_Cen: \n",
    "        cof_f = beta * cof_f;\n",
    "\n",
    "        for i in range(Centroids.shape[0]):\n",
    "            if DataType == 'Text':\n",
    "                Centroids[i, :m] = Centroids[i, m:2*m]/Centroids[i, 2*m]\n",
    "                Centroids[i, :m] = Centroids[i, m:2*m]/LA.norm(Centroids[i, m:2*m])\n",
    "            else:\n",
    "                Centroids[i, :m] = Centroids[i, m:2*m]/Centroids[i, 2*m]\n",
    "        \n",
    "        Centroids_til = Centroids[0, :]\n",
    "        Centroids_til = Centroids_til.reshape(1, len(Centroids_til))\n",
    "\n",
    "        for i in range(1, Centroids.shape[0]): # for each z in the preivous facility C\n",
    "            \n",
    "            Dist_til = np.sum((Centroids_til[:,:m] - Centroids[i, :m])**2, axis = 1)\n",
    "            delta_z = np.min(Dist_til)\n",
    "            idx_z = np.argmin(Dist_til)\n",
    "\n",
    "            if random()<(delta_z*Centroids[i, 2*m]/cof_f):\n",
    "                Centroids_til = np.append(Centroids_til, [Centroids[i, :]], axis = 0)\n",
    "            else:\n",
    "                Assign[Assign[:]==i] = idx_z\n",
    "                Centroids_til[idx_z, m:2*m] = Centroids_til[idx_z, m:2*m] + Centroids[i, m:2*m]\n",
    "                Centroids_til[idx_z, 2*m] += Centroids[i, 2*m]\n",
    "                \n",
    "        Centroids = Centroids_til\n",
    "\n",
    "    return Centroids, Assign, cof_f\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = np.array([[0.5,0.5], [0.3,0.2], [-1,-1], [2,1], [1,-1], [2,2], [3,5]])\n",
    "Centroids = np.array([])\n",
    "Assign = []\n",
    "for i in range(0, x.shape[0]):\n",
    "    print \"Step\", i, \": current point is \", x[i]\n",
    "    [Centroids, Assign, cof_f] = StrmKMmeans(x[i], Centroids, Assign, 10.0, 2, 5, DataType = 'Text')\n",
    "    print \"Step\", i, \"finished!\"\n",
    "    print Centroids, Assign\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "###############################################################################################################################\n",
    "\n",
    "\n",
    "# MiLOF Algorithm\n",
    "\n",
    "# Inputs:\n",
    "    # kpar: number of nearest neighbours\n",
    "    # dimension: dimension of data points\n",
    "    # buck: bucket size (memory limit)\n",
    "    # filepath: path of input data file\n",
    "    # num_k: number of clusters used in kmeans, streaming kmeans and weighted kmeans\n",
    "    # width: N/A\n",
    "\n",
    "import time\n",
    "time2 = time.time()\n",
    "    \n",
    "    \n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import configparser\n",
    "#import wKmeans as wkm\n",
    "#import strmKMeans as skm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "Scores = []\n",
    "\n",
    "class Point:\n",
    "    def __init__(self):\n",
    "        self.kdist = []\n",
    "        self.knn = []\n",
    "        self.lrd = []\n",
    "        self.LOF = []\n",
    "        # self.rdist = {}                                                                 ##Can I turn this off later?\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self.center = []\n",
    "        # self.LS = []                                                                    ##Can I turn this off later?\n",
    "\n",
    "def LOF(datastream, kpar):\n",
    "    Points = Point()\n",
    "    clf = LocalOutlierFactor(n_neighbors=kpar, algorithm=\"kd_tree\", leaf_size=30, metric='euclidean',contamination='auto')  \n",
    "    #Changed algorithm from kd_tree to auto; Changed metric from euclidean to minkowski with p=2;\n",
    "    clf.fit(datastream)\n",
    "    Points.LOF = [-x for x in clf.negative_outlier_factor_.tolist()]  #Gets the LOF score of each element\n",
    "    Points.lrd = clf._lrd.tolist()                                    #Gets the lrd score of each element\n",
    "    dist, ind = clf.kneighbors()                               #Gets the dist of an element from its k NN's and their indexes\n",
    "    Points.kdist = dist.tolist()                                      #Converts the dist array and stores it as a list\n",
    "    Points.knn = ind.tolist()                                         #Converts the index array and stores it as a list\n",
    "    return Points\n",
    "\n",
    "def ComputeDist(A, B):                        #Computes the linear distance between two arrays A and B after normalizing them  \n",
    "    dist = np.linalg.norm(A-B)     \n",
    "    return dist\n",
    "\n",
    "def union(list1, list2):                                                                  ##Why is this function used?\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    list3 = list1 + list(set2 - set1)\n",
    "    return list3\n",
    "\n",
    "def setdiff(list1, list2):                                                                ##Why is this function used?     \n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    list3 = list(set1 - set2)\n",
    "    return list3\n",
    "\n",
    "def IncrementalLOF_Fixed(Points, datastream, PointsC, Clusters, kpar, buck, width):\n",
    "    i = datastream.shape[0]\n",
    "    # print(\"******************* Processing Data Point\", i-1, \"*******************\")\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=kpar, algorithm='brute', metric='euclidean')\n",
    "    #Changed algorithm from brute to auto; Changed metric from euclidean to minkowski with p=2;\n",
    "    nbrs.fit(datastream[0:i-1, :])                                 #Fitting the NN on the dataset\n",
    "    dist, ind = nbrs.kneighbors(datastream[i-1, :].reshape(1, -1)) #Getting the dist and index of the n neighbors\n",
    "    Points.kdist = Points.kdist + dist.tolist()                                           ##Why doing this?\n",
    "    Points.knn = Points.knn + ind.tolist()                                                ##Why doing this?\n",
    "    # print(\"Points.kdist = \", Points.kdist)\n",
    "    # print(\"Points.knn = \", Points.knn)\n",
    "\n",
    "    dist = []\n",
    "    for j in range(0, len(Clusters.center)):\n",
    "        distCI = ComputeDist(Clusters.center[j], datastream[i-1, :]) - width;\n",
    "        # if distCI <=0:\n",
    "        # \tdistCI=PointsC.kdist[j]\n",
    "        dist = dist + [distCI]\n",
    "    # print(\"dist = \", dist)\n",
    "\n",
    "    if len(dist):\n",
    "        minval, ind =  min((dist[j], j) for j in range(0, len(dist)))\n",
    "        for j in range(0, len(Points.kdist[i-1])):\n",
    "            if minval < Points.kdist[i-1][j]:\n",
    "                Points.kdist[i-1][j] = minval\n",
    "                Points.knn[i-1][j] = buck + ind\n",
    "                if j < len(Points.kdist[i-1])-1:\n",
    "                    Points.kdist[i-1][j+1:] = [minval] * (len(Points.kdist[i-1]) - j - 1)\n",
    "                    Points.knn[i-1][j+1:] = [buck+ind] * (len(Points.kdist[i-1]) - j - 1)\n",
    "                break\n",
    "\n",
    "    rNN = []\n",
    "    for k in range(0, i-1):\n",
    "        distance = ComputeDist(datastream[k, :], datastream[i-1, :])\n",
    "        # print (\"distance between point\", k, \"and point\", i-1, \"=\", distance)\n",
    "        if (Points.kdist[k][-1] >= distance):\n",
    "            for kk in range(0, len(Points.knn[k])):\n",
    "                if distance <= Points.kdist[k][kk]:\n",
    "                    if kk == 0:\n",
    "                        Points.knn[k]   = [i-1] + Points.knn[k][kk:]\n",
    "                        Points.kdist[k] = [distance] + Points.kdist[k][kk:]\n",
    "                    else:\n",
    "                        Points.knn[k]   = Points.knn[k][0:kk] + [i-1] + Points.knn[k][kk:]\n",
    "                        Points.kdist[k] = Points.kdist[k][0:kk]+ [distance] + Points.kdist[k][kk:]\n",
    "                    break\n",
    "\n",
    "            for kk in range(kpar, len(Points.knn[k])):\n",
    "                # print(Points.kdist[k][kk], Points.kdist[k][kpar-1])\n",
    "                if Points.kdist[k][kk] != Points.kdist[k][kpar-1]:\n",
    "                    del Points.kdist[k][kk:]\n",
    "                    del Points.knn[k][kk:] \n",
    "                    break\n",
    "\n",
    "            rNN = rNN + [k]\n",
    "    # print(\"rNN = \", rNN)\n",
    "    # print(\"Points.kdist = \", Points.kdist)\n",
    "    # print(\"Points.knn = \", Points.knn)\n",
    "\n",
    "    updatelrd = rNN\n",
    "    if len(rNN) > 0:\n",
    "        for j in rNN:\n",
    "            for ii in Points.knn[j]:\n",
    "                if ii < len(Points.knn) and ii != i-1 and j in Points.knn[ii]:\n",
    "                    updatelrd = union(updatelrd, [ii])\n",
    "    # print(\"updatelrd = \", updatelrd)\n",
    "\n",
    "    rdist = 0\n",
    "    for p in Points.knn[i-1]:\n",
    "        if p > buck-1:\n",
    "            rdist = rdist + max(ComputeDist(datastream[i-1, :], Clusters.center[p-buck]) - width, PointsC.kdist[p-buck])\n",
    "        else:\n",
    "            rdist = rdist + max(ComputeDist(datastream[i-1, :], datastream[p,:]), Points.kdist[p][-1])\n",
    "    Points.lrd = Points.lrd + [1/(rdist/len(Points.knn[i-1]))]\n",
    "    # print(\"Points.lrd = \", Points.lrd)\n",
    "\n",
    "    updateLOF = updatelrd\n",
    "    if len(updatelrd) > 0:\n",
    "        for m in updatelrd:\n",
    "            rdist = 0\n",
    "            for p in Points.knn[m]:\n",
    "                if p > buck-1:\n",
    "                    rdist = rdist + max(ComputeDist(datastream[m, :], Clusters.center[p-buck])-width, PointsC.kdist[p-buck])\n",
    "                else:\n",
    "                    rdist = rdist + max(ComputeDist(datastream[m, :], datastream[p, :]), Points.kdist[p][-1])\n",
    "            Points.lrd[m] = 1/(rdist/len(Points.knn[m]))\n",
    "            for k in range(0, len(Points.knn)):\n",
    "                if k != m and Points.knn[k].count(m) > 0:\n",
    "                    updateLOF = union(updateLOF, [k])\n",
    "    # print(\"Points.lrd =\", Points.lrd)\n",
    "    # print(\"updateLOF =\", updateLOF)\n",
    "\n",
    "    # LOF neighbours\n",
    "    if len(updateLOF) > 0:\n",
    "        for l in updateLOF:\n",
    "            lof = 0\n",
    "            for ll in Points.knn[l]:\n",
    "                if ll > buck-1:\n",
    "                    lof = lof + PointsC.lrd[ll-buck]/Points.lrd[l]\n",
    "                else:\n",
    "                    lof = lof + Points.lrd[ll]/Points.lrd[l]\n",
    "            if l == len(Points.LOF):\n",
    "                Points.LOF = Points.LOF + [lof/len(Points.knn[l])]\n",
    "            else:\n",
    "                Points.LOF.append(lof/len(Points.knn[l]))\n",
    "                #Points.LOF[l] = lof/len(Points.knn[l])\n",
    "    # print(\"Points.LOF =\", Points.LOF)\n",
    "\n",
    "    # LOF-i\n",
    "    lof = 0\n",
    "    for ll in Points.knn[i-1]:\n",
    "        if ll > buck-1:\n",
    "            lof = lof + PointsC.lrd[ll-buck]/Points.lrd[i-1]\n",
    "        else:\n",
    "            lof = lof + Points.lrd[ll]/Points.lrd[i-1]\n",
    "    if i-1 == len(Points.LOF):\n",
    "        Points.LOF = Points.LOF + [lof/len(Points.knn[i-1])]\n",
    "    else:\n",
    "        Points.LOF[i-1] = lof/len(Points.knn[i-1])\n",
    "\n",
    "    # print(\"Points.knn =\", Points.knn)\n",
    "    # print(\"Points.LOF =\", Points.LOF)\n",
    "    # print(\"Points.lrd =\", Points.lrd)\n",
    "    # print(\"rNN = \", rNN)\n",
    "    # print(\"updatelrd = \", updatelrd)\n",
    "    # print(\"updateLOF =\", updateLOF)\n",
    "    \n",
    "    return Points\n",
    "\n",
    "def MILOF(kpar,dimension,buck,filename,num_k,width):\n",
    "    #config = configparser.ConfigParser()\n",
    "    #config.read(configFile)\n",
    "    #filepath = config['Analyzer']['InputMatFile']\n",
    "    filename = filename\n",
    "    #dimension = int(config['Analyzer']['Dimension'])\n",
    "    dimension = dimension\n",
    "    #num_k = int(config['Analyzer']['NumK'])\n",
    "    num_k = num_k\n",
    "    #kpar = int(config['Analyzer']['KPar'])\n",
    "    kpar = kpar\n",
    "    #buck = int(config['Analyzer']['Bucket'])\n",
    "    buck = buck\n",
    "    #width = int(config['Analyzer']['Width'])\n",
    "    width = width\n",
    "    \n",
    "    #datastream = sio.loadmat(filepath)                                             #Commented it\n",
    "    #datastream = np.array(datastream['DataStream'])                                #Commented it\n",
    "    # datastream = datastream[0:10*buck, :]\n",
    "    #datastream = np.delete(np.genfromtxt(filename,delimiter=','),obj=0,axis=0)\n",
    "    datastream = np.genfromtxt(filename,delimiter=',')                              #Can use this when comma is absent\n",
    "    datastream_for_plotting = datastream\n",
    "    #datastream = datastream[:, 0:dimension]                                        #Commented it\n",
    "    #datastream = np.unique(datastream, axis=0)                                     #Commented it\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(datastream)\n",
    "    datastream = scaler.transform(datastream)\n",
    "    PointNum = datastream.shape[0]\n",
    "    print (\"number of data points =\", PointNum)\n",
    "    # print (\"normalized data = \", datastream)\n",
    "\n",
    "    hbuck = int(buck/2) # half of buck\n",
    "    kdist = []\n",
    "    Scores = []\n",
    "    clusterLog = []\n",
    "    Clusters = Cluster()\n",
    "    PointsC= Point()\n",
    "    Points = LOF(datastream[0:kpar+1, :], kpar)\n",
    "    Scores = Scores + Points.LOF\n",
    "\n",
    "    for i in range(0, kpar+1):\n",
    "        kdist = kdist + [Points.kdist[i][-1]]\n",
    "\n",
    "    #print(\"Scores =\", Scores)    ## I have turned it off\n",
    "    #print(len(Scores))           ## I have turned it off\n",
    "    #print(\"kdist =\", kdist)\n",
    "\n",
    "    for i in range(kpar+2, hbuck+1):\n",
    "        Points = IncrementalLOF_Fixed(Points, datastream[0:i, :], PointsC, Clusters, kpar, buck, width)\n",
    "        Scores = Scores + [Points.LOF[i-1]]\n",
    "        kdist  = kdist + [Points.kdist[i-1][-1]]\n",
    "\n",
    "    # print(\"Points.kdist =\", Points.kdist)\n",
    "    # print(\"Points.knn =\", Points.knn)\n",
    "    # print(\"Points.LOF =\", Points.LOF)\n",
    "    # print(\"Points.lrd =\", Points.lrd)\n",
    "    # print(\"Scores =\", Scores)\t\n",
    "    # print(\"kdist =\", kdist)\n",
    "\n",
    "    exit = False\n",
    "    step = 0\n",
    "    while not exit:\n",
    "        for i in range(hbuck+1, buck+1):\n",
    "            if i > datastream.shape[0]:\n",
    "                exit = True\n",
    "                break\n",
    "            Points = IncrementalLOF_Fixed(Points, datastream[0:i, :], PointsC, Clusters, kpar, buck, width)\n",
    "            Scores = Scores + [Points.LOF[i-1]]\n",
    "            kdist  = kdist + [Points.kdist[i-1][-1]]\n",
    "\n",
    "        # print(\"Scores =\", Scores)\n",
    "        # print(\"kdist =\", kdist)\n",
    "        # print(\"Points.kdist =\", Points.kdist)\n",
    "        # print(\"Points.knn =\", Points.knn)\n",
    "        # print(\"Points.LOF =\", Points.LOF)\n",
    "        # print(\"Points.lrd =\", Points.lrd)\n",
    "\n",
    "        if not exit:\n",
    "            step = step + 1\n",
    "\n",
    "            print(\"*******************Step\", step, \": processing data points\", step*hbuck, \"to\", (step+1)*hbuck, \"*******************\")\n",
    "\n",
    "            indexNormal = list(range(0, hbuck))\n",
    "            kmeans = KMeans(n_clusters=num_k, init='k-means++', max_iter=100, n_jobs=-1)  # Considering precompute_distances for faster but more memory\n",
    "            kmeans.fit(datastream[0:hbuck, :]) # need to check how to configure to match result of matlab code \n",
    "            center = kmeans.cluster_centers_\n",
    "            clusterindex = kmeans.labels_\n",
    "\n",
    "            # print(\"label =\", clusterindex)\n",
    "            # print(\"center =\", center)\n",
    "\n",
    "            remClustLbl = list(range(0, num_k))\n",
    "            lof_scores = []\n",
    "            for itr in range(0, hbuck):\n",
    "                lof_scores = lof_scores + [Points.kdist[itr][-1]]\n",
    "            lof_scores = np.array(lof_scores)\n",
    "            lof_threshold = np.mean(lof_scores) + 3 * np.std(lof_scores) # Not sure if calcuating for each i is necessary\n",
    "\n",
    "            # print(\"lof_scores=\", lof_scores)\n",
    "            # print(\"lof_threshold=\", lof_threshold)\n",
    "\n",
    "            for kk in range(0, num_k):\n",
    "                clusterMembers = np.where(clusterindex==kk)\n",
    "                clusterMembersList = np.asarray(clusterMembers).flatten().tolist()\n",
    "                if np.sum(lof_scores[clusterMembers]>lof_threshold) > 0.5*len(clusterMembersList):\n",
    "                    indexNormal = setdiff(indexNormal, clusterMembersList)\n",
    "                    remClustLbl = setdiff(remClustLbl, [kk])\n",
    "            clusterindex = clusterindex[indexNormal]\n",
    "            center = center[remClustLbl,:]\n",
    "            \n",
    "            #print(\"center = \", center)\n",
    "            #print(\"label = \", clusterindex)\n",
    "        \n",
    "        \n",
    "            for j in range(0, len(remClustLbl)):\n",
    "                num = np.sum(clusterindex == remClustLbl[j])\n",
    "                PointsC.knn = PointsC.knn + [num]\n",
    "                Ckdist = Clrd = CLOF = 0\n",
    "                for k in np.array(indexNormal)[np.where(clusterindex==remClustLbl[j])]:\n",
    "                    Ckdist = Ckdist + Points.kdist[k][-1]\n",
    "                    Clrd   = Clrd   + Points.lrd[k]\n",
    "                    CLOF   = CLOF   + Points.LOF[k]\n",
    "\n",
    "                PointsC.kdist = PointsC.kdist + [Ckdist/num]\n",
    "                PointsC.lrd   = PointsC.lrd   + [Clrd/num]\n",
    "                PointsC.LOF   = PointsC.LOF   + [CLOF/num]\n",
    "\n",
    "            # print(PointsC.kdist)\n",
    "            # print(PointsC.lrd)\n",
    "            # print(PointsC.LOF)\n",
    "\n",
    "            datastream = np.delete(datastream, range(0,hbuck), axis=0)\n",
    "            del Points.kdist[0:hbuck]\n",
    "            del Points.knn[0:hbuck]\n",
    "            del Points.lrd[0:hbuck]\n",
    "            del Points.LOF[0:hbuck]\n",
    "\n",
    "            # print (\"remaining data = \", datastream)\n",
    "\n",
    "            initialClusters = len(Clusters.center)\n",
    "            if initialClusters > 0:\n",
    "                old_center = np.array(Clusters.center)\n",
    "                cluster_num = max(old_center.shape[0], center.shape[0])\n",
    "                print(cluster_num)\n",
    "                initial_center = []\n",
    "                if cluster_num == center.shape[0]:\n",
    "                    for x in center.tolist():\n",
    "                        initial_center.append(np.array(x))\n",
    "                else:\n",
    "                    for x in old_center.tolist():\n",
    "                        initial_center.append(np.array(x))\n",
    "\n",
    "                # print(\"old_center = \", old_center)\n",
    "                # print(\"center = \", center)\n",
    "                # print(\"weights = \", PointsC.knn[0:old_center.shape[0]+center.shape[0]])\n",
    "\n",
    "                #wkmeans = wkm.KPlusPlus(cluster_num, X=np.concatenate((old_center, center), axis=0), c=PointsC.knn[0:old_center.shape[0]+center.shape[0]], max_runs=5, verbose=False, mu=initial_center)\n",
    "                #Original\n",
    "                wkmeans = KPlusPlus(cluster_num, X=np.concatenate((old_center, center), axis=0), c=PointsC.knn[0:old_center.shape[0]+center.shape[0]], max_runs=5, verbose=False, mu=initial_center)\n",
    "                #My\n",
    "                wkmeans.find_centers(method='++')\n",
    "                # strmkmeans = skm.StrmKMmeans(x=np.concatenate((old_center, center), axis=0), Centroids=initial_center, Assign=PointsC.knn[0:old_center.shape[0]+center.shape[0]], )\n",
    "\n",
    "                merge_center = np.array(wkmeans.mu)\n",
    "                mergedindex = wkmeans.cluster_indices\n",
    "                cluster_num = len(merge_center)\n",
    "                clusterLog = clusterLog + [cluster_num]\n",
    "\n",
    "                PC = Point()\n",
    "                for j in range(0, cluster_num):\n",
    "                    num = np.sum(mergedindex==j)\n",
    "                    PCknn = PCkdist = PClrd = PCLOF = 0\n",
    "                    for k in np.asarray(np.where(mergedindex==j)).flatten().tolist():\n",
    "                        PCknn   = PCknn   + PointsC.knn[k]\n",
    "                        PCkdist = PCkdist + PointsC.knn[k] * PointsC.kdist[k]\n",
    "                        PClrd   = PClrd   + PointsC.knn[k] * PointsC.lrd[k]\n",
    "                        PCLOF   = PCLOF   + PointsC.knn[k] * PointsC.LOF[k]\n",
    "                    \n",
    "                    PC.knn   = PC.knn + [PCknn]\n",
    "                    PC.kdist = PC.knn + [PCkdist / PCknn]\n",
    "                    PC.lrd   = PC.knn + [PClrd / PCknn]\n",
    "                    PC.LOF   = PC.knn + [PCLOF / PCknn]\n",
    "                PointsC = PC\n",
    "                \n",
    "                Clusters.center = merge_center.tolist()\n",
    "            else:\n",
    "                Clusters.center = center.tolist()\n",
    "\n",
    "            #print (\"Clusters.center = \", len(Clusters.center))  #Uncommented it\n",
    "\n",
    "            for j in range(0, len(Points.knn)):\n",
    "                for k in range(0, len(Points.knn[j])):\n",
    "                    if Points.knn[j][k] >= hbuck:\n",
    "                        if Points.knn[j][k] < buck:\n",
    "                            Points.knn[j][k] = Points.knn[j][k] - hbuck\n",
    "                        else:\n",
    "                            Points.knn[j][k] = mergedindex[Points.knn[j][k]-buck] + buck\n",
    "                    else:\n",
    "                        indarr = np.where(np.array(indexNormal)==Points.knn[j][k])\n",
    "                        ind = np.asarray(indarr).flatten().tolist()\n",
    "                        if len(ind):\n",
    "                            cLabel = clusterindex[indarr]\n",
    "                            ci = np.asarray(np.where(np.array(remClustLbl)==cLabel)).flatten().tolist()[0]\n",
    "                            if not initialClusters:\n",
    "                                Points.knn[j][k] = ci + buck\n",
    "                            else:\n",
    "                                Points.knn[j][k] = mergedindex[ci+initialClusters] + buck\n",
    "                        else:\n",
    "                            mindist = []\n",
    "                            for kk in range(0, len(Clusters.center)):\n",
    "                                mindist = mindist + [ComputeDist(datastream[j,:], Clusters.center[kk])]\n",
    "                            mindistVal, ci =  min((mindist[x], x) for x in range(0, len(mindist)))\n",
    "                            Points.knn[j][k] = ci + buck\n",
    "\n",
    "            # print(\"Points.knn = \", Points.knn)\n",
    "    #print(\"Scores =\", Scores)\n",
    "    #print(len(Scores))\n",
    "    #Detect = LocalOutlierFactor.fit_predict(Scores)\n",
    "    #print(\"place2\",len(Clusters.center))  #Uncomment it\n",
    "    \n",
    "    \n",
    "    outlier_count = 0\n",
    "    outliers_for_display = []\n",
    "    for element in Scores:\n",
    "        if element > 3:\n",
    "            outlier_count = outlier_count + 1\n",
    "    #print('\\nNumber of Outliers For LOF_Thresh > 2 =\\t',outlier_count)\n",
    "    outlier_count1 = 0\n",
    "    for element1 in Scores:\n",
    "        if element1 > 1.5:\n",
    "            outlier_count1 = outlier_count1 + 1\n",
    "    print('\\nNumber of Outliers For LOF_Thresh > 1.5 =',outlier_count1)\n",
    "    print('\\nNumber of Outliers For LOF_Thresh > 2 =',outlier_count)\n",
    "    \n",
    "    \n",
    "    for i in range(len(datastream_for_plotting)):\n",
    "        if Scores[i] > 2:\n",
    "            outliers_for_display.append(datastream_for_plotting[i])\n",
    "    outliers_for_display_array = np.array(outliers_for_display)\n",
    "    \n",
    "    \n",
    "    if dimension == 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.scatter(datastream_for_plotting[:,0], datastream_for_plotting[:,1], c='blue', label='Datapoints')\n",
    "        plt.scatter(outliers_for_display_array[:,0], outliers_for_display_array[:,1], c='red', label='Outliers')\n",
    "        plt.xlabel('X-Coordinate')\n",
    "        plt.ylabel('Y-Coordinate')\n",
    "        plt.title('Plotting')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "kpar = int(input(\"\\nEnter the number of nearest neighbours\\n\"))\n",
    "dimension = int(input(\"\\nEnter the dimension of data points\\n\"))\n",
    "buck = int(input(\"\\nEnter the memory limit\\n\"))\n",
    "filename = 'Vowel.csv'#str(input('\\nEnter the filename\\n')) #'Corners(F).csv'\n",
    "num_k = int(input(\"\\nEnter the number of clusters used in kmeans and weighted kmeans\\n\"))\n",
    "width = int(input(\"\\nEnter width\\n\"))\n",
    "\n",
    "MILOF(kpar,dimension,buck,filename,num_k,width)\n",
    "\n",
    "elapsed2 = time.time() - time2\n",
    "\n",
    "\n",
    "\n",
    "print(elapsed2, \" sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
