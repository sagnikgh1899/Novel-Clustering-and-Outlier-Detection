{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liK-fTSTpFJj",
    "outputId": "23e7f590-4573-4225-8215-9d78d2aa3b7e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "\n",
    "# !pip install validclust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Bo3-2noPOK"
   },
   "source": [
    "#### PARAMETERS USED:\n",
    "<b>Dataset and K varies</b> <br>\n",
    "1. thresh (opt = 0.3)\n",
    "2. alpha_weight_avg (opt = 0.6)\n",
    "3. per in LLE (opt = 0.5)\n",
    "4. percent in GetCutOffDistMod (opt = 0.7)\n",
    "5. val2 in Correcting_wrong_clustering (opt = 20)\n",
    "6. val3 in Correcting_wrong_clustering (opt = 15)\n",
    "7. percent1 in Correcting_wrong_clustering (opt = 0.8)\n",
    "8. percent2 in Correcting_wrong_clustering (opt = 0.16)\n",
    "9. percent3 in Correcting_wrong_clustering (opt = 0.10)\n",
    "10. percent4 in Correcting_wrong_clustering (opt = 0.01)\n",
    "11. algo for NN = kd_tree\n",
    "13. KVal in Outlier Postprocessing = 25\n",
    "14. percent5 in Outlier Postprocessing = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QWCPTBOoPOR"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import csv\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "from validclust import dunn\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn .metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import shelve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQJNxGSUoPOT"
   },
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## USER INPUTS ##########################\n",
    "data_path = r\"E:\\ISI\\ISI PROJECT\\DATASETS 28-10-2020\\pendigits(data).csv\"\n",
    "# data_path = r\"/content/drive/MyDrive/DATASETS/Mnist(data).csv\"\n",
    "Ground_Truth_data_path=r\"E:\\ISI\\ISI PROJECT\\DATASETS 28-10-2020\\pendigits(gt).csv\"\n",
    "# Ground_Truth_data_path = r\"/content/drive/MyDrive/DATASETS/Mnist(gt).csv\"\n",
    "K = 50            ## Actually K-1 neighbors are considered. Remember              #### Imp\n",
    "#25 #20 #10\n",
    "Dataset_Name_Only = \"pendigits\"\n",
    "# For streaming\n",
    "train_start = 0\n",
    "train_end = 1000 #60\n",
    "test_start = 1000\n",
    "test_end = 1020 #6870 \n",
    "\n",
    "####################### Constant Parameters #######################\n",
    "percent5 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_storage_file = \"Streaming_Model_\" + Dataset_Name_Only + \".db\"\n",
    "SM = shelve.open(name_storage_file)\n",
    "print(\"Current Status of Shelve Dictionaries: \", list(SM.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ux_CTRpToPOU",
    "outputId": "4107086b-4c8e-4761-fa87-3500ba55a162"
   },
   "outputs": [],
   "source": [
    "# Name: Reading Data and Thresholds\n",
    "\n",
    "if 'Name: Reading Data and Thresholds' not in SM:\n",
    "    with open(data_path, 'r') as f:\n",
    "        data_all = np.genfromtxt(f, delimiter=',')   \n",
    "        ## If we take whole \n",
    "        # data1 = data_all\n",
    "        ## For Streaming\n",
    "        data1 = data_all[train_start:train_end]\n",
    "\n",
    "    Concept_Evolution_Thresh = 0.1*len(data_all)   # 0.06\n",
    "\n",
    "    Dimension = data1.shape[1]\n",
    "    #data1=data1/np.linalg.norm(data1)\n",
    "    print(\"Training Length: \", len(data1))\n",
    "    print(\"Dimension of Dataset: \",Dimension)\n",
    "    thresh = 0.3                                                                       #### Imp\n",
    "    alpha_weight_avg = 0.6\n",
    "    algo = 'ball_tree' #'kd_tree' \n",
    "\n",
    "    Outlier_Detection = True   # False \n",
    "    Clustering = False         # True\n",
    "    \n",
    "    ############### SAVING ###############\n",
    "    SM['data_path'] = data_path\n",
    "    SM['Ground_Truth_data_path'] = Ground_Truth_data_path\n",
    "    SM['K'] = K\n",
    "    SM['Dimension'] = Dimension\n",
    "    SM['thresh'] = thresh\n",
    "    SM['alpha_weight_avg'] = alpha_weight_avg\n",
    "    SM['algo'] = algo\n",
    "    SM['Outlier_Detection'] = Outlier_Detection\n",
    "    SM['Clustering'] = Clustering\n",
    "    SM['data1'] = data1\n",
    "    SM['train_start'] = train_start\n",
    "    SM['train_end'] = train_end\n",
    "    SM['test_start'] = test_start\n",
    "    SM['test_end'] = test_end\n",
    "    SM['data_all'] = data_all\n",
    "    SM['Concept_Evolution_Thresh'] = Concept_Evolution_Thresh\n",
    "\n",
    "    SM['Name: Reading Data and Thresholds'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Get Incoming Time\n",
    "\n",
    "if 'Name: Get Incoming Time' not in SM:\n",
    "    def GetIncomingTime(data_all):\n",
    "        inc_start_time = time.time()\n",
    "\n",
    "        for i in range(len(data_all)):\n",
    "            pass\n",
    "\n",
    "        total_time = time.time() - inc_start_time\n",
    "        avg_inc_time = total_time/(len(data_all)-1)\n",
    "        return avg_inc_time\n",
    "\n",
    "    data_all = SM['data_all']\n",
    "    incoming_time_dur = GetIncomingTime(data_all)\n",
    "    SM['incoming_time_dur'] = incoming_time_dur\n",
    "    print(\"Average Incoming Point is {} seconds\".format(incoming_time_dur))\n",
    "    \n",
    "    SM['Name: Get Incoming Time'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35bsEcZsoPOW"
   },
   "outputs": [],
   "source": [
    "# LLE: Locally linear Embedding\n",
    "def LLE(data, K): #, data_point_Idx):\n",
    "    row=len(data)\n",
    "    #print(row)\n",
    "    dimension=len(data[0])\n",
    "    #print(dimension)\n",
    "    neighbors = K\n",
    "    #W=np.zeros((row, K)) # W=np.zeros((row, row)) \n",
    "    count_pts = 0\n",
    "\n",
    "    hash_map = {value:np.array([]) for value in range(len(data))}\n",
    "\n",
    "    for i in range(row):\n",
    "        D_i=np.array(data-data[i, :])\n",
    "        #print(D_i)\n",
    "        distance=(D_i**2).sum(1)\n",
    "        #print(distance)\n",
    "        nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "        #print(nearest_neighbor)\n",
    "        D_nbrs=D_i[nearest_neighbor, :]\n",
    "        #print(D_nbrs)\n",
    "        ##Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "        Q=np.matmul(D_nbrs, D_nbrs.T)\n",
    "        #print(Q)\n",
    "#         t=np.trace(Q)\n",
    "#         r=0.001*t\n",
    "        r = 0.001 * float(Q.trace())\n",
    "        if(neighbors>=dimension):\n",
    "#             sig2 = (np.linalg.svd(D_i,compute_uv=0))**2\n",
    "#             r = np.sum(sig2[dimension:])\n",
    "            Q=Q+(r*np.identity(neighbors))\n",
    "#             Q.flat[::neighbors+1] += r\n",
    "        ##w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "        w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "        \n",
    "        w = w/sum(w)\n",
    "        ##w=w/sum(w)\n",
    "        #print(i, w)\n",
    "        #W[i, nearest_neighbor] = w\n",
    "        \n",
    "        # 10 2 6 7 3 9 5\n",
    "        # weight: 10 9 7 6 5 3 2\n",
    "        # pt:     (1,3,5,6,7)  pt2 pt 4 ::5\n",
    "\n",
    "        # My code\n",
    "        temp = []\n",
    "        for ele in w:\n",
    "            #print(ele)\n",
    "            #if ele != 0:\n",
    "            temp.append(ele)\n",
    "            temp.sort(reverse = True)\n",
    "            hash_map[i] = np.array(temp) \n",
    "            \n",
    "#     I=np.identity(row)\n",
    "#     M=I-W\n",
    "        if (count_pts+1) % 100 == 0:\n",
    "            print(\"****************Computed LLE for {0} datapoints****************\".format(count_pts+1))\n",
    "        count_pts += 1\n",
    "\n",
    "    per = 5 #0.6 #0.3 #0.5 #0.2 #0.05 #0.1\n",
    "    store_K = defaultdict()\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        count = 0\n",
    "        for i in range(1, len(hash_map[j])):\n",
    "            #print(((1/neighbors) - (dec_per*(1/neighbors))), ((1/neighbors) + (dec_per*(1/neighbors))))\n",
    "            if round((abs(hash_map[j][i-1] - hash_map[j][i])/abs(hash_map[j][i-1]))*100,1) <= per:\n",
    "                count += 1\n",
    "        store_K[j] = count\n",
    "        \n",
    "    # print(store_K)\n",
    "    for key in store_K:\n",
    "        store_K[key] += 1\n",
    "    return store_K\n",
    "\n",
    "    # return store_K[data_point_Idx] + 1  # +1 because all neighbouring algorithms consider the datapoint itself to be a part\n",
    "                                          # of its neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "67cBghs_oPOd",
    "outputId": "d99a5985-ddca-4160-a36c-ec03e468cacc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: LLE_lookup Calculation\n",
    "\n",
    "if 'Name: LLE_lookup Calculation' not in SM:\n",
    "    data1 = SM['data1']\n",
    "    K = SM['K']\n",
    "    \n",
    "    start4 = time.time()\n",
    "    # Optimization: Instead of calling the LLE for each point, we make it constant time\n",
    "    LLE_lookup = {}\n",
    "    LLE_lookup = LLE(data1, K)\n",
    "    # time taken = \",time.time()-start4,\" sec\")\n",
    "    \n",
    "    SM['LLE_lookup'] = LLE_lookup\n",
    "\n",
    "    print(\"time taken = \",time.time()-start4,\" sec\")\n",
    "    \n",
    "    SM['Name: LLE_lookup Calculation'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn1vHwCsoPOf",
    "outputId": "1e3ed7be-825c-45c3-f324-ee870227c0e7"
   },
   "outputs": [],
   "source": [
    "# Name: Distance with one another calculation\n",
    "\n",
    "if 'Name: Distance with one another calculation' not in SM:\n",
    "    data1 = SM['data1']\n",
    "    algo = SM['algo']\n",
    "    \n",
    "    ## Calculate the distance of each point with other points in the dataset\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "    print(\"****************Distance Between All Datapoints Computed****************\")\n",
    "    # # print(indices_ddcal)\n",
    "\n",
    "    # Hash_Map = {i:0 for i in range(len(data1))}\n",
    "    # labels = {i:0 for i in range(len(data1))}\n",
    "    # density = []\n",
    "    # for i in range(len(data1)):\n",
    "    #     density.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    #     Hash_Map[i] = density[i]\n",
    "    \n",
    "    # SM['nbrs_ddcal'] = nbrs_ddcal\n",
    "    \n",
    "    with open('distances_ddcal.p', 'wb') as pfile1:\n",
    "        pickle.dump(distances_ddcal, pfile1, protocol=4)\n",
    "    # SM['distances_ddcal'] = distances_ddcal\n",
    "    with open('indices_ddcal.p', 'wb') as pfile2:\n",
    "        pickle.dump(indices_ddcal, pfile2, protocol=4)\n",
    "    # SM['indices_ddcal'] = indices_ddcal\n",
    "    \n",
    "    SM['Name: Distance with one another calculation'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE11FTwAoPOh",
    "outputId": "3a5c3e35-0025-4cfb-f749-91ee80895d13",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Density Computation of Each\n",
    "\n",
    "if 'Name: Density Computation of Each' not in SM:\n",
    "    data1 = SM['data1']\n",
    "    data_all = SM['data_all']\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    with open('distances_ddcal.p', 'rb') as pfile1:\n",
    "        distances_ddcal = pickle.load(pfile1)\n",
    "    # distances_ddcal = SM['distances_ddcal']\n",
    "    with open('indices_ddcal.p', 'rb') as pfile2:\n",
    "        indices_ddcal = pickle.load(pfile2)\n",
    "    # indices_ddcal = SM['indices_ddcal']\n",
    "\n",
    "    # New Method of finding the densest point\n",
    "    Hash_Map = {i:0 for i in range(len(data1))}\n",
    "    labels = {i:0 for i in range(len(data1))}\n",
    "    Store_Self_Density = {i:0 for i in range(len(data_all))}\n",
    "    Store_Radius_Density = {i:[0,0] for i in range(len(data_all))}\n",
    "    Store_Parents = {i:[0] for i in range(len(data_all))}\n",
    "    density = []\n",
    "\n",
    "    for i in range(len(data1)):\n",
    "        K = LLE_lookup[i]  # 1 already added to K by the LLE function \n",
    "        distances = distances_ddcal[i][:K]\n",
    "        idxs = indices_ddcal[i][1:K]\n",
    "        # density.append([sum(distances)/(K-1), data1[i], max(distances), idxs, i])\n",
    "        Radius = max(distances)\n",
    "        # print(\"Sum: \", sum(distances))\n",
    "        # print(\"Area: \", (math.pi*Radius*Radius))\n",
    "        # print(\"Density: \", sum(distances)/(math.pi*Radius*Radius))\n",
    "        density.append([(sum(distances)/(math.pi*Radius*Radius)), data1[i], max(distances), idxs, i])\n",
    "        Hash_Map[i] = density[i]\n",
    "        if (i+1)%100 == 0: \n",
    "            print(\"****************Density Computed for {} Datapoints****************\".format(i+1))\n",
    "            \n",
    "    SM['density'] = density\n",
    "    SM['Hash_Map'] = Hash_Map\n",
    "    SM['labels'] = labels\n",
    "    SM['Store_Self_Density'] = Store_Self_Density\n",
    "    SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "    SM['Store_Parents'] = Store_Parents\n",
    "    \n",
    "    SM['Name: Density Computation of Each'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5ZoTBpOoPOi"
   },
   "outputs": [],
   "source": [
    "# Name: Sort Density\n",
    "\n",
    "if 'Sort Density' not in SM:\n",
    "    density = SM['density']\n",
    "\n",
    "    # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "    # 0: 0.45; (25,5); 0.2; [1,2,3,4,5]; 0\n",
    "\n",
    "    density.sort(key = itemgetter(0), reverse = True)  #False\n",
    "    #print(density[:3])\n",
    "    \n",
    "    SM['density'] = density\n",
    "    \n",
    "    SM['Name: Sort Density'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoySclAtoPOj"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the weighted moving average\n",
    "def numpy_ewma_vectorized_v2(values, alpha):\n",
    "    #values = np.array(values)\n",
    "    span = (2/alpha) - 1\n",
    "    df = pd.DataFrame(values)\n",
    "    return df.ewm(span=span).mean().iloc[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-s5Qe8TwoPOj"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BFS based Approach: \n",
    "\n",
    "It takes care of the maximum recursion depth constraint in Python programming Language.\n",
    "This however may take more time than DFS\n",
    "\"\"\"\n",
    "def NovelClus_FF(Parent, Hash_Map, dataset, K, list_of_parents_queue, labels, thresh, cluster_num, can_form_cluster, \n",
    "                 alpha_weight_avg, Tree_Structure, Roots, Store_Radius_Density, Store_Parents,\n",
    "                distances_ddcal, indices_ddcal, Store_Self_Density):\n",
    "    ##print(\"Cluster =\", cluster_num)\n",
    "    Hold_Root_Density = Parent[0]\n",
    "    \n",
    "    # Adding only the root\n",
    "    Roots[Parent[4]] = [Parent[1],Parent[0]]\n",
    "    Store_Radius_Density[Parent[4]] = [Parent[2], Parent[0]]\n",
    "    \n",
    "    # Add density of the root to Store_Self_Density\n",
    "    Store_Self_Density[Parent[4]] = Parent[0]\n",
    "    \n",
    "    \n",
    "    # We make use of a Deque for both side operation\n",
    "    Queue = deque()\n",
    "    \n",
    "    # Append from left the Parent Node\n",
    "    Queue.appendleft(Parent)\n",
    "    \n",
    "    \n",
    "    # print(\"IP: \", Parent)\n",
    "    \n",
    "    \n",
    "    # While Queue is not Null:\n",
    "        # Queue extracts the first Node\n",
    "        # We check if k children of Node satisfies the density criterion\n",
    "        # if a child satisfies, it is pushed into the Queue from the end; else it is termed -1 and left\n",
    "    \n",
    "    while Queue:\n",
    "        # Pop from the right end\n",
    "        Parent = Queue.pop()\n",
    "        # print(\"P: \", Parent[-1])\n",
    "            \n",
    "        # Getting important information from Parent\n",
    "        Density_Parent = Parent[0]\n",
    "        Radius = Parent[2]\n",
    "        #Child_Idx = Parent[3][idx] \n",
    "        Child_Idx_array = Parent[3]   # [94,  32, 117, .... 100, 118]\n",
    "        # print(\"CI: \", Child_Idx_array)\n",
    "        #Child = Hash_Map[Child_Idx]\n",
    "        #Child_Datapoint = Child[1]\n",
    "        \n",
    "        # Update the list_of_parents for the weighted moving average\n",
    "        list_of_parents = list_of_parents_queue.pop() + [Density_Parent] \n",
    "        Store_Parents[Parent[4]] = list_of_parents\n",
    "        #print(\"LoP =\", list_of_parents)\n",
    "        \n",
    "        \n",
    "        Child_array = []\n",
    "        Child_Datapoint_array = []\n",
    "        for Child_Idx in Child_Idx_array:\n",
    "            Child_array.append(Hash_Map[Child_Idx])\n",
    "            # print(\"F1: \", Child_Idx, Hash_Map[Child_Idx])\n",
    "            Child_Datapoint_array.append(Hash_Map[Child_Idx][1])  \n",
    "        \n",
    "        # For each Parent find which child satiesfies the density threshold criterion\n",
    "        ##print(\"Number of Children Nodes =\", len(Child_Idx_array), \"\\n\")\n",
    "        for i in range(len(Child_Idx_array)):\n",
    "            \n",
    "            Child = Child_array[i]\n",
    "            Child_Idx = Child[-1]             ##Child_Idx_array[i]\n",
    "            # print(\"Only CIN: \", Child_Idx)\n",
    "            Child_Datapoint = Child[1]        ##Child_Datapoint_array[i]\n",
    "            \n",
    "            # Base case: if child is already labelled in cluster then ignore\n",
    "            if labels[Child_Idx] > 0:\n",
    "                #print(\"Child is already labelled!!\")\n",
    "                continue\n",
    "             \n",
    "            \n",
    "            # # Old\n",
    "            # # print(\"Radius: \", Radius)\n",
    "            # neigh = NearestNeighbors(radius = Radius)\n",
    "            # neigh.fit(dataset)\n",
    "            # rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "            # print(\"Prev: \", len(rng[0][0]))\n",
    "            \n",
    "            # New\n",
    "            neigh_dist = []\n",
    "            neigh_idxs = []\n",
    "            rng = [[],[]]\n",
    "            for i in range(1, len(distances_ddcal[Child_Idx])):\n",
    "                if distances_ddcal[Child_Idx][i] < Radius:\n",
    "                    neigh_dist.append(distances_ddcal[Child_Idx][i])\n",
    "                    # print(\"NIs: \", indices_ddcal[Child_Idx][i])\n",
    "                    neigh_idxs.append(np.asarray(int(indices_ddcal[Child_Idx][i])))\n",
    "                else:\n",
    "                    break\n",
    "            rng[0].append(np.asarray(neigh_dist))\n",
    "            rng[1].append(np.asarray(neigh_idxs))\n",
    "            rng = tuple(rng)\n",
    "            # print(\"New RNG:\", len(rng[0][0]))\n",
    "            ####\n",
    "            \n",
    "            \n",
    "            if len(rng[0][0]) == 0:\n",
    "                Density_Child = 0\n",
    "            else:\n",
    "                # Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "                Density_Child = sum(rng[0][0])/(math.pi*Radius*Radius)\n",
    "                \n",
    "            #print(\"No. of Parent =\",len(list_of_parents))\n",
    "            \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "            #print(\"Density of Parent =\", weighted_density)\n",
    "            #print(\"Density of Child = \", Density_Child)\n",
    "            numerator = abs(weighted_density - Density_Child)\n",
    "            denominator = weighted_density\n",
    "            #print(\"Weighted Density =\", numerator/denominator)\n",
    "            \n",
    "            \n",
    "            # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "            if numerator/denominator <= thresh:\n",
    "                \n",
    "                labels[Child_Idx] = cluster_num\n",
    "                \n",
    "                list_of_parents_queue.appendleft(list_of_parents)\n",
    "                       \n",
    "                # Update the information for this child\n",
    "                Child[0] = Density_Child\n",
    "                if len(rng[0][0]) == 0:\n",
    "                    Child[2] = 0\n",
    "                else:\n",
    "                    Child[2] = max(rng[0][0])\n",
    "                Child[3] = rng[1][0]\n",
    "                \n",
    "                \n",
    "                # Store Adaptive Radius and New Density of the Parent Node for latter usage\n",
    "                list_of_parents_1 = list_of_parents + [Density_Child]\n",
    "                data_parent_1 = np.array(list_of_parents_1) #, dtype=np.float64)\n",
    "                weighted_density_1 = numpy_ewma_vectorized_v2(data_parent_1, alpha_weight_avg)  #O()??\n",
    "                Store_Radius_Density[Child[4]] = [Child[2], weighted_density_1]\n",
    "                \n",
    "                # Add density of child to Store_Self_Density\n",
    "                Store_Self_Density[Child[4]] = Density_Child\n",
    "                \n",
    "                # Add Child to the Tree Structure\n",
    "                if Parent[4] not in Tree_Structure:\n",
    "                    Tree_Structure[Parent[4]] = [Child[4]]\n",
    "                else:\n",
    "                    Tree_Structure[Parent[4]].append(Child[4])\n",
    "                \n",
    "                Queue.appendleft(Child)\n",
    "                \n",
    "                # Update the hashmap OLD\n",
    "                ##Hash_Map[Child[-1]] = Child\n",
    "                \n",
    "                # Update the hashmap NEW\n",
    "                temp_dict = {Child[-1]:Child}\n",
    "                Hash_Map.update(temp_dict)\n",
    "                ##################################\n",
    "                \n",
    "            \n",
    "                # The Parent can form a cluster of its own\n",
    "                can_form_cluster[0] = [True]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #print(\"Possible Anomaly!!\")\n",
    "                labels[Child_Idx] = -1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VRCA2MNoPOn"
   },
   "outputs": [],
   "source": [
    "# # Is this function really needed now??\n",
    "# def update_nbrs(data1, idx, K, algo): # 70\n",
    "#     nbrs_ddcal = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "#     distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "#     density_mod = []\n",
    "#     density_mod.append([sum(distances_ddcal[idx])/(K-1), data1[idx], max(distances_ddcal[idx]), indices_ddcal[idx][1:], idx])\n",
    "    \n",
    "#     density_mod.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "#     # density_cpy.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    \n",
    "#     return density_mod[0] #, density_mod[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmX3UW8soPOr"
   },
   "outputs": [],
   "source": [
    "def CheckWhichNodesHaveNoParentInTree(Tree_Structure):\n",
    "    SupposedRoots = set()\n",
    "    keysofDict = Tree_Structure.keys()\n",
    "    for key_idx in Tree_Structure:\n",
    "        boolean = False\n",
    "        for item in keysofDict:\n",
    "            if key_idx in set(Tree_Structure[item]):\n",
    "                boolean = True\n",
    "        if boolean == False:\n",
    "            SupposedRoots.add(key_idx)\n",
    "    return list(SupposedRoots)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PaGr4q7WlLc"
   },
   "outputs": [],
   "source": [
    "def AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density):\n",
    "  # Roots[key]: {[dp, density_value]}\n",
    "  for idx in Supposed_Roots:\n",
    "    dp = data1[idx]\n",
    "\n",
    "    boolean = False\n",
    "    for root in Roots:\n",
    "      item1 = Roots[root][0]\n",
    "      comparison = item1 == dp\n",
    "      equals_array = comparison.all()\n",
    "      if equals_array == True:\n",
    "        boolean = True\n",
    "        break\n",
    "\n",
    "    if boolean == False:\n",
    "      Roots[idx] = [dp, Store_Self_Density[idx]]\n",
    "  \n",
    "  return Roots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQyr41qi53dZ"
   },
   "outputs": [],
   "source": [
    "def MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density):\n",
    "  CurrRootsWithSameLabel = dict()\n",
    "  for i in range(len(Supposed_Roots)-1):\n",
    "    for j in range(i+1, len(Supposed_Roots)):\n",
    "      if labels[Supposed_Roots[i]] == labels[Supposed_Roots[j]]:\n",
    "        if labels[Supposed_Roots[i]] in CurrRootsWithSameLabel:\n",
    "          if Supposed_Roots[j] not in set(CurrRootsWithSameLabel[labels[Supposed_Roots[i]]]):\n",
    "            CurrRootsWithSameLabel[labels[Supposed_Roots[i]]].append(Supposed_Roots[j])\n",
    "        else:\n",
    "          CurrRootsWithSameLabel[labels[Supposed_Roots[i]]] = [Supposed_Roots[i]]\n",
    "          CurrRootsWithSameLabel[labels[Supposed_Roots[i]]].append(Supposed_Roots[j])\n",
    "\n",
    "  # CurrRootsWithSameLabel = {1: [1,2,3]}\n",
    "  if len(CurrRootsWithSameLabel) > 0:\n",
    "    for key_label in CurrRootsWithSameLabel:\n",
    "      MaxDensityTillNow = float(\"-inf\")\n",
    "      for idx in CurrRootsWithSameLabel[key_label]:\n",
    "        if Store_Self_Density[idx] > MaxDensityTillNow:\n",
    "          MaxDensityTillNow = Store_Self_Density[idx]\n",
    "          MostDenseIdx = idx\n",
    "      \n",
    "      # Add the remaining siblings with same label as children of the most dense sibling\n",
    "      for idx in CurrRootsWithSameLabel[key_label]:\n",
    "        if idx != MostDenseIdx:\n",
    "          Tree_Structure[MostDenseIdx].append(idx)\n",
    "      \n",
    "  return Tree_Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04PpI0-soPOr",
    "outputId": "9059c848-a4b3-4d5e-9382-50f26aa078b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Build Model\n",
    "\n",
    "if 'Name: Build Model' not in SM:\n",
    "    \n",
    "    def BuildModel(data1, density, Hash_Map, labels, thresh, alpha_weight_avg, distances_ddcal, indices_ddcal, start_with, \n",
    "                   Store_Self_Density, Store_Radius_Density, Store_Parents, LLE_lookup):\n",
    "\n",
    "        data_idx = 0\n",
    "        cluster_num = start_with\n",
    "        cluster_centers = []\n",
    "        hold_cluster_val = cluster_num ####\n",
    "\n",
    "        # For Storing Tree Structure\n",
    "        Tree_Structure = dict()\n",
    "        # Only Roots\n",
    "        Roots = dict()\n",
    "        # For Storing Radius and the Weighted Density of Ancestors including itself\n",
    "        # Store_Radius_Density = dict()\n",
    "        # For Storing parents\n",
    "        # Store_Parents = dict()\n",
    "        # For Storing Self Density\n",
    "        ## Store_Self_Density = dict()\n",
    "\n",
    "\n",
    "        for data_idx in range(len(data1)):  #O(n)\n",
    "\n",
    "            if (data_idx + 1) % 100 == 0:\n",
    "                print(\"**************************Computed {} Datapoints**************************\".format(data_idx+1))\n",
    "            idx = 0\n",
    "            list_of_parents_queue = deque()\n",
    "            list_of_parents_queue.appendleft([])\n",
    "\n",
    "\n",
    "            if labels[density[data_idx][-1]] == 0: #<= 0:\n",
    "                cluster_num += 1\n",
    "                labels[density[data_idx][-1]] = cluster_num\n",
    "                # print(\"CHECK1: \", density[data_idx][-1], labels[density[data_idx][-1]])\n",
    "                can_form_cluster = [False]\n",
    "\n",
    "\n",
    "                # Update information for the Parent node\n",
    "                #   0                         1         2        3                    4\n",
    "                # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "\n",
    "                K_mod = LLE_lookup[density[data_idx][-1]]      #O(1) Optimized One\n",
    "                #print(\"K modified: \",K_mod) # Can be uncommented for better intuition\n",
    "\n",
    "                #########################################\n",
    "                # # New One\n",
    "                # density[data_idx] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "                # temp_dict = {density[data_idx][-1]:density[data_idx]}\n",
    "                # #########################################\n",
    "\n",
    "                NovelClus_FF(density[data_idx], Hash_Map, data1, K_mod, list_of_parents_queue, labels, thresh, cluster_num, \n",
    "                             can_form_cluster, alpha_weight_avg, Tree_Structure, Roots, Store_Radius_Density, Store_Parents,\n",
    "                            distances_ddcal, indices_ddcal, Store_Self_Density)\n",
    "\n",
    "                Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "                # Check if cluster_num is same as the Supposed_Roots: if not then this function corrects it.\n",
    "                # I am considering that len(Supposed_Roots) > or = cluster_num. Is this correct?\n",
    "                if len(Supposed_Roots) != cluster_num:\n",
    "                  Tree_Structure = MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density)\n",
    "                  Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "                Roots = AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density)\n",
    "                # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "                if len(list(Supposed_Roots)) > len(Roots):\n",
    "                    pass\n",
    "                    # print(\"Error at BuildModel\")\n",
    "                    # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "                    # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "                    # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "\n",
    "                # # Visualization: For higher dimensions this time complexity can be ignored\n",
    "                # if Dimension <= 2:\n",
    "                #     plt.figure(figsize=(8,8))\n",
    "                #     for i in range(len(data1)):\n",
    "                #         if labels[i] == cluster_num:\n",
    "                #             plt.scatter(data1[i][0],data1[i][1],c='pink')\n",
    "                #         elif labels[i] == -1:\n",
    "                #             plt.scatter(data1[i][0],data1[i][1], c='yellow')\n",
    "                #         else:\n",
    "                #             plt.scatter(data1[i][0], data1[i][1],c='blue')\n",
    "                #     plt.scatter(density[data_idx][1][0], density[data_idx][1][1], c='red',s=200, marker = '+')\n",
    "                #     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "                # If the last Parent Node cannot form a clusterof its own then it is a part of a cluster with only 1 datapoint\n",
    "                # which means \" a possible Anomaly\"\n",
    "                # then change its label to -1 and reduce the cluster num by 1\n",
    "                if can_form_cluster[0] == False:\n",
    "                    labels[density[data_idx][-1]] = -1\n",
    "                    cluster_num = hold_cluster_val  ####\n",
    "                else:\n",
    "                    cluster_centers.append([density[data_idx][1].tolist(), cluster_num])\n",
    "                    hold_cluster_val += 1  ####\n",
    "\n",
    "                ## print(\"CC: \", cluster_centers)\n",
    "\n",
    "        return Roots, cluster_centers, Store_Radius_Density, Store_Parents, Store_Self_Density, Tree_Structure             \n",
    "\n",
    "\n",
    "\n",
    "    data1 = SM['data1']\n",
    "    density = SM['density']\n",
    "    Hash_Map = SM['Hash_Map']\n",
    "    labels = SM['labels']\n",
    "    thresh = SM['thresh']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    with open('distances_ddcal.p', 'rb') as pfile1:\n",
    "        distances_ddcal = pickle.load(pfile1)\n",
    "    # distances_ddcal = SM['distances_ddcal']\n",
    "    with open('indices_ddcal.p', 'rb') as pfile2:\n",
    "        indices_ddcal = pickle.load(pfile2)\n",
    "    # indices_ddcal = SM['indices_ddcal']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    \n",
    "\n",
    "    \n",
    "    start_with = 0\n",
    "    Roots, cluster_centers, Store_Radius_Density, Store_Parents, Store_Self_Density, Tree_Structure = BuildModel(data1, density, \n",
    "                        Hash_Map, labels, thresh, alpha_weight_avg, distances_ddcal, indices_ddcal, start_with, \n",
    "                        Store_Self_Density, Store_Radius_Density, Store_Parents, LLE_lookup)\n",
    "\n",
    "    Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "    # Check if cluster_num is same as the Supposed_Roots: if not then this function corrects it.\n",
    "    # I am considering that len(Supposed_Roots) > or = cluster_num. Is this correct?\n",
    "    cluster_num = len(cluster_centers)\n",
    "    if len(Supposed_Roots) != cluster_num:\n",
    "      Tree_Structure = MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density)\n",
    "      Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "    Roots = AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density)\n",
    "    # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "    if len(list(Supposed_Roots)) > len(Roots):\n",
    "        pass\n",
    "        # print(\"Error at BuildModel Called Function\")\n",
    "        # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "        # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "        # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "        \n",
    "    SM['Roots'] = Roots\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "    SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "    SM['Store_Parents'] = Store_Parents\n",
    "    SM['Store_Self_Density'] = Store_Self_Density\n",
    "    SM['Tree_Structure'] = Tree_Structure\n",
    "    SM['Supposed_Roots'] = Supposed_Roots\n",
    "    SM['cluster_num'] = cluster_num\n",
    "         \n",
    "    SM['Name: Build Model'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJ_x7VaXoPOt",
    "outputId": "74b5f734-9661-45e0-80a5-562051e0c051",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Display Cluster Centers\n",
    "\n",
    "if 'Name: Display Cluster Centers' not in SM:\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "\n",
    "    # Following print statements can be uncommented for better intuition\n",
    "    print(\"Current Cluster Centers: \\n\")\n",
    "    # print(cluster_centers)\n",
    "    print(len(cluster_centers))\n",
    "    \n",
    "    SM['Name: Display Cluster Centers'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x1OwUMmoPOu"
   },
   "outputs": [],
   "source": [
    "# print(Tree_Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2p-q8SgoPOu"
   },
   "outputs": [],
   "source": [
    "def GetIdxFromWholeDataset(whole_dataset, dp):\n",
    "    for i in range(len(whole_dataset)):\n",
    "        item = whole_dataset[i]\n",
    "        comparison = item == dp\n",
    "        equal_arrays = comparison.all()\n",
    "        if equal_arrays == True:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkS1bq8uoPOv"
   },
   "outputs": [],
   "source": [
    "# Function corrects the wrong clustering\n",
    "def Correcting_wrong_clustering(labels, cluster_centers, data1, Roots, Tree_Structure):  \n",
    "    cluster_labels = []\n",
    "    for key in labels:\n",
    "        cluster_labels.append(labels[key])\n",
    "    \n",
    "    Hash_Map_2 = dict()\n",
    "    X_label = dict()\n",
    "    \n",
    "    for i in range(len(data1)): \n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        if labels[i] in Hash_Map_2:\n",
    "            Hash_Map_2[labels[i]] += 1\n",
    "            X_label[labels[i]] = X_label[labels[i]] + [data1[i].tolist()]\n",
    "        else:\n",
    "            Hash_Map_2[labels[i]] = 1\n",
    "            X_label[labels[i]] = [data1[i].tolist()]\n",
    "            \n",
    "    # print(\"\\nFF1: HASH MAP\\n\", Hash_Map_2)\n",
    "\n",
    "    for key in Hash_Map_2:\n",
    "        temp = Hash_Map_2[key]\n",
    "        hold_idx = 0\n",
    "        for idx in range(len(cluster_centers)):\n",
    "            if cluster_centers[idx][1] == key:\n",
    "                hold_idx = idx\n",
    "                break\n",
    "        ## print(\"FF3: \", temp, cluster_centers)\n",
    "        Hash_Map_2[key] = [temp, cluster_centers[hold_idx][0]]\n",
    "        \n",
    "    # print(\"FF2: Hash_Map_2\\n\",Hash_Map_2,\"\\n\") ######\n",
    "        \n",
    "    # Function to find Intra Cluster Distance    \n",
    "    def IntraClusterMetric(X):\n",
    "        Intra_Clus_Dist = 0\n",
    "        for i in range(len(X)):\n",
    "            Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "        return Intra_Clus_Dist/(len(X)-1)\n",
    "           \n",
    "    \n",
    "    for key in X_label:\n",
    "        X = X_label[key]\n",
    "        Intra_metric = IntraClusterMetric(X)\n",
    "        ## print(\"F: \", Hash_Map_2[key], \"F1: \", [Intra_metric])\n",
    "        Hash_Map_2[key] = Hash_Map_2[key] + [Intra_metric]  \n",
    "    \n",
    "    # print(\"Hash_Map_2\\n\",Hash_Map_2,\"\\n\")\n",
    "    \n",
    "    clusnum_card = sorted(Hash_Map_2.items(), key=lambda kv:(kv[1], kv[0]), reverse = False)\n",
    "    cluster_center_new = []\n",
    "    for ele in clusnum_card:\n",
    "        cluster_center_new.append(np.array(ele[1][1]))   \n",
    "\n",
    "    ## Following print statements can be uncommented for better intuition\n",
    "    # print(Hash_Map_2)\n",
    "    # print(\"\\nAfter Sorting \")\n",
    "    # print(clusnum_card)\n",
    "    # print(\"\\nOnly cluster centers\")\n",
    "    # print(cluster_center_new)\n",
    "    \n",
    "    \n",
    "    if len(cluster_center_new) < len(cluster_centers):\n",
    "        K1 = len(cluster_center_new)\n",
    "    else:\n",
    "        K1 = len(cluster_centers)\n",
    "    nbrs = NearestNeighbors(n_neighbors=K1, algorithm='auto').fit(cluster_center_new)\n",
    "    distances, indices = nbrs.kneighbors(cluster_center_new)\n",
    "    #print(indices,\"\\n\")\n",
    "    #print(distances)\n",
    "\n",
    "    \n",
    "    def GetCutoffDistMod(cluster_centers):\n",
    "        percent = 0.7\n",
    "        i = 0\n",
    "        j = 1\n",
    "        Count = 0\n",
    "        Sum = 0\n",
    "        while i < len(cluster_centers)-1:\n",
    "            while j < len(cluster_centers):\n",
    "                Sum += np.linalg.norm(cluster_centers[i] - cluster_centers[j]) #math.dist(cluster_centers[i], cluster_centers[j])\n",
    "                j += 1\n",
    "                Count += 1\n",
    "            i += 1\n",
    "            j = i + 1\n",
    "        Count = max(Count, 1)\n",
    "        return math.ceil(Sum/Count)*percent\n",
    "    \n",
    "    \n",
    "    #print(clusnum_card)\n",
    "    \n",
    "    #print(\"\\n\\n\")\n",
    "    dist_val = GetCutoffDistMod(cluster_center_new)\n",
    "    val2 = 20\n",
    "    val3 = 15\n",
    "    percent1 = 0.8\n",
    "    percent2 = 0.16\n",
    "    percent3 = 0.10\n",
    "    percent4 = 0.02\n",
    "\n",
    "    # Format of clus_to_change: [curr_clus, clus_to_be]\n",
    "    clus_to_change = []\n",
    "    for i in range(len(clusnum_card)-1):\n",
    "        # We are not considering till the last term since,\n",
    "        # last term is the most dense and biggest among all \n",
    "        # the other clusters. It cannot be merged with others\n",
    "        \n",
    "        j = 1\n",
    "        cardinality_of_self = Hash_Map_2[clusnum_card[i][0]][0]\n",
    "        Intra_Clus_Self = Hash_Map_2[clusnum_card[i][0]][2]\n",
    "        cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "        \n",
    "        \n",
    "        while cardinality_of_self >= cardinality_of_neighbor and j < len(clusnum_card)-1:\n",
    "            #print(\"YES\")\n",
    "            j += 1\n",
    "            #print(\"j=\",j)\n",
    "            cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]  #-1\n",
    "            Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "\n",
    "        # cardinality_of_self = Hash_Map_2[clusnum_card[i][0]]\n",
    "        # cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        # print(cardinality_of_neighbor)\n",
    "    \n",
    "        ## Following print statements can be uncommented for better intuition\n",
    "        # print(\"Cluster number\", clusnum_card[i][0], \"Cardinality \", Hash_Map_2[clusnum_card[i][0]][0])\n",
    "        # print(\"Neighbor Cluster number\", clusnum_card[indices[i][j]][0], \"Cardinality \", Hash_Map_2[clusnum_card[indices[i][j]][0]][0])\n",
    "        # print(\"Intra_Clus_Self\", Intra_Clus_Self, \"Intra_Clus_Neigh \", Intra_Clus_Neigh)\n",
    "        # print(\"Intra Val Lower =\", (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh), \"Intra Val Higher =\", (Intra_Clus_Neigh + percent1*Intra_Clus_Neigh))\n",
    "        # print(\"Percent1*CNeigh\", int(percent1*cardinality_of_neighbor))\n",
    "        # print(\"Percent2 =\", (cardinality_of_self/cardinality_of_neighbor))\n",
    "        # print(\"val3\", val3)\n",
    "        # print(\"Or condition\", abs(cardinality_of_self - cardinality_of_neighbor), \"val2\", val2)\n",
    "        # print(\"Distances between self and neighbor\", int(round(distances[i][j],0)), \"Dist_val\", dist_val)\n",
    "        \n",
    "        \n",
    "        # Using a boolean merge to keep a track if the self cluster is merged or not\n",
    "        # Unless and until the cluster is merged, it is processed through every if condition\n",
    "        merged = False\n",
    "        \n",
    "        # If cardinality of self and cardinality of neighbor are both less than a given threshold\n",
    "        # where the threshold is very very small; say 10\n",
    "        if merged == False:\n",
    "            # print(\"Here Card1: \", cardinality_of_self, cardinality_of_neighbor)\n",
    "            if cardinality_of_self <= val2 and cardinality_of_neighbor <= val2:\n",
    "                # Distance checking here isn't necessary\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                #print(\"Merged 1st condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions\n",
    "        \n",
    "        # If cardinality of self is less than 16% of cardinality of neighbor \n",
    "        if merged == False:\n",
    "            # print(\"Here Card2: \", cardinality_of_self, cardinality_of_neighbor, (cardinality_of_self/cardinality_of_neighbor))\n",
    "            if (cardinality_of_self/cardinality_of_neighbor) <= percent2 and cardinality_of_self <= val3:\n",
    "                # print(\"F1: \", int(round(distances[i][j],0)), dist_val)\n",
    "                if int(round(distances[i][j],0)) <= dist_val:\n",
    "                    # print(\"F\")\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 2nd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions\n",
    "                \n",
    "        # If Cardinality of self > val3 but is less than percent6 of the cardinality of neighbor\n",
    "        if merged == False:\n",
    "            # print(\"Here Card3: \", cardinality_of_self, cardinality_of_neighbor)\n",
    "            if val3 < cardinality_of_self <= val3*2.7 and cardinality_of_self/cardinality_of_neighbor <= percent3:\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 3rd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions          \n",
    "        \n",
    "        # If Cardinality of self is less than percent4 of the cardinality of neighbor; where percent4 ~ 2 percent\n",
    "        if merged == False:\n",
    "            # print(\"Here Card4: \", cardinality_of_self, cardinality_of_neighbor)\n",
    "            if cardinality_of_self/cardinality_of_neighbor <= percent4:\n",
    "                # Here we don't need to check the distance and intra cluster density;\n",
    "                # The self cluster is very very small compared to the neighbor cluster.\n",
    "                # So just merge them\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                # print(\"Merged 4th condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "        \n",
    "        if merged == False: # Some merges by this logic as well\n",
    "            # print(\"Here Card5: \", cardinality_of_self, cardinality_of_neighbor)\n",
    "            if (cardinality_of_self <= cardinality_of_neighbor and cardinality_of_self <= val3) or (abs(cardinality_of_self - cardinality_of_neighbor) <= val2 and cardinality_of_self <= val3):\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    # print(\"Merged 5th condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "\n",
    "        # print(\"\\n\")\n",
    "    print(\"\\nChanged Clusters are (If any): [initial cluster --> New Cluster]\")\n",
    "    print(clus_to_change)\n",
    "    \n",
    "    \n",
    "    for pair in clus_to_change:\n",
    "        # print(\"F10: \", cluster_centers, pair[0], cluster_centers[pair[0]-1][0])\n",
    "        # print(\"F11: \", cluster_centers, pair[1], cluster_centers[pair[1]-1][0])\n",
    "        # print(\"Change: \", cluster_centers[pair[0]-1][0], \"To: \", cluster_centers[pair[1]-1][0])\n",
    "\n",
    "        idx_of_changing_cluster = GetIdxFromWholeDataset(data1, cluster_centers[pair[0]-1][0])\n",
    "        changingtolabel = pair[1]\n",
    "        # print(\"Change to label: \", changingtolabel)\n",
    "        for key in Tree_Structure:\n",
    "            if labels[key] == changingtolabel:\n",
    "                # print(\"Entering: \", idx_of_changing_cluster)\n",
    "                Tree_Structure[key].append(idx_of_changing_cluster)\n",
    "                break\n",
    "                \n",
    "        keystodel = []\n",
    "        for key in Roots:\n",
    "            if labels[key] == pair[0]:\n",
    "                keystodel.append(key)\n",
    "        \n",
    "        for key in keystodel:\n",
    "            del Roots[key]\n",
    "        ## print(Roots)\n",
    "                \n",
    "                \n",
    "    for key in labels:\n",
    "        for item in clus_to_change:\n",
    "            if labels[key] == item[0]:\n",
    "                labels[key] = item[1]\n",
    "    \n",
    "    return Roots, labels, Tree_Structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B3pPck4oPOw"
   },
   "outputs": [],
   "source": [
    "def RemoveDulplicatePositions(Tree_Structure, Store_Self_Density):\n",
    "    for parent_idx in Tree_Structure:\n",
    "        child_idxs = Tree_Structure[parent_idx]\n",
    "        for child_idx in child_idxs:\n",
    "            if child_idx in Tree_Structure:\n",
    "                if parent_idx in set(Tree_Structure[child_idx]):\n",
    "                    Tree_Structure[child_idx].remove(parent_idx)\n",
    "                    # print(\"DUPLICATE FOUND!!!!\")\n",
    "    return Tree_Structure\n",
    "\n",
    "\n",
    "def CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density):   \n",
    "    Hash_map = dict()\n",
    "    # print(Tree_Structure)\n",
    "    for parent_idx in Tree_Structure:\n",
    "        for child_idx in Tree_Structure[parent_idx]:\n",
    "            if child_idx in Hash_map:\n",
    "                parent1_idx = Hash_map[child_idx]\n",
    "                parent2_idx = parent_idx\n",
    "                # print(\"F\", parent1_idx, parent2_idx)\n",
    "                if Store_Self_Density[parent1_idx] > Store_Self_Density[parent2_idx]:\n",
    "                    Tree_Structure[parent2_idx].remove(child_idx)\n",
    "                else:\n",
    "                    Hash_map[child_idx] = parent2_idx\n",
    "                    Tree_Structure[parent1_idx].remove(child_idx)\n",
    "            \n",
    "            else:\n",
    "                Hash_map[child_idx] = parent_idx\n",
    "    return Tree_Structure\n",
    "            \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lip-5Q4PoPOw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For Properly Aligning the Tree Structure, \n",
    "## i.e., Parents are more denser than child (less density value for parent)\n",
    "\n",
    "## This function checks whether or not the Tree Structure adheres to our intuition\n",
    "def CheckTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, K):\n",
    "    Same_label_points = []\n",
    "    for i in range(len(data1)):\n",
    "        if labels[i] == K:\n",
    "            Same_label_points.append(i)    ## To store the points having similar labels\n",
    "    # print(Same_label_points)\n",
    "    Idx_Density = []\n",
    "    for idx in Same_label_points:   \n",
    "        Idx_Density.append([idx, Store_Self_Density[idx]])\n",
    "    # print(\"Without sort: \", Idx_Density)\n",
    "    Idx_Density.sort(key=itemgetter(1), reverse=True) #False\n",
    "    # print(\"With sort: \", Idx_Density)\n",
    "\n",
    "    Tree_Structure_Similar_label = {}\n",
    "    for pair in Idx_Density:\n",
    "        try:\n",
    "            Tree_Structure_Similar_label[pair[0]] = Tree_Structure[pair[0]]\n",
    "        except:\n",
    "            Tree_Structure_Similar_label[pair[0]] = []\n",
    "    # print(\"Tree_Structure_Similar_label: \", Tree_Structure_Similar_label)\n",
    "\n",
    "    Wrong_child_parent = []\n",
    "    for parent_idx in Tree_Structure_Similar_label:\n",
    "        if len(Tree_Structure_Similar_label[parent_idx]) != 0:\n",
    "            parent_density = Store_Self_Density[parent_idx]             \n",
    "            for child_idx in Tree_Structure_Similar_label[parent_idx]:\n",
    "                child_density = Store_Self_Density[child_idx]           \n",
    "                if child_density > parent_density:   ## Since we are considering density as it really means, so \n",
    "                                                     ## child node should be less dense than the parent node\n",
    "                    Wrong_child_parent.append([child_idx, parent_idx])\n",
    "    return Wrong_child_parent\n",
    "\n",
    "\n",
    "\n",
    "## This function swaps Current Parent and New Parent in the Tree Structure\n",
    "def SwapParentChild(Curr_Parent, New_Parent, Tree_Structure):\n",
    "    Childs_Curr_Parent = Tree_Structure[Curr_Parent]\n",
    "    if New_Parent in Tree_Structure:\n",
    "        Childs_New_Parent = Tree_Structure[New_Parent]\n",
    "    else:\n",
    "        Childs_New_Parent = []\n",
    "    for i in range(len(Childs_Curr_Parent)):\n",
    "        if Childs_Curr_Parent[i] == New_Parent:\n",
    "            Childs_Curr_Parent[i] = Curr_Parent\n",
    "            break\n",
    "    ## Uncomment for better intuition \n",
    "    # print(\"CCP: \", Childs_Curr_Parent)\n",
    "    # print(\"CNP: \", Childs_New_Parent)\n",
    "    \n",
    "    Tree_Structure[New_Parent] = Childs_Curr_Parent\n",
    "    Tree_Structure[Curr_Parent] = Childs_New_Parent\n",
    "    \n",
    "    for node in Tree_Structure:\n",
    "        if node != New_Parent:\n",
    "            Child_nodes = set(Tree_Structure[node])\n",
    "            if Curr_Parent in Child_nodes:\n",
    "                Child_nodes.remove(Curr_Parent)\n",
    "                Child_nodes.add(New_Parent)\n",
    "                Tree_Structure[node] = list(Child_nodes)\n",
    "                ## Uncomment for better intuition \n",
    "                # print(\"Changed:\", Tree_Structure[node])\n",
    "                \n",
    "    return Tree_Structure\n",
    "\n",
    "\n",
    "def AdjustRoots(Curr_Parent_idx, New_Parent_idx, Store_Self_Density, Roots, data1, Incoming_points):\n",
    "    if Curr_Parent_idx in Roots:\n",
    "        # print(\"ROOTS PREV: \", Roots)\n",
    "        del Roots[Curr_Parent_idx]\n",
    "        try:\n",
    "            Roots[New_Parent_idx] = [data1[New_Parent_idx], Store_Self_Density[New_Parent_idx]]\n",
    "        except:\n",
    "            Roots[New_Parent_idx] = [Incoming_points[New_Parent_idx], Store_Self_Density[New_Parent_idx]]\n",
    "        print(\"--> Roots Have Been Adjusted\")\n",
    "        # print(\"ROOTS NOW: \", Roots)\n",
    "                \n",
    "                \n",
    "                \n",
    "## This function analyzes the Tree Structure and makes necessary amendments\n",
    "def RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Incoming_points, Roots):\n",
    "  K = 1\n",
    "  while K <= cluster_num:\n",
    "    hold = []\n",
    "    while True:     \n",
    "      Wrong_child_parent_list = CheckTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, K)\n",
    "      if Wrong_child_parent_list in hold:\n",
    "        break\n",
    "      hold.append(Wrong_child_parent_list)\n",
    "      # print(\"Wrong_child_parent_list: \", Wrong_child_parent_list, \"len(Wrong_child_parent_list): \", len(Wrong_child_parent_list)) ###################\n",
    "      if len(Wrong_child_parent_list) == 0:\n",
    "        break\n",
    "      else:\n",
    "        Wrong_child_parent = deque() ####################\n",
    "        for pair1 in Wrong_child_parent_list:\n",
    "            Wrong_child_parent.appendleft(pair1)\n",
    "        # print(\"I am here!!\")\n",
    "        # print(\"Wrong_child_parent: \", Wrong_child_parent)\n",
    "        \n",
    "        # while Wrong_child_parent:\n",
    "        pts_with_same_parent = []\n",
    "        temp = Wrong_child_parent.pop()\n",
    "        pts_with_same_parent.append(temp) #[0]) # If there is only one pair of wrong child and parent\n",
    "                                                            # that is taken care here.\n",
    "\n",
    "        # If there are multiple pairs of wrong child and parent\n",
    "        while Wrong_child_parent and Wrong_child_parent[-1][1] == temp[1]:\n",
    "            temp1 = Wrong_child_parent.pop()\n",
    "            pts_with_same_parent.append(temp1)\n",
    "#                 if len(Wrong_child_parent) > 1:\n",
    "#                     i = 1\n",
    "#                     while i < len(Wrong_child_parent) and Wrong_child_parent[i][1] == Wrong_child_parent[i-1][1]:\n",
    "#                         pts_with_same_parent.append(Wrong_child_parent[i])\n",
    "#                         i += 1\n",
    "        # print(\"Same Parents: \", pts_with_same_parent)\n",
    "        parent_idx = pts_with_same_parent[0][1]\n",
    "        # print(\"Parent: \", parent_idx)\n",
    "        Hold_Child_Idx_Density = [None, float(\"-inf\")]\n",
    "        for pair in pts_with_same_parent:\n",
    "            density_child_node = Store_Self_Density[pair[0]]\n",
    "            if density_child_node > Hold_Child_Idx_Density[1]:  # <\n",
    "                Hold_Child_Idx_Density[0] = pair[0] \n",
    "                Hold_Child_Idx_Density[1] = density_child_node\n",
    "        # print(\"Hold_Child_Idx_Density \", Hold_Child_Idx_Density)\n",
    "        child_idx = Hold_Child_Idx_Density[0]\n",
    "        # print(\"Parent idx: \", parent_idx)\n",
    "        # print(\"Child idx: \", child_idx)\n",
    "        # print(\"Before: \", Tree_Structure[parent_idx], parent_idx)\n",
    "        Tree_Structure = SwapParentChild(parent_idx, child_idx, Tree_Structure)\n",
    "        # print(\"After: \", Tree_Structure[child_idx], child_idx)\n",
    "        AdjustRoots(parent_idx, child_idx, Store_Self_Density, Roots, data1, Incoming_points)  ################# \n",
    "        # print(\"Doing all Wrong_child_parent: \", Wrong_child_parent)\n",
    "    \n",
    "      Tree_Structure = RemoveDulplicatePositions(Tree_Structure, Store_Self_Density)\n",
    "      Tree_Structure = CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density)\n",
    "\n",
    "      Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "      # Check if cluster_num is same as the Supposed_Roots: if not then this function corrects it.\n",
    "      # I am considering that len(Supposed_Roots) > or = cluster_num. Is this correct?\n",
    "      if len(Supposed_Roots) != cluster_num:\n",
    "        Tree_Structure = MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density)\n",
    "        Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "      Roots = AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density)\n",
    "      # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "      if len(list(Supposed_Roots)) > len(Roots):\n",
    "          pass\n",
    "          #print(\"Error at Rearrange Tree Structure\")\n",
    "          #print(\"Supposed Roots: \", Supposed_Roots)\n",
    "          # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "          # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "        \n",
    "    print(\"****************Analyzed and Adjusted Datapoints of Cluster {}******************\".format(K))\n",
    "\n",
    "    # Increment K\n",
    "    K += 1\n",
    "  \n",
    "  RemoveNodesWithNoChild(Tree_Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yHuGRDIoPOx"
   },
   "outputs": [],
   "source": [
    "def RemoveNodesWithNoChild(Tree_Structure):\n",
    "    keystoremove = []\n",
    "    \n",
    "    for key in Tree_Structure:\n",
    "        if len(Tree_Structure[key]) == 0:\n",
    "            keystoremove.append(key)\n",
    "    \n",
    "    for key in keystoremove:\n",
    "        del Tree_Structure[key]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uzZq2qoFoPOy",
    "outputId": "99c700a0-72c2-4302-ba7c-fac3493d5b7a"
   },
   "outputs": [],
   "source": [
    "# Name: Correcting Wrong Clustering\n",
    "\n",
    "if 'Name: Correcting Wrong Clustering' not in SM:\n",
    "    labels = SM['labels']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    Roots = SM['Roots']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "\n",
    "    # print(cluster_centers)\n",
    "    # # Correct the wrong clusters\n",
    "    Roots, labels, Tree_Structure = Correcting_wrong_clustering(labels, cluster_centers, data1, Roots, Tree_Structure)\n",
    "    \n",
    "    SM['Roots'] = Roots\n",
    "    SM['labels'] = labels\n",
    "    SM['Tree_Structure'] = Tree_Structure\n",
    "\n",
    "    SM['Name: Correcting Wrong Clustering'] = True\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNbvMfQfoPOy"
   },
   "outputs": [],
   "source": [
    "# Name: Correcting Labels2\n",
    "\n",
    "if 'Name: Correcting Labels2' not in SM:\n",
    "\n",
    "\n",
    "    # This returns cluster numbers in a non-missing sorted order\n",
    "    # Before this function many clusters may be removed in the previous step\n",
    "    # So, to keep everything ordered, this function makes sure all clusters have a number\n",
    "    # and the number starts from 1 all the way to n without any missing number in between\n",
    "    def get_count_in_label(labels):\n",
    "        table = {value:0 for value in labels}\n",
    "        for ele in labels:\n",
    "            table[ele] += 1\n",
    "        key_list = table.keys()\n",
    "        #sorted(key_list, reverse = False)\n",
    "        for ele in sorted(key_list, reverse = False):\n",
    "            print(ele,\":\",table[ele])\n",
    "\n",
    "    def get_label_list(hash_map):\n",
    "        label_list = []\n",
    "        for key in hash_map.keys():\n",
    "            if hash_map[key] > 0:\n",
    "                label_list.append(hash_map[key])\n",
    "            #else:\n",
    "            #    get_labels.append(hash_map[key])\n",
    "        #print(get_labels)\n",
    "        return label_list    \n",
    "\n",
    "\n",
    "    def find_missing_idx(hash_map):\n",
    "        max_idx = float(\"-inf\")\n",
    "        for key in hash_map:\n",
    "            if hash_map[key] > max_idx:\n",
    "                max_idx = hash_map[key]\n",
    "        #max_idx = max(hash_map.keys())\n",
    "\n",
    "        min_idx = float(\"inf\")\n",
    "        for key in hash_map:\n",
    "            if hash_map[key] < min_idx and hash_map[key] != -1:\n",
    "                min_idx = hash_map[key]\n",
    "        #min_idx = 0\n",
    "\n",
    "        ########### print(min_idx, max_idx)                     \n",
    "\n",
    "        label_list = get_label_list(hash_map)\n",
    "\n",
    "        missing_idx = []\n",
    "        count = 1\n",
    "        while count <= max_idx:\n",
    "            if count not in label_list:\n",
    "                missing_idx.append(count)\n",
    "            count += 1\n",
    "\n",
    "        ########## print(missing_idx)  \n",
    "        return missing_idx\n",
    "\n",
    "    def correct_the_idx(hash_map): #labels):\n",
    "        missing_idx = find_missing_idx(hash_map) #labels)\n",
    "\n",
    "        # Can be uncommented for better intuition\n",
    "        # print(missing_idx)\n",
    "\n",
    "        while missing_idx:\n",
    "            for i in missing_idx:\n",
    "                for key in hash_map:# in range(len(labels)):\n",
    "                    if hash_map[key] == i + 1: #labels[j] == i+1:\n",
    "                        hash_map[key] = hash_map[key] - 1 #labels[j] = labels[j] - 1\n",
    "\n",
    "            #get_count_in_label(labels)\n",
    "\n",
    "            #print(\"\\n\")\n",
    "            #coorect_the_idx(labels)\n",
    "\n",
    "            missing_idx = find_missing_idx(hash_map) #labels)\n",
    "\n",
    "\n",
    "    #missing_idx = correct_the_label_idx(labels)\n",
    "    #print(missing_idx)\n",
    "\n",
    "    #get_count_in_label(labels)\n",
    "    # print(\"\\n\\n\")\n",
    "    labels = SM['labels']\n",
    "    correct_the_idx(labels)\n",
    "    #get_count_in_label(labels)\n",
    "    #find_missing_idx(hash_map)\n",
    "    #print(hash_map)\n",
    "\n",
    "    #label_l = get_label_list(hash_map)\n",
    "    #get_count_in_label(label_l)\n",
    "    \n",
    "    SM['labels'] = labels\n",
    "    SM['Name: Correcting Labels2'] = True\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tvu7yRmzoPOz",
    "outputId": "c6dabb95-5a74-46e2-912a-3cbf9534c228"
   },
   "outputs": [],
   "source": [
    "# Name: Before Post Processing Display\n",
    "\n",
    "if 'Name: Before Post Processing Display' not in SM:\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "\n",
    "    label_list = []\n",
    "    anomaly_list = []\n",
    "    max_val = float(\"-inf\")\n",
    "    for key in labels:\n",
    "        if max_val < labels[key]:\n",
    "            max_val = labels[key]\n",
    "        if labels[key] == -1:\n",
    "            anomaly_list.append(data1[key])\n",
    "        else:\n",
    "            label_list.append(labels[key])\n",
    "    cluster_num = max_val\n",
    "    #print(label_list)\n",
    "    print(\"\\n---------------------Before Outlier Post Processing----------------------\")\n",
    "    print(\"Total number of datapoints =\", len(data1))\n",
    "    print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "    print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "    print(\"Number of Cluster(s) =\", cluster_num)\n",
    "    \n",
    "    SM['data1'] = data1\n",
    "    SM['label_list'] = label_list\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    \n",
    "    SM['Name: Before Post Processing Display'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctZA1dYloPO0"
   },
   "outputs": [],
   "source": [
    "# Name: Display1\n",
    "\n",
    "if 'Name: Display1' not in SM:\n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization Before Outlier Post Processing')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    SM['Name: Display1'] = True\n",
    "    SM.sync()\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPKX2ET0oPO1"
   },
   "outputs": [],
   "source": [
    "# Name: Outlier Post Processing\n",
    "\n",
    "if 'Name: Outlier Post Processing' not in SM:\n",
    "\n",
    "    def OutlierPostProcessing(LLE_lookup, algo, data1, labels, percent5, alpha_weight_avg, \n",
    "                              Store_Radius_Density, distances_ddcal, indices_ddcal, Store_Parents, Store_Self_Density,\n",
    "                             Tree_Structure):\n",
    "        # Select a predetermined value of K\n",
    "        # Find the K nearest Neighbors of an outlier\n",
    "        # For each of those neighbors:\n",
    "        #     Find their Intracluster distance with the same K value\n",
    "        #     If the Intracluster distance of outlier is within +-5% of the intracuster distance of the neighbor,\n",
    "        #     and the neighbor point is not an outlier, \n",
    "        #     then consider the outlier to be a part of the smae cluster that the neighbor point belongs.\n",
    "\n",
    "        ##nbrs = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "        ##distances, indices = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] == -1:\n",
    "                # print(\"Here i: \", i)\n",
    "                # Predicted Outlier, so find its K nearest neighbors\n",
    "                #############################################################\n",
    "                Opt_K = LLE_lookup[i]\n",
    "                # print(\"Opt k: \", Opt_K)\n",
    "                indices = indices_ddcal[i][1:Opt_K]\n",
    "                # print(\"Indices: \", indices)\n",
    "                # nbrs = NearestNeighbors(n_neighbors=Opt_K, algorithm=algo).fit(data1)              ########################\n",
    "                # _, indices = nbrs.kneighbors(data1)\n",
    "                #############################################################\n",
    "\n",
    "                PredOutlierNNidx = indices               ##[i]\n",
    "                # print(\"IDXs: \", PredOutlierNNidx)\n",
    "                PredOutlierNN = []\n",
    "                for idx in PredOutlierNNidx:\n",
    "                    PredOutlierNN.append(data1[idx])\n",
    "                # Find the intracluster distance for the Predicted Outlier\n",
    "                PredOutlierICD = IntraClusterDistance(PredOutlierNN)\n",
    "                # print(\"Outlier\", i, PredOutlierICD)\n",
    "\n",
    "\n",
    "                # For each of the outlier's K neighbors\n",
    "                for idx in PredOutlierNNidx[1:]:\n",
    "\n",
    "                    # Check if the neighbor is an outlier or inlier\n",
    "                    if labels[idx] != -1:\n",
    "                        # print(\"Not an Outlier\")\n",
    "                        pruned_k = LLE_lookup[idx]\n",
    "                        NNeighboridx = indices_ddcal[i][1:pruned_k]\n",
    "                        NNeighbor = []\n",
    "                        for idx1 in NNeighboridx:\n",
    "                            NNeighbor.append(data1[idx1])\n",
    "\n",
    "                        # Find each neighbor's IntraClusterDistance(ICD)\n",
    "                        NNICD = IntraClusterDistance(NNeighbor)\n",
    "                        #print(\"NNICD: \", NNICD)\n",
    "\n",
    "                        # Check if outlier's ICD is about +-5% of neighbor's ICD\n",
    "                        # print(\"Lower: \",(NNICD-percent5*NNICD), \"Upper: \",(NNICD+percent5*NNICD), \"Self: \",PredOutlierICD)\n",
    "                        if (NNICD-percent5*NNICD) <= PredOutlierICD <= (NNICD+percent5*NNICD):\n",
    "                            # Change the label of the Predicted Outlier 'i' to the \n",
    "                            # label of the 1st Nearest Neighbor which satifies the condition\n",
    "                            labels[i] = labels[idx]\n",
    "                            # print(\"Changed\\n\")\n",
    "                            if idx not in Tree_Structure:\n",
    "                                Tree_Structure[idx] = [i]\n",
    "                            else:\n",
    "                                Tree_Structure[idx].append(i)\n",
    "\n",
    "                            # Update its Store_Radius_Density Value\n",
    "                            # print(\"IDX:\",idx)\n",
    "\n",
    "                            # # Old\n",
    "                            # Radius = Store_Radius_Density[idx][0]\n",
    "                            # neigh = NearestNeighbors(radius = Radius)\n",
    "                            # neigh.fit(data1)\n",
    "                            # rng = neigh.radius_neighbors([data1[i]])\n",
    "\n",
    "                            # New\n",
    "                            Radius = Store_Radius_Density[idx][0]\n",
    "                            neigh_dist = []\n",
    "                            neigh_idxs = []\n",
    "                            rng = [[],[]]\n",
    "                            for k in range(1, len(distances_ddcal[i])):\n",
    "                                if distances_ddcal[i][k] < Radius:\n",
    "                                    neigh_dist.append(distances_ddcal[i][k])\n",
    "                                    # print(\"NIs: \", indices_ddcal[Child_Idx][i])\n",
    "                                    neigh_idxs.append(np.asarray(int(indices_ddcal[i][k])))\n",
    "                                else:\n",
    "                                    break\n",
    "                            rng[0].append(np.asarray(neigh_dist))\n",
    "                            rng[1].append(np.asarray(neigh_idxs))\n",
    "                            rng = tuple(rng)\n",
    "                            # print(\"New RNG:\", len(rng[0][0]))\n",
    "                            ####\n",
    "\n",
    "\n",
    "                            # Check if there is noneighbour within the radius\n",
    "                            # If yes then the density of the recently changed datapoint is 0\n",
    "                            if len(rng[0][0]) == 0:\n",
    "                                Density_Changed_Dp = 0\n",
    "                            else:\n",
    "                                #Density_Parent = sum(rng[0][0])/len(rng[0][0])       ########################\n",
    "                                Radius = max(rng[0][0])\n",
    "                                Density_Changed_Dp = sum(rng[0][0])/(math.pi*Radius*Radius)\n",
    "\n",
    "                            list_of_parents = Store_Parents[idx] + [Density_Changed_Dp] \n",
    "                            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                            if len(rng[0][0]) == 0:\n",
    "                                Radius_new = 0\n",
    "                            else:\n",
    "                                Radius_new = max(rng[0][0])\n",
    "                            Store_Radius_Density[i] = [Radius_new, weighted_density]\n",
    "                            Store_Parents[i] = list_of_parents\n",
    "                            Store_Self_Density[i] = Density_Changed_Dp\n",
    "\n",
    "                            break\n",
    "\n",
    "\n",
    "    def IntraClusterDistance(X):\n",
    "            Intra_Clus_Dist = 0\n",
    "            for i in range(len(X)):\n",
    "                Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "            return Intra_Clus_Dist/(len(X)-1)\n",
    "\n",
    "\n",
    "\n",
    "    # KVal = int(0.01*len(data1)) #5 #0.03 \n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    algo = SM['algo']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    with open('distances_ddcal.p', 'rb') as pfile1:\n",
    "        distances_ddcal = pickle.load(pfile1)\n",
    "    # distances_ddcal = SM['distances_ddcal']\n",
    "    with open('indices_ddcal.p', 'rb') as pfile2:\n",
    "        indices_ddcal = pickle.load(pfile2)\n",
    "    # indices_ddcal = SM['indices_ddcal']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    OutlierPostProcessing(LLE_lookup, algo, data1, labels, percent5, alpha_weight_avg, \n",
    "                          Store_Radius_Density, distances_ddcal, indices_ddcal, Store_Parents, Store_Self_Density,\n",
    "                         Tree_Structure)\n",
    "    \n",
    "    SM['Name: Outlier Post Processing'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j83yU2W-oPO1",
    "outputId": "ab77727e-ec2f-495e-c371-31bf0cefb2e7"
   },
   "outputs": [],
   "source": [
    "# Name: After Post Processing Display\n",
    "\n",
    "if 'Name: After Post Processing Display' not in SM:\n",
    "    \n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    \n",
    "    label_list = []\n",
    "    anomaly_list = []\n",
    "    max_val = float(\"-inf\")\n",
    "    for key in labels:\n",
    "        if max_val < labels[key]:\n",
    "            max_val = labels[key]\n",
    "        if labels[key] == -1:\n",
    "            anomaly_list.append(data1[key])\n",
    "        else:\n",
    "            label_list.append(labels[key])\n",
    "    cluster_num = max_val\n",
    "    #print(label_list)\n",
    "    print(\"\\n------------ After Outlier Post Processing ------------\")\n",
    "    print(\"Total number of datapoints =\", len(data1))\n",
    "    print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "    print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "    print(\"Number of Cluster(s) =\", cluster_num)\n",
    "    \n",
    "    SM['data1'] = data1\n",
    "    SM['label_list'] = label_list\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    \n",
    "    SM['Name: After Post Processing Display'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2bq94iQoPO2"
   },
   "outputs": [],
   "source": [
    "# Name: Display2\n",
    "\n",
    "if 'Name: Display2' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization After Outlier Post Processing')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    SM['Name: Display2'] = True\n",
    "    SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knLgBW4doPOz",
    "outputId": "ce0186cd-8148-4f66-95e6-e8335e120a7c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: RearrangeTreeStructure1\n",
    "\n",
    "if 'Name: RearrangeTreeStructure1' not in SM:\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Roots = SM['Roots']\n",
    "    \n",
    "    RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, data1, Roots)\n",
    "    \n",
    "    SM['Name: RearrangeTreeStructure1'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jH5muYoXoPO2"
   },
   "outputs": [],
   "source": [
    "# if Clustering:\n",
    "#     # Performance Evaluation\n",
    "#     # Ground_Truth_data_path=r\"E:\\ISI\\GROUND TRUTHS CLUSTERING\\a2_gt.csv\"\n",
    "#     with open(Ground_Truth_data_path, 'r') as f:\n",
    "#         reader = csv.reader(f, delimiter=',')\n",
    "#         data_f = list(reader)\n",
    "\n",
    "#     # Clusters from Ground Truth Data\n",
    "#     clustersFromGT = []\n",
    "#     clustersFromGT.append(0)\n",
    "\n",
    "#     count = 1\n",
    "#     temp = []\n",
    "#     temp.append(int(data_f[0][-1]))\n",
    "\n",
    "#     for i in range(1, len(data_f)):\n",
    "#         if int(data_f[i][-1]) != int(data_f[i-1][-1]) and int(data_f[i][-1]) not in temp:\n",
    "#             temp.append(int(data_f[i][-1]))\n",
    "#             clustersFromGT.append(i)\n",
    "#             count += 1\n",
    "#     clustersFromGT.append(i+1)\n",
    "\n",
    "\n",
    "#     def EvaluateClustering(labels_PC, n_e_val_0, data_path, clustersFromGT, data1, labels_Idxs):\n",
    "#         with open(data_path, 'r') as f:\n",
    "#             reader = csv.reader(f, delimiter=',')\n",
    "#             data = list(reader)\n",
    "#             data = np.array(data).astype(float)\n",
    "\n",
    "#         d=2\n",
    "#         row=len(data)\n",
    "#         dimension=len(data[0])\n",
    "#         neighbors=4\n",
    "#         W=np.zeros((row, row))\n",
    "\n",
    "#         for i in range(row):\n",
    "#             D_i=np.array(data-data[i, :])\n",
    "#             distance=(D_i**2).sum(1)\n",
    "#             nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "#             D_nbrs=D_i[nearest_neighbor, :]\n",
    "#             Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "#             t=np.trace(Q)\n",
    "#             r=0.001*t\n",
    "#             if(neighbors>=dimension):\n",
    "#                 Q=Q+(r*np.identity(neighbors))\n",
    "#             w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "#             w=w/sum(w)\n",
    "#             W[i, nearest_neighbor]=w\n",
    "\n",
    "#         I=np.identity(row)\n",
    "#         M=I-W\n",
    "\n",
    "#         U, S, Vt = np.linalg.svd(M)\n",
    "#         e_val=S**2\n",
    "#         e_val_0=np.array(e_val<10**-20)\n",
    "\n",
    "\n",
    "#         uniq=np.array(np.unique(labels_PC))\n",
    "#         confusion_mat=np.zeros((n_e_val_0,n_e_val_0))\n",
    "#         status=np.zeros((len(uniq)))\n",
    "#         # print(\"\\nSTATUS_INIT\",status,\"\\n\")                                                                    \n",
    "\n",
    "#         x = 0\n",
    "#         y = 1\n",
    "#         f = 0                       \n",
    "\n",
    "#         get_cluster_from_ground_truth = clustersFromGT\n",
    "\n",
    "\n",
    "#         while y < len(get_cluster_from_ground_truth):\n",
    "#             cm_k=np.zeros((n_e_val_0))\n",
    "#             start = get_cluster_from_ground_truth[x]\n",
    "#             end = get_cluster_from_ground_truth[y]\n",
    "#             pred_labels=labels_PC[start:end]\n",
    "#             f += 1\n",
    "#             #print(f)\n",
    "#             while True:\n",
    "#                 maj_label=np.argmax(np.bincount(pred_labels))\n",
    "#                 if(status[maj_label]!=1):\n",
    "#                     status[maj_label]=1\n",
    "#                     break\n",
    "#                 pred_labels=pred_labels[pred_labels!=maj_label]\n",
    "#                 if len(pred_labels) == 0:\n",
    "#                     break\n",
    "#             cl_k_k=np.count_nonzero(np.array(pred_labels==maj_label))  # True Positive\n",
    "#             cm_k[maj_label]=cl_k_k  \n",
    "#             for i in range((n_e_val_0)): # n_e_val_0 = no.of clusters in dataset ? Till now 'NO'  \n",
    "#                 if (i != maj_label):\n",
    "#                     cl=np.count_nonzero(np.array(pred_labels==uniq[i]))\n",
    "#                     cm_k[i]=cl\n",
    "#             confusion_mat[maj_label, :]=cm_k\n",
    "#             x += 1\n",
    "#             y += 1\n",
    "\n",
    "#         # print(status,\"\\n\")\n",
    "\n",
    "#         def precision(label, confusion_mat):\n",
    "#             col=confusion_mat[:,label]\n",
    "#             return confusion_mat[label, label]/col.sum()\n",
    "\n",
    "#         def recall(label, confusion_mat):\n",
    "#             row=confusion_mat[label,:]\n",
    "#             return confusion_mat[label,label]/row.sum()\n",
    "\n",
    "#         precision_arr=[]\n",
    "#         recall_arr=[]\n",
    "#         f_score=[]\n",
    "#         g_mean=[]\n",
    "#         for i in range(n_e_val_0):\n",
    "#             p=precision(i, confusion_mat)\n",
    "#             r=recall(i, confusion_mat)\n",
    "#             f=2*p*r/(p+r)\n",
    "#             g=np.sqrt(p*r)\n",
    "#             precision_arr=np.append(precision_arr, p)\n",
    "#             recall_arr=np.append(recall_arr, r)\n",
    "#             f_score=np.append(f_score, f)\n",
    "#             g_mean=np.append(g_mean, g)\n",
    "\n",
    "#         f_score = f_score[np.logical_not(np.isnan(f_score))]\n",
    "#         g_mean = g_mean[np.logical_not(np.isnan(g_mean))]\n",
    "\n",
    "#         f_score=np.mean(f_score)\n",
    "#         g_mean=np.mean(g_mean)\n",
    "#         accuracy=(np.trace(confusion_mat)/len(data))*100\n",
    "\n",
    "#         print(\"\\n***************************** PERFORMANCE MEASURE *****************************\")\n",
    "#         print(\"Accuracy: \",accuracy,\"--------- f score:\",f_score,\"--------- g mean:\",g_mean)\n",
    "#         print(\"DB Index =\", davies_bouldin_score(data1, labels_Idxs))\n",
    "#         distances = pairwise_distances(data1)\n",
    "#         print(\"Dunn Index =\", dunn(distances, labels_Idxs))\n",
    "#         print('Silhouette Score =',silhouette_score(data1, labels_Idxs))\n",
    "#         print('Calinski-Harabasz Index =',metrics.calinski_harabasz_score(data1, labels_Idxs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "#     def GetLabelsForIdxs(labels):\n",
    "#         get_labels_DB = []\n",
    "#         for key in labels:\n",
    "#             if labels[key] > 0:\n",
    "#                 get_labels_DB.append(labels[key]-1)\n",
    "#             else:\n",
    "#                 get_labels_DB.append(labels[key])\n",
    "#         return get_labels_DB\n",
    "\n",
    "#     # Function to generate labels for getting the accurcay, f score and g mean\n",
    "#     def GetLabelsForPC(labels):\n",
    "#         get_labels_PC = []\n",
    "#         for key in labels:\n",
    "#             if labels[key] > 0:\n",
    "#                 get_labels_PC.append(labels[key]-1)\n",
    "#         return get_labels_PC\n",
    "\n",
    "\n",
    "#     get_labels_PC = GetLabelsForPC(labels)\n",
    "#     labels_PC = np.array(get_labels_PC)\n",
    "#     get_labels_DB = GetLabelsForIdxs(labels)\n",
    "#     labels_Idxs = np.array(get_labels_DB)\n",
    "#     EvaluateClustering(labels_PC, cluster_num, Ground_Truth_data_path, clustersFromGT, data1, labels_Idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCSdga8aoPO3"
   },
   "outputs": [],
   "source": [
    "# Name: Display3\n",
    "\n",
    "if 'Name: Display3' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Roots = SM['Roots']\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization After Outlier Post Processing')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    \n",
    "    SM['Name: Display3'] = True\n",
    "    SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZoF4KLXoPO3",
    "outputId": "fae9f78f-80fc-46b4-e06e-7f302974b172",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Before Restructuring\n",
    "\n",
    "if 'Name: Before Restructuring' not in SM:\n",
    "    Roots = SM['Roots']\n",
    "    print(\"Before Restructuring: \", Roots)\n",
    "    \n",
    "    SM['Name: Before Restructuring'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kws1f4iPoPO4",
    "outputId": "aa0eb5c9-aa1f-4e35-915e-b309680f9073"
   },
   "outputs": [],
   "source": [
    "# Name: RearrangeTreeStructure2\n",
    "\n",
    "if 'Name: RearrangeTreeStructure2' not in SM:\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Roots = SM['Roots']\n",
    "    \n",
    "    RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, [], Roots)\n",
    "    # print(CheckTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, K=4))\n",
    "    # for key in Tree_Structure:\n",
    "    #     for child in Tree_Structure[key]:\n",
    "    #         if Store_Self_Density[key] > Store_Self_Density[child]:\n",
    "    #             print(\"Fault!!\")\n",
    "    \n",
    "    SM['Name: RearrangeTreeStructure2'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tV8os9bdoPO4",
    "outputId": "efc3ee43-6fdd-4293-db91-4c38f6e34282",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Restructuring1\n",
    "\n",
    "if 'Name: Restructuring1' not in SM:\n",
    "    Roots = SM['Roots']\n",
    "    labels = SM['labels']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    \n",
    "    toremovepairs1 = []\n",
    "    idxs_1 = list(Roots.keys())\n",
    "    for i in range(len(idxs_1)-1):\n",
    "        for j in range(i+1, len(idxs_1)):\n",
    "            if labels[idxs_1[i]] == labels[idxs_1[j]]:\n",
    "                toremovepairs1.append([idxs_1[i], idxs_1[j]])\n",
    "\n",
    "    for pair in toremovepairs1:\n",
    "        if Store_Self_Density[pair[0]] < Store_Self_Density[pair[1]]:\n",
    "          try:\n",
    "            del Roots[pair[0]]\n",
    "          except:\n",
    "            pass\n",
    "        else:\n",
    "          try:\n",
    "            del Roots[pair[1]]\n",
    "          except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    for i in range(len(cluster_centers)):\n",
    "        for key in Roots:\n",
    "            if labels[key] == cluster_centers[i][1]:\n",
    "                cluster_centers[i][0] = Roots[key][0].tolist()\n",
    "                break\n",
    "\n",
    "\n",
    "    print(\"After Restructuring: \", Roots)\n",
    "    #print(cluster_centers)\n",
    "    \n",
    "    SM['Roots'] = Roots\n",
    "    \n",
    "    SM['Name: Restructuring1'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvwFKgIMoPO4"
   },
   "outputs": [],
   "source": [
    "# Name: Display4\n",
    "\n",
    "if 'Name: Display4' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Roots = SM['Roots']\n",
    "\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization After Root Adjustment')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "        \n",
    "    SM['Name: Display4'] = True\n",
    "    SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CuaiEneoPO5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEN1ZswboPO5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aE2HoV_8oPO5"
   },
   "outputs": [],
   "source": [
    "def Plot(countimg):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "    if cluster_num <= len(color):\n",
    "        for i in labels:\n",
    "            # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "            if labels[i] != -1:\n",
    "                try:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                except IndexError:\n",
    "                    plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                except IndexError:\n",
    "                    plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "                    \n",
    "                    \n",
    "    else:\n",
    "        for i in labels:\n",
    "            if labels[i] == -1:\n",
    "                try:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                except:\n",
    "                    plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                except:\n",
    "                    plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "    \n",
    "    for root in Roots:\n",
    "        plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "                    \n",
    "    # plt.xlabel('X-Coordinates')\n",
    "    # plt.ylabel('Y-Coordinates')\n",
    "    # plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "    plt.savefig('E:\\ISI\\STREAMING RESULTS\\RESULTS 29-06-2021\\Outlier(F)\\{}.png'.format(countimg[0]+1), bbox_inches='tight')\n",
    "    plt.close()\n",
    "    ## print(\"SAVED!!\")\n",
    "    # plt.show()\n",
    "    return [countimg[0]+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDcW-vsgoPO5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgZ6lhIBoPO6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_n3lL6-rOM9o"
   },
   "outputs": [],
   "source": [
    "def GetSingleRootIdxs(labels, data1):\n",
    "    SingleRootIdxs = []\n",
    "    Hash_Map_F = dict()\n",
    "    for i in range(len(data1)):\n",
    "        if labels[i] in Hash_Map_F:\n",
    "            Hash_Map_F[labels[i]].append(i)\n",
    "        else:\n",
    "            Hash_Map_F[labels[i]] = [i]\n",
    "    \n",
    "    for key_label in Hash_Map_F:\n",
    "        if len(Hash_Map_F[key_label]) == 1:\n",
    "            SingleRootIdxs.append(Hash_Map_F[key_label][0])\n",
    "    \n",
    "    return SingleRootIdxs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EamAd6HoOM9p"
   },
   "outputs": [],
   "source": [
    "def ChangeLabelsofSingleNodeTree(SingleRootIdxs, data1, algo, labels):\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "    \n",
    "    for item_idx in SingleRootIdxs:\n",
    "        neighbour_idx = indices_ddcal[item_idx][1]\n",
    "        # Change the sngle Root's label to the label of its nearest neighbour\n",
    "        labels[item_idx] = labels[neighbour_idx]\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQNwHNpzoPO6"
   },
   "outputs": [],
   "source": [
    "def RunConceptEvolution(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                         Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "                        Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "                       algo, alpha_weight_avg):\n",
    "    \n",
    "#     # 1\n",
    "#     data1 = ConceptEvolutionOne(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "#                          Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "#                         Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "#                     algo, alpha_weight_avg)\n",
    "\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    print(\"TOTAL NUMBER OF DATAPOINTS: \", len(data1))\n",
    "    \n",
    "    \n",
    "    # Change single node points part of some other cluster\n",
    "    SingleRootIdxs = GetSingleRootIdxs(labels, data1)\n",
    "    labels = ChangeLabelsofSingleNodeTree(SingleRootIdxs, data1, algo, labels)\n",
    "\n",
    "    \n",
    "    # 1\n",
    "    PlotSeparationAnomalies(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n",
    "    print(\"1 Done\")\n",
    "    \n",
    "    \n",
    "    # 2\n",
    "    Roots, cluster_centers, anomaly_idxs, anomaly_dps, anomaly_dict = CorrectRootsandClusterCentersandgetCurrentAnomalies(data1, \n",
    "                        Roots, cluster_centers, labels, Store_Self_Density)\n",
    "    print(\"2 Done\")\n",
    "    \n",
    "    # 3\n",
    "    density_1, distances_ddcal_1, indices_ddcal_1, Hash_Map, dense_pt = RunFindMostDensePoint(anomaly_dps, algo, \n",
    "                        Incoming_points, data1, Hash_Map, LLE_lookup, anomaly_idxs, anomaly_dict)\n",
    "    print(\"3 Done\")\n",
    "    \n",
    "#     # 4\n",
    "#     PlotWithDensePoint(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots, dense_pt)\n",
    "#     print(\"4 Done\")\n",
    "    \n",
    "    # 5\n",
    "    labels, Roots_1, cluster_centers_1, Store_Radius_Density_1, Store_Parents_1, Store_Self_Density_1, Tree_Structure_1, \\\n",
    "    density_1 = ChangelabelsandRunBuildModelStreaming(labels, \n",
    "    anomaly_idxs, anomaly_dps, density_1, Hash_Map, thresh, alpha_weight_avg, distances_ddcal_1, indices_ddcal_1,\n",
    "    data1, LLE_lookup, Store_Self_Density, train_end, train_start, Roots, Store_Radius_Density, Store_Parents)\n",
    "    print(\"5 Done\")\n",
    "    \n",
    "    # 6\n",
    "    cluster_centers = GetPresentClusterInformation1(cluster_centers_1, cluster_centers)\n",
    "    print(\"6 Done\")\n",
    "    \n",
    "#     # 7\n",
    "#     Roots_1, labels, Tree_Structure_1 = RunCorrectingwrongclusteringAndcorrectthelabels(labels, cluster_centers, data1, Roots_1)    \n",
    "    RunCorrectingwrongclusteringAndcorrectthelabels(labels, cluster_centers, data1, Roots_1)\n",
    "    print(\"7 Done\")\n",
    "    \n",
    "    # 8\n",
    "    Roots, Roots_1, cluster_centers = UpdateRootsandCluster1(cluster_centers, Roots, Roots_1, labels)\n",
    "    print(\"8 Done\")\n",
    "    \n",
    "    # 9\n",
    "    percent5 = 5\n",
    "    Tree_Structure, Store_Self_Density, Store_Radius_Density, Store_Parents, Roots, cluster_num = MergeAll(Tree_Structure, \n",
    "                            Tree_Structure_1, Store_Self_Density, Store_Self_Density_1, Store_Radius_Density, \n",
    "                             Store_Radius_Density_1, Store_Parents, Store_Parents_1, Roots, Roots_1,\n",
    "                             distances_ddcal_1, indices_ddcal_1, LLE_lookup, algo, anomaly_dps, anomaly_idxs, \n",
    "                            labels, percent5, alpha_weight_avg,)\n",
    "    print(\"9 Done\")\n",
    "    \n",
    "    \n",
    "    Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "    # Check if cluster_num is same as the Supposed_Roots: if not then this function corrects it.\n",
    "    # I am considering that len(Supposed_Roots) > or = cluster_num. Is this correct?\n",
    "    if len(Supposed_Roots) != cluster_num:\n",
    "      Tree_Structure = MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density)\n",
    "      Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "    Roots = AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density)\n",
    "    # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "    if len(list(Supposed_Roots)) > len(Roots):\n",
    "        pass\n",
    "        # print(\"Error at RunConceptEvolution after 9 done\")\n",
    "        # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "        # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "        # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "    \n",
    "    \n",
    "    # 10\n",
    "    Roots, cluster_centers, cluster_num = UpdateRootsAndClusterCentersAgain(Roots, cluster_centers, Store_Self_Density)\n",
    "    print(\"10 Done\")\n",
    "    \n",
    "#     # 11\n",
    "#     InitalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n",
    "#     print(\"11 Done\")\n",
    "    \n",
    "    # 12\n",
    "    labels = RunRearranging5(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots)\n",
    "    print(\"12 Done\")\n",
    "    \n",
    "#     # 13\n",
    "#     Correct1(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh)    ##### I added this\n",
    "#     print(\"13 Done\")\n",
    "    \n",
    "    # 14\n",
    "    updated_anomaly_idxs, updated_anomaly_dps = RunCheckandCorrectStreaming(data1, anomaly_idxs, LLE_lookup, anomaly_dps, \n",
    "                                Roots, Store_Radius_Density, Store_Parents, Hash_Map, thresh,\n",
    "                                Store_Self_Density, Tree_Structure, labels, Concept_Evolution_Thresh, cluster_centers,\n",
    "                                Dimension, train_end, train_start, algo, alpha_weight_avg)\n",
    "    print(\"14 Done\")\n",
    "    \n",
    "#     # 14\n",
    "#     Correct1(labels, data1, algo, LLE_lookup, Tree_Structure)    ##### I added this\n",
    "#     print(\"14 Done\")\n",
    "    \n",
    "    # 15\n",
    "    Roots, cluster_centers, cluster_num = UpdateRootsandCluster2(Roots, cluster_centers, Store_Self_Density)\n",
    "    print(\"15 Done\")\n",
    "    \n",
    "    # 16\n",
    "    labels = RunRearranging7(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots)\n",
    "    print(\"16 Done\")\n",
    "    \n",
    "    # 17\n",
    "    FinalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n",
    "    print(\"17 Done\")\n",
    "    \n",
    "    # 18\n",
    "    DisplayInformation(updated_anomaly_idxs, Roots)\n",
    "    print(\"18 Done\")\n",
    "    \n",
    "#     # 19\n",
    "#     Correct1(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh)    ##### I added this\n",
    "#     print(\"19 Done\")\n",
    "\n",
    "    Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "    # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "    if len(list(Supposed_Roots)) > len(Roots):\n",
    "        pass\n",
    "        # print(\"Error at RunConceptEvolution after 19 done\")\n",
    "        # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "        # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "        # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "    \n",
    "    \n",
    "    return data1, Roots, cluster_centers, labels, updated_anomaly_dps, updated_anomaly_idxs, \\\n",
    "            Tree_Structure, Store_Self_Density, cluster_num, Store_Radius_Density, Store_Parents, Hash_Map \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xSGXeCooPO6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsK779MioPO6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6EED1BUoPO9"
   },
   "outputs": [],
   "source": [
    "## Streaming portion of the model\n",
    "# Incomming datapoint: Inc_Dp\n",
    "\n",
    "def Euclidean_Distance(point1, point2):\n",
    "    # calculating Euclidean distance\n",
    "    dist = np.linalg.norm(point1 - point2)\n",
    "    return dist\n",
    "    \n",
    "    \n",
    "    \n",
    "def GetNearestRoot(Roots, Inc_Dp):\n",
    "    Dp_Rootidx_Rootdist = []\n",
    "    for root_idx in Roots:\n",
    "        dist_with_root = Euclidean_Distance(Roots[root_idx][0], Inc_Dp)\n",
    "        Dp_Rootidx_Rootdist.append([root_idx, dist_with_root])\n",
    "        Dp_Rootidx_Rootdist.sort(key=itemgetter(1), reverse=False)\n",
    "    return Dp_Rootidx_Rootdist\n",
    "\n",
    "\n",
    "\n",
    "def GetSiblingNode(Inc_Dp, Roots, Root_node_idx, data1, Tree_Structure):\n",
    "    parentnode = Roots[Root_node_idx][0]\n",
    "    distance_with_parent = Euclidean_Distance(Inc_Dp, parentnode)\n",
    "    # print(\"PN: \",parentnode, \"PD: \", distance_with_parent)\n",
    "    ImmediateAncestorNode_idx = Root_node_idx\n",
    "    \n",
    "    if Root_node_idx in Tree_Structure:\n",
    "        child_nodes_idxs = Tree_Structure[Root_node_idx]\n",
    "    else:\n",
    "        child_nodes_idxs = []\n",
    "    min_dist = float(\"inf\")\n",
    "    for child_node_idx in child_nodes_idxs:\n",
    "        curr_dist = Euclidean_Distance(Inc_Dp, data1[child_node_idx])\n",
    "        # print(\"CN: \", data1[child_node], \"CD: \",curr_dist, \"MD: \", min_dist)\n",
    "        if curr_dist < min_dist:\n",
    "            min_dist = curr_dist\n",
    "            curr_sibling_node_idx = child_node_idx\n",
    "    distance_with_child = min_dist\n",
    "    \n",
    "    # Set initial sibling node as the Root node\n",
    "    sibling_node_idx = Root_node_idx\n",
    "    \n",
    "    # Check for the nearest node in the Tree Structure if it exists!!\n",
    "    while distance_with_parent > distance_with_child: # (3 1)\n",
    "        # print(\"DWP: \", distance_with_parent, \"DWC: \", distance_with_child, \"CSN: \",data1[curr_sibling_node])\n",
    "        sibling_node_idx = curr_sibling_node_idx\n",
    "        parent_node_idx = child_node_idx\n",
    "        \n",
    "        if parent_node_idx not in Tree_Structure:\n",
    "            # print(\"YES\")\n",
    "            break\n",
    "        else:\n",
    "            ImmediateAncestorNode_idx = parent_node_idx\n",
    "            child_nodes_idxs = Tree_Structure[parent_node_idx]\n",
    "            min_dist = float(\"inf\")\n",
    "            for child_node_idx in child_nodes_idxs:\n",
    "                curr_dist = Euclidean_Distance(Inc_Dp, data1[child_node_idx])\n",
    "                if curr_dist < min_dist:\n",
    "                    min_dist = curr_dist\n",
    "                    curr_sibling_node_idx = child_node_idx\n",
    "\n",
    "            distance_with_parent = Euclidean_Distance(Inc_Dp, data1[parent_node_idx])\n",
    "            distance_with_child = min_dist\n",
    "            \n",
    "            sibling_node_idx = curr_sibling_node_idx\n",
    "            parent_node_idx = child_node_idx\n",
    "            \n",
    "\n",
    "    return sibling_node_idx, ImmediateAncestorNode_idx\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "def IncludeWithinTree(Inc_Dp, Inc_Dp_Idx, Sibling_node, ImmediateAncestorNode, Store_Radius_Density, \n",
    "                      Tree_Structure, dataset, Hash_Map, labels, thresh, Store_Parents, Roots):\n",
    "    # For the point to be included within the same tree, the density of that has to be calculated wrt the tree.\n",
    "    # Meaning, the neighbours of that new point, should be already members of that Tree.\n",
    "    # They should not be anomalies or other cluster members\n",
    "    labelofCurrentTree = labels[Sibling_node]\n",
    "\n",
    "    rng = [[],[]]\n",
    "    neigh_dist = []\n",
    "    neigh_idxs = []        \n",
    "\n",
    "    # Get density of the Inc_Dp\n",
    "    Radius = Store_Radius_Density[Sibling_node][0]\n",
    "    # print(\"Radius: \", Radius)\n",
    "    neigh = NearestNeighbors(radius = Radius)\n",
    "    neigh.fit(dataset)\n",
    "    rng1 = neigh.radius_neighbors([Inc_Dp])\n",
    "\n",
    "    for i in range(len(rng1[1][0])):\n",
    "      if (rng1[1][0][i]) in labels:  \n",
    "        if labels[rng1[1][0][i]] == labelofCurrentTree:\n",
    "          neigh_dist.append(rng1[0][0][i])\n",
    "          neigh_idxs.append(np.asarray(int(rng1[1][0][i])))\n",
    "\n",
    "    rng[0].append(np.asarray(neigh_dist))\n",
    "    rng[1].append(np.asarray(neigh_idxs))   \n",
    "    rng = tuple(rng)\n",
    "\n",
    "\n",
    "    # Check if there is no neighbour within the radius\n",
    "    # If yes then the density of the incoming datapoint is 0\n",
    "    if len(rng[0][0]) == 0:\n",
    "        Density_Inc_Dp = 0\n",
    "    else:\n",
    "        # Density_Inc_Dp = sum(rng[0][0])/len(rng[0][0])\n",
    "        Density_Inc_Dp = sum(rng[0][0])/(math.pi*Radius*Radius) \n",
    "    \n",
    "    while True:\n",
    "        # We check for the three possible positions for the new point's inclusion:\n",
    "        # 1. As a child of the parent of its Sibling\n",
    "        # 2. As a child of its Sibling\n",
    "        # 3. As a child of any of its Sibling's childs\n",
    "        \n",
    "        # Set as Child of Parent of Sibling\n",
    "        Parent = ImmediateAncestorNode\n",
    "        wt_density_ancestors = Store_Radius_Density[Parent][1]\n",
    "        numerator = abs(wt_density_ancestors - Density_Inc_Dp)\n",
    "        denominator = wt_density_ancestors\n",
    "        # print(\"DIP1: \", Density_Inc_Dp, \"N: \", numerator, \"D: \", denominator, \"Frac: \", numerator/denominator)\n",
    "        \n",
    "        # Calculate the distance with parent(d1). Calculate the distance of the parent with its parent(d2).\n",
    "        # If d1 is within __ % of d2, then include otherwise don't\n",
    "#         Parent_dp = dataset[Parent]\n",
    "#         d1 = np.linalg.norm(Parent_dp - Inc_Dp)\n",
    "        # Getting the Parent's parent idx\n",
    "#         Parents_parent_dp = None   # If the parent is a root no need to check\n",
    "#         for key in Tree_Structure:\n",
    "#             if Parent in set(Tree_Structure[key]):\n",
    "#                 Parents_parent_idx = key\n",
    "#                 Parents_parent_dp = dataset[Parents_parent_idx]\n",
    "#                 break\n",
    "#         if Parents_parent_dp is not None:\n",
    "#             d2 = np.linalg.norm(Parent_dp - Parents_parent_dp)\n",
    "#             # print(\"d1: \", d1, \"d2: \", d2)\n",
    "                    \n",
    "        if numerator/denominator <= thresh: ############## and (Parents_parent_dp is None or (d2/d1)>0.7):    \n",
    "            labels[Inc_Dp_Idx] = labels[ImmediateAncestorNode]\n",
    "            # Add this newly added nodes information to the existing Hash_Map\n",
    "            #            0                1         2        3                    4\n",
    "            # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "            if len(rng[0][0]) == 0:\n",
    "                Radius_new_node = 0\n",
    "            else:\n",
    "                Radius_new_node = max(rng[0][0])\n",
    "            NewNode = [Density_Inc_Dp, Inc_Dp, Radius_new_node, rng[1][0], Inc_Dp_Idx]\n",
    "            temp_dict = {NewNode[-1]:NewNode}\n",
    "            Hash_Map.update(temp_dict)\n",
    "\n",
    "            # Store Adaptive Radius and New Weighted Density of all ansectors including the New Node\n",
    "            # print(\"IDX: \", NewNode[4])\n",
    "            list_of_parents = Store_Parents[Parent] + [Density_Inc_Dp] \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "            Store_Radius_Density[NewNode[4]] = [NewNode[2], weighted_density]\n",
    "            Store_Parents[NewNode[4]] = list_of_parents\n",
    "            Store_Self_Density[NewNode[4]] = Density_Inc_Dp\n",
    "\n",
    "            # Add the New Node to the Tree Structure\n",
    "            if Parent not in Tree_Structure:\n",
    "                Tree_Structure[Parent] = [NewNode[4]]\n",
    "            else:\n",
    "                Tree_Structure[Parent].append(NewNode[4])\n",
    "            # print(\"CHILD OF PARENT OF SIBLING\")\n",
    "            \n",
    "            # Check for duplicate poistions\n",
    "            Tree_Structure = RemoveDulplicatePositions(Tree_Structure, Store_Self_Density)\n",
    "            Tree_Structure = CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density)\n",
    "            \n",
    "            print(\"----------------------------------Rearranging1----------------------------------\")\n",
    "            cluster_num = len(Roots)\n",
    "            # dataset.append(Inc_Dp)\n",
    "            RearrangeTreeStructure(Tree_Structure, dataset, labels, Store_Self_Density, cluster_num, Incoming_points, Roots)\n",
    "            \n",
    "            break\n",
    "        # print(\"NOT CHILD OF PARENT OF SIBLING\")\n",
    "\n",
    "\n",
    "        # Set as Child of Sibling\n",
    "        Parent = Sibling_node\n",
    "        wt_density_ancestors = Store_Radius_Density[Parent][1]\n",
    "        numerator = abs(wt_density_ancestors - Density_Inc_Dp)\n",
    "        denominator = wt_density_ancestors\n",
    "        # print(\"DIP2: \", Density_Inc_Dp, \"N: \", numerator, \"D: \", denominator, \"Frac: \", numerator/denominator)\n",
    "        \n",
    "        # Calculate the distance with parent(d1). Calculate the distance of the parent with its parent(d2).\n",
    "        # If d1 is within __ % of d2, then include otherwise don't\n",
    "#         Parent_dp = dataset[Parent]\n",
    "#         d1 = np.linalg.norm(Parent_dp - Inc_Dp)\n",
    "#         # Getting the Parent's parent idx\n",
    "#         Parents_parent_dp = None   # If the parent is a root no need to check\n",
    "#         for key in Tree_Structure:\n",
    "#             if Parent in set(Tree_Structure[key]):\n",
    "#                 Parents_parent_idx = key\n",
    "#                 Parents_parent_dp = dataset[Parents_parent_idx]\n",
    "#                 break\n",
    "#         if Parents_parent_dp is not None:\n",
    "#             d2 = np.linalg.norm(Parent_dp - Parents_parent_dp)\n",
    "#             # print(\"d1: \", d1, \"d2: \", d2)\n",
    "        \n",
    "        if numerator/denominator <= thresh:  ################## and (Parents_parent_dp is None or (d2/d1)>0.7):\n",
    "            labels[Inc_Dp_Idx] = labels[Sibling_node]\n",
    "            # Add this newly added nodes information to the existing Hash_Map\n",
    "            #            0                1         2        3                    4\n",
    "            # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "            if len(rng[0][0]) == 0:\n",
    "                Radius_new_node = 0\n",
    "            else:\n",
    "                Radius_new_node = max(rng[0][0])\n",
    "            NewNode = [Density_Inc_Dp, Inc_Dp, Radius_new_node, rng[1][0], Inc_Dp_Idx]\n",
    "            temp_dict = {NewNode[-1]:NewNode}\n",
    "            Hash_Map.update(temp_dict)\n",
    "\n",
    "            # Store Adaptive Radius and New Weighted Density of all ansectors including the New Node\n",
    "            # print(\"IDX: \", NewNode[4])\n",
    "            list_of_parents = Store_Parents[Parent] + [Density_Inc_Dp] \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "            Store_Radius_Density[NewNode[4]] = [NewNode[2], weighted_density]\n",
    "            Store_Parents[NewNode[4]] = list_of_parents\n",
    "            Store_Self_Density[NewNode[4]] = Density_Inc_Dp\n",
    "\n",
    "            # Add the New Node to the Tree Structure\n",
    "            if Parent not in Tree_Structure:\n",
    "                Tree_Structure[Parent] = [NewNode[4]]\n",
    "            else:\n",
    "                Tree_Structure[Parent].append(NewNode[4])\n",
    "            # print(\"CHILD OF SIBLING\")\n",
    "            \n",
    "            # Check for duplicate poistions\n",
    "            Tree_Structure = RemoveDulplicatePositions(Tree_Structure, Store_Self_Density)\n",
    "            Tree_Structure = CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density)\n",
    "            \n",
    "            print(\"----------------------------------Rearranging2----------------------------------\")\n",
    "            cluster_num = len(Roots)\n",
    "            # dataset.append(Inc_Dp)\n",
    "            RearrangeTreeStructure(Tree_Structure, dataset, labels, Store_Self_Density, cluster_num, Incoming_points, Roots)\n",
    "            \n",
    "            break\n",
    "        # print(\"NOT CHILD OF SIBLING\")\n",
    "        \n",
    "\n",
    "        # Set as Child of Childs of Sibling\n",
    "        boolean = False\n",
    "        Childs_of_Sibling_node = deque()\n",
    "        if Sibling_node in Tree_Structure:\n",
    "            for childnode in Tree_Structure[Sibling_node]:\n",
    "                Childs_of_Sibling_node.appendleft(childnode)  # Leftward addition so that \n",
    "                                                              # all childs of one layer are processed\n",
    "                    \n",
    "            # Creating a set to prevent multiple checking on the same node\n",
    "            nodes_visited = set()\n",
    "            # Check the density criterion with each child\n",
    "            while Childs_of_Sibling_node:\n",
    "                childnode = Childs_of_Sibling_node.pop()\n",
    "                # print(\"Ch_ND: \", childnode)\n",
    "                Parent = childnode\n",
    "                if childnode in Store_Radius_Density:  ## I added this\n",
    "                    wt_density_ancestors = Store_Radius_Density[childnode][1]\n",
    "                    numerator = abs(wt_density_ancestors - Density_Inc_Dp)\n",
    "                    denominator = wt_density_ancestors\n",
    "                    # print(\"DIP3: \", Density_Inc_Dp, \"N: \", numerator, \"D: \", denominator, \"Frac: \", numerator/denominator)\n",
    "\n",
    "                    # Calculate the distance with parent(d1). Calculate the distance of the parent with its parent(d2).\n",
    "                    # If d1 is within __ % of d2, then include otherwise don't\n",
    "#                     Parent_dp = dataset[Parent]\n",
    "#                     d1 = np.linalg.norm(Parent_dp - Inc_Dp)\n",
    "#                     # Getting the Parent's parent idx\n",
    "#                     Parents_parent_dp = None   # If the parent is a root no need to check\n",
    "#                     for key in Tree_Structure:\n",
    "#                         if Parent in set(Tree_Structure[key]):\n",
    "#                             Parents_parent_idx = key\n",
    "#                             Parents_parent_dp = dataset[Parents_parent_idx]\n",
    "#                             break\n",
    "#                     if Parents_parent_dp is not None:\n",
    "#                         d2 = np.linalg.norm(Parent_dp - Parents_parent_dp)\n",
    "#                         # print(\"d1: \", d1, \"d2: \", d2)\n",
    "\n",
    "                    if numerator/denominator <= thresh: ################# and (Parents_parent_dp is None or (d2/d1)>0.7):\n",
    "                        labels[Inc_Dp_Idx] = labels[childnode]\n",
    "                        # Add this newly added nodes information to the existing Hash_Map\n",
    "                        #            0                1         2        3                    4\n",
    "                        # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "                        if len(rng[0][0]) == 0:\n",
    "                            Radius_new_node = 0\n",
    "                        else:\n",
    "                            Radius_new_node = max(rng[0][0])\n",
    "                        NewNode = [Density_Inc_Dp, Inc_Dp, Radius_new_node, rng[1][0], Inc_Dp_Idx]\n",
    "                        temp_dict = {NewNode[-1]:NewNode}\n",
    "                        Hash_Map.update(temp_dict)\n",
    "\n",
    "                        # Store Adaptive Radius and New Weighted Density of all ansectors including the New Node\n",
    "                        # print(\"IDX: \", NewNode[4])\n",
    "                        list_of_parents = Store_Parents[Parent] + [Density_Inc_Dp] \n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        Store_Radius_Density[NewNode[4]] = [NewNode[2], weighted_density]\n",
    "                        Store_Parents[NewNode[4]] = list_of_parents\n",
    "                        Store_Self_Density[NewNode[4]] = Density_Inc_Dp\n",
    "\n",
    "                        # Add the New Node to the Tree Structure\n",
    "                        if Parent not in Tree_Structure:\n",
    "                            Tree_Structure[Parent] = [NewNode[4]]\n",
    "                        else:\n",
    "                            Tree_Structure[Parent].append(NewNode[4])\n",
    "                        # print(\"CHILD OF CHILDS OF SIBLING\")\n",
    "\n",
    "                        # Check for duplicate poistions\n",
    "                        Tree_Structure = RemoveDulplicatePositions(Tree_Structure, Store_Self_Density)\n",
    "                        Tree_Structure = CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density)\n",
    "\n",
    "                        print(\"----------------------------------Rearranging3----------------------------------\")\n",
    "                        cluster_num = len(Roots)\n",
    "                        # dataset.append(Inc_Dp)\n",
    "                        RearrangeTreeStructure(Tree_Structure, dataset, labels, Store_Self_Density, cluster_num, \\\n",
    "                                               Incoming_points, Roots)\n",
    "\n",
    "                        boolean = True\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        if childnode not in nodes_visited and childnode in Tree_Structure:\n",
    "                            for new_child_node in Tree_Structure[childnode]:\n",
    "                                Childs_of_Sibling_node.appendleft(childnode)  # Leftward addition so that all\n",
    "                                                                              # childs of one layer are processed\n",
    "                            # print(\"CSN: \", Childs_of_Sibling_node)\n",
    "                            nodes_visited.add(childnode)  \n",
    "\n",
    "                            \n",
    "        ##  We have already checked whether it can be its sibling's child. So no need to check again\n",
    "        \n",
    "                            \n",
    "                            \n",
    "            if boolean == True:\n",
    "                break\n",
    "        #     else:\n",
    "        #         print(\"NOT CHILD OF CHILDS OF SIBLING\")\n",
    "        # else:\n",
    "        #     print(\"NOT CHILD OF CHILDS OF SIBLING\")\n",
    "            \n",
    "        # If we reach here, it means the incoming point is a possible anomaly\n",
    "        # Assign its label as -1 and terminate the loop\n",
    "        labels[Inc_Dp_Idx] = -1\n",
    "        break\n",
    "    \n",
    "    return Store_Radius_Density, Store_Self_Density#, dataset\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def Updatedistance_ddcalandindices_ddcal(data1, algo):\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "    return distances_ddcal, indices_ddcal\n",
    "\n",
    "\n",
    "\n",
    "# LLE: Locally linear Embedding\n",
    "def LLE_Individual_Datapoint(data_pt, K, dp_idx, total_num_pts, data): #, data_point_Idx):\n",
    "    row=total_num_pts\n",
    "    #print(row)\n",
    "    dimension=len(data_pt)\n",
    "    #print(dimension)\n",
    "    neighbors= K\n",
    "\n",
    "    hash_map = {value:np.array([]) for value in range(0, 1)}\n",
    "\n",
    "    # for i in range(row):\n",
    "    D_i=np.array(data-data[dp_idx, :])\n",
    "    #print(D_i)\n",
    "    distance=(D_i**2).sum(1)\n",
    "    #print(distance)\n",
    "    nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "    #print(nearest_neighbor)\n",
    "    D_nbrs=D_i[nearest_neighbor, :]\n",
    "    #print(D_nbrs)\n",
    "    ##Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "    Q=np.matmul(D_nbrs, D_nbrs.T)\n",
    "    #print(Q)\n",
    "#         t=np.trace(Q)\n",
    "#         r=0.001*t\n",
    "    r = 0.001 * float(Q.trace())\n",
    "    if(neighbors>=dimension):\n",
    "#             sig2 = (np.linalg.svd(D_i,compute_uv=0))**2\n",
    "#             r = np.sum(sig2[dimension:])\n",
    "        Q=Q+(r*np.identity(neighbors))\n",
    "#             Q.flat[::neighbors+1] += r\n",
    "    ##w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "    w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "\n",
    "    w = w/sum(w)\n",
    "    \n",
    "    temp = []\n",
    "    for ele in w:\n",
    "        temp.append(ele)\n",
    "    temp.sort(reverse=True)\n",
    "    # print(\"w: \", temp)\n",
    "    \n",
    "    hash_map[0] = np.array(temp) \n",
    "\n",
    "    \n",
    "\n",
    "    per = 5 #0.6 #0.3 #0.5 #0.2 #0.05 #0.1\n",
    "\n",
    "    for j in range(1):\n",
    "        count = 0\n",
    "        for i in range(1, len(hash_map[j])):\n",
    "            #print(((1/neighbors) - (dec_per*(1/neighbors))), ((1/neighbors) + (dec_per*(1/neighbors))))\n",
    "            if round((abs(hash_map[j][i-1] - hash_map[j][i])/abs(hash_map[j][i-1]))*100,1) <= per:\n",
    "                count += 1\n",
    "\n",
    "\n",
    "    return count+1              # +1 because all neighbouring algorithms consider the datapoint itself to be a part\n",
    "                                # of its neighbour\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "def GetNeighbours(inlier, data1, K):\n",
    "    dist_idx = []\n",
    "    for i in range(len(data1)):\n",
    "        comparison = data1[i] == inlier\n",
    "        equal_arrays = comparison.all()\n",
    "        if equal_arrays == False: #True:\n",
    "            # print(data1[i], inlier)\n",
    "            dist_idx.append([np.linalg.norm(inlier - data1[i]), i])   #math.dist(inlier, data1[i]), i])\n",
    "    dist_idx.sort(key=operator.itemgetter(0))\n",
    "    idxs = [row[1] for row in dist_idx][:K]\n",
    "    return idxs\n",
    "    \n",
    "def GetNeighbourAnomalies(data_pt_idx, LLE_lookup, data1):\n",
    "    opt_k = LLE_lookup[data_pt_idx]\n",
    "    nearest_neighbour_idxs = GetNeighbours(data1[data_pt_idx], data1, opt_k)\n",
    "    \n",
    "    neighbour_anomalies = []\n",
    "    idxs = []\n",
    "    for neighbour_idx in nearest_neighbour_idxs:\n",
    "        if labels[neighbour_idx] == -1:\n",
    "            idxs.append(neighbour_idx)\n",
    "            neighbour_anomalies.append(data1[neighbour_idx])\n",
    "    # print(\"Neighbour Anomaly indexes: \", idxs)\n",
    "    return neighbour_anomalies\n",
    "\n",
    "\n",
    "def getanomalyindexes(anomaly_list, data1):\n",
    "    anomaly_idxs = []\n",
    "    for anomaly in anomaly_list:\n",
    "        for i in range(len(data1)):\n",
    "            comparison = data1[i] == anomaly\n",
    "            equal_arrays = comparison.all()\n",
    "            if equal_arrays == True:\n",
    "                anomaly_idxs.append(i)\n",
    "                break\n",
    "    return anomaly_idxs\n",
    "\n",
    "\n",
    "def GetParentfromTree_F1(Tree_Structure, idx):\n",
    "    send_key = None\n",
    "    for key in Tree_Structure:\n",
    "        if idx in set(Tree_Structure[key]):\n",
    "            send_key = key\n",
    "            break\n",
    "    if send_key is None:\n",
    "        return send_key\n",
    "    else:\n",
    "        return int(send_key)\n",
    "        \n",
    "\n",
    "def GetCardinality_F(labels, idx):\n",
    "    cardinality = 0\n",
    "    label_idx = labels[idx]\n",
    "    for key in labels:\n",
    "        if labels[key] == label_idx:\n",
    "            cardinality += 1\n",
    "    return cardinality\n",
    "\n",
    "\n",
    "def ChangeAllLabels_F(labels, Tree_Structure, idx, changetolabel):\n",
    "    ToChange = deque()\n",
    "    ToChange.append(idx)\n",
    "    visited = {key:False for key in labels}\n",
    "    while True:\n",
    "        # print(\"ToChange: \", ToChange)\n",
    "        if len(ToChange) == 0:\n",
    "            break\n",
    "        curr_idx = ToChange.pop()\n",
    "        labels[curr_idx] = changetolabel\n",
    "        if curr_idx in Tree_Structure:\n",
    "            for child_idx in Tree_Structure[curr_idx]:\n",
    "                if child_idx not in set(ToChange) and visited[child_idx] == False:\n",
    "                    ToChange.appendleft(child_idx)\n",
    "                    visited[child_idx] = True\n",
    "                    \n",
    "    # return labels\n",
    "\n",
    "\n",
    "def GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, data1):\n",
    "    opt_k = LLE_lookup[Inc_Dp_Idx]\n",
    "    nearest_neighbour_idxs = GetNeighbours(data1[Inc_Dp_Idx], data1, opt_k)\n",
    "    \n",
    "    neighbour_idx_of_different_clusters = []\n",
    "    for neighbour_idx in nearest_neighbour_idxs:\n",
    "        if labels[neighbour_idx] != -1 and labels[neighbour_idx] != labels[Inc_Dp_Idx]:\n",
    "            neighbour_idx_of_different_clusters.append(neighbour_idx)\n",
    "            # neighbour_anomalies.append(data1[neighbour_idx])\n",
    "    # print(\"Neighbour Anomaly indexes: \", idxs)\n",
    "    return neighbour_idx_of_different_clusters\n",
    "    \n",
    "\n",
    "    \n",
    "def GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx, data1):\n",
    "    parentsofIncDpincludingself = deque()\n",
    "    parentsofIncDpincludingself.appendleft(Store_Self_Density[Inc_Dp_Idx])\n",
    "    parentlist = deque()\n",
    "    parentlist.append(Inc_Dp_Idx)\n",
    "    visited = {i:False for i in range(len(data1))}\n",
    "    while parentlist:\n",
    "        # print(\"PL: \", parentlist)\n",
    "        curr_dp_idx = parentlist.pop()\n",
    "        if visited[curr_dp_idx] == True:\n",
    "          continue\n",
    "        else:\n",
    "          visited[curr_dp_idx] = True\n",
    "          for key in Tree_Structure:\n",
    "              if curr_dp_idx in set(Tree_Structure[key]):\n",
    "                  parentlist.append(key)\n",
    "                  parentsofIncDpincludingself.appendleft(Store_Self_Density[key])\n",
    "                  break\n",
    "    # print(\"RETURNING PARENTLIST\")\n",
    "    return list(parentsofIncDpincludingself)\n",
    "\n",
    "    \n",
    "\n",
    "def RunStreamingData(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                     Store_Parents, Hash_Map, thresh, anomaly_list, countimg, Store_Self_Density, cluster_num,\n",
    "                    Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "                    algo, alpha_weight_avg, incoming_time_dur):\n",
    "     \n",
    "    thresh_5 = 10 #(for corners)\n",
    "    percent7 = 0.05  #0.5 #0.3\n",
    "    Sibling_nodes = []\n",
    "    total_num_pts = len(data1)\n",
    "    # Check if Incoming point was a potential anomaly and now an inlier\n",
    "    # In such a scenario just skip that data point\n",
    "    \n",
    "    # Initialize curr_waiting_time as 0 as there is no waiting for the very first incoming datapoint\n",
    "    curr_waiting_time = 0\n",
    "    # Initialize the Total_Time as 0\n",
    "    Total_Time = 0\n",
    "    \n",
    "    \n",
    "    for Inc_Dp in Incoming_points:\n",
    "        print(\"********************************DATAPOINT INCOMING*****************************\")\n",
    "        Bool = False\n",
    "        \n",
    "        # print(\"Here F100: \", Tree_Structure)\n",
    "        \n",
    "        for i in range(len(data1)):\n",
    "            item = data1[i]\n",
    "            comparison = item == Inc_Dp\n",
    "            equal_arrays = comparison.all()\n",
    "            if equal_arrays == True:\n",
    "                Bool = True\n",
    "                hold_idx = i\n",
    "                break\n",
    "        if Bool == False:\n",
    "            # This is a new data point so calculate its processing time\n",
    "            pro_start_time = time.time()\n",
    "            \n",
    "            data1 = np.append(data1, [Inc_Dp], axis=0) # Adding Data Point to the dataset\n",
    "            Inc_Dp_Idx = len(data1)-1                  # Assigning index number of new Data point  \n",
    "            labels[Inc_Dp_Idx] = 0                     # Assigning new data point with label 0\n",
    "            \n",
    "        else:\n",
    "            Inc_Dp_Idx = hold_idx                             # Assigning index number of already seen Data point\n",
    "        \n",
    "        if (Inc_Dp_Idx in labels and labels[Inc_Dp_Idx] <= 0):\n",
    "            labels[Inc_Dp_Idx] = 0                            # Assigning anmalous/new data point with label 0\n",
    "\n",
    "            # print(\"Incoming Data point Idx: \", Inc_Dp_Idx)\n",
    "\n",
    "            # Get all roots in increasing order of their distance with the incoming data point\n",
    "            idx_dist_with_roots = GetNearestRoot(Roots, Inc_Dp)\n",
    "            # print(\"idx_dist_with_roots: \", idx_dist_with_roots)\n",
    "\n",
    "            i = 0\n",
    "            limit = 1\n",
    "            # print(\"i: \", i, \"idx_dist_with_roots: \", idx_dist_with_roots)\n",
    "\n",
    "#             while i < len(idx_dist_with_roots) and i < limit and labels[Inc_Dp_Idx] <= 0:\n",
    "\n",
    "                # print(\"i: \", i)\n",
    "            Root_node_idx = idx_dist_with_roots[i][0]\n",
    "            # print(\"Closest Root of\", Inc_Dp, \"is:\", Roots[Root_node_idx][0])\n",
    "            sibling_node_idx, ImmediateAncestorNode_idx = GetSiblingNode(Inc_Dp, Roots, Root_node_idx, data1, Tree_Structure)\n",
    "            # print(\"Sibling Node of \", Inc_Dp, \"is:\", data1[sibling_node_idx])\n",
    "            Sibling_nodes.append(data1[sibling_node_idx])\n",
    "\n",
    "            # print(\"ImmediateAncestorNode of Sibling is: \", data1[ImmediateAncestorNode_idx])\n",
    "            Store_Radius_Density, Store_Self_Density = IncludeWithinTree(Inc_Dp, Inc_Dp_Idx, sibling_node_idx, ImmediateAncestorNode_idx, Store_Radius_Density,\n",
    "                              Tree_Structure, data1, Hash_Map, labels, thresh, Store_Parents, Roots)\n",
    "#                 i += 1\n",
    "                # print(\"\\n\")\n",
    "                ## print(\"i: \", i, \"idx_dist_with_roots: \", idx_dist_with_roots)\n",
    "\n",
    "        \n",
    "        \n",
    "            ### I Commented this - Since these parameters are not used in this function\n",
    "            ## Update distances_ddcal and indices_ddcal\n",
    "            #distances_ddcal, indices_ddcal = Updatedistance_ddcalandindices_ddcal(data1, algo)\n",
    "            ###print(len(distances_ddcal), len(indices_ddcal))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Get Pruned k for the new data point\n",
    "            opt_k = LLE_Individual_Datapoint(Inc_Dp, K, Inc_Dp_Idx, total_num_pts, data1)\n",
    "            ## print(\"Opt K: \", opt_k)\n",
    "            LLE_lookup[Inc_Dp_Idx] = opt_k\n",
    "            ## print(len(LLE_lookup))\n",
    "            \n",
    "\n",
    "            ## For saving Individual plots\n",
    "            # if labels[Inc_Dp_Idx] == -1:\n",
    "            #     countimg = Plot(countimg)\n",
    "            \n",
    "\n",
    "            if labels[Inc_Dp_Idx] != -1:    \n",
    "                \n",
    "                # For the point that is just labelled, compare its distance with its present parent and its neighbours \n",
    "                # which belong to some different cluster. If the distance and density criteria satisfies,\n",
    "                # include it to that neighbour cluster and break\n",
    "                parent_of_curr_pt_idx = GetParentfromTree_F1(Tree_Structure, Inc_Dp_Idx)\n",
    "                neighbour_idx_of_different_clusters = GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, data1)\n",
    "                print(\"--> ASSESSING WITH ITS NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                for neighbour_idx in neighbour_idx_of_different_clusters:\n",
    "\n",
    "                    neighbour_dp = data1[neighbour_idx]\n",
    "                    # If the current point has become a root node\n",
    "                    if parent_of_curr_pt_idx == None: \n",
    "                        # print(\"--> ROOT OBTAINED!! with idx: \", neighbour_idx, \"and label:\", labels[neighbour_idx])\n",
    "                        \n",
    "                        # If root is obtained, check the density of the root with the probable parent.\n",
    "                        # It is to be done similar to the method of inclusion of a data point (same threshold)\n",
    "                        # If the root satisfies the density criterion, include it otherwise don't\n",
    "                        \n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx, data1)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                        \n",
    "                        # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                        cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                        if cardinality_curr_cluster <= 8 and numerator/denominator <= thresh:\n",
    "                            # cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                            # cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                            # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                            # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                            #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, Inc_Dp_Idx, labels[neighbour_idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "                            if neighbour_idx in Tree_Structure:\n",
    "                                Tree_Structure[neighbour_idx].append(Inc_Dp_Idx)\n",
    "                            else:\n",
    "                                Tree_Structure[neighbour_idx] = [Inc_Dp_Idx]\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "                    else:\n",
    "                        # If the current point is not a root Node\n",
    "                        \n",
    "                        if type(parent_of_curr_pt_idx) != int:\n",
    "                            raise Exception(\"More than 1 idx present(1)\")\n",
    "                        parent_of_curr_pt = data1[parent_of_curr_pt_idx]\n",
    "                        # parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, Inc_Dp_Idx)\n",
    "                        # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "                        # Not the child's parent but check with the child itself.\n",
    "\n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx, data1)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "\n",
    "                        # Find distances with the present parent and the probable parent.\n",
    "                        dist_orig_parent = np.linalg.norm(Inc_Dp - parent_of_curr_pt)\n",
    "                        dist_prob_parent = np.linalg.norm(Inc_Dp - neighbour_dp)\n",
    "                        \n",
    "                        # Find density differences with the present parent and the probable parent.\n",
    "                        # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "                        # density_diff_with_prob_parent = Store_Self_Density[Inc_Dp_Idx] - Store_Self_Density[neighbour_idx]\n",
    "                        \n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                        \n",
    "                        ## print(\"Idx: \", neighbour_idx, \"Label: \", labels[neighbour_idx], \"dist_orig_parent: \", dist_orig_parent, \"dist_prob_parent: \", dist_prob_parent)\n",
    "                        if (dist_orig_parent)*0.3 <= (dist_prob_parent) and numerator/denominator > thresh: ## and (dist_orig_parent) <= thresh_5:\n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                            # Everything is fine. Nothing needs to be changed\n",
    "                            # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                            # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                            continue\n",
    "\n",
    "                        else:\n",
    "                            # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                            # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                            #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                            # Plot to check whats happening\n",
    "                            # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", Inc_Dp)\n",
    "                            # PlotF2(Inc_Dp, parent_of_curr_pt, neighbour_dp, Dimension, cluster_num, data1, \\\n",
    "                            #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "\n",
    "                            if neighbour_idx in Tree_Structure:\n",
    "                                Tree_Structure[neighbour_idx].append(Inc_Dp_Idx)\n",
    "                            else:\n",
    "                                Tree_Structure[neighbour_idx] = [Inc_Dp_Idx]\n",
    "                            if parent_of_curr_pt_idx != None:\n",
    "                                Tree_Structure[parent_of_curr_pt_idx].remove(Inc_Dp_Idx)\n",
    "\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, Inc_Dp_Idx, labels[neighbour_idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "                            break\n",
    "\n",
    "                print(\"--> COMPLETED ASSESSING WITH ITS NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Similar to above, find neighbours which are assigned to some other cluster\n",
    "                # Check the distance between them and their parent and them and probable parent\n",
    "                # Assign point to the closer parent\n",
    "                # Change labels of that point and its child nodes and add them to the Tree Structure of the curr_pt\n",
    "                # What to do if neighbour is a Root --> Check Cardinality. If very small include it\n",
    "                neighbour_idx_of_different_clusters = GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, data1)\n",
    "                print(\"--> ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                for neighbour_idx in neighbour_idx_of_different_clusters:\n",
    "\n",
    "                    neighbour_dp = data1[neighbour_idx]\n",
    "                    parent_of_neighbour_idx = GetParentfromTree_F1(Tree_Structure, neighbour_idx)\n",
    "                    \n",
    "                    # If the neighbour is a root node\n",
    "                    if parent_of_neighbour_idx == None: # or parent_of_curr_dp_idx == None:\n",
    "                        # print(\"--> ROOT OBTAINED!! with idx: \", neighbour_idx, \"and label:\", labels[neighbour_idx])\n",
    "                        \n",
    "                        # If root is obtained, check the density of the root with the probable parent.\n",
    "                        # It is to be done similar to the method of inclusion of a data point (same threshold)\n",
    "                        # If the root satisfies the density criterion, include it otherwise don't\n",
    "                        \n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx, data1)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_neighbour)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                        \n",
    "                        # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                        cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                        if cardinality_neighbour_cluster <= 8 and numerator/denominator <= thresh:\n",
    "                            # cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                            # cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                            # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                            # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                            #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "                            if Inc_Dp_Idx in Tree_Structure:\n",
    "                                Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "                            else:\n",
    "                                Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "                    \n",
    "                    \n",
    "                    else:\n",
    "                        # If the neighbour is not a root Node\n",
    "                        \n",
    "                        if type(parent_of_neighbour_idx) != int:\n",
    "                            raise Exception(\"More than 1 idx present(1)\")\n",
    "                        parent_of_neighbour = data1[parent_of_neighbour_idx]\n",
    "                        # parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, Inc_Dp_Idx)\n",
    "                        # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "                        # Not the child's parent but check with the child itself.\n",
    "\n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx, data1)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_neighbour)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "\n",
    "                        # Find distances with the present parent and the probable parent.\n",
    "                        dist_orig_parent = np.linalg.norm(neighbour_dp - parent_of_neighbour)\n",
    "                        dist_prob_parent = np.linalg.norm(neighbour_dp - Inc_Dp)\n",
    "                        \n",
    "                        # Find density differences with the present parent and the probable parent.\n",
    "                        # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "                        # density_diff_with_prob_parent = Store_Self_Density[Inc_Dp_Idx] - Store_Self_Density[neighbour_idx]\n",
    "                        \n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                        \n",
    "                        ## print(\"Idx: \", neighbour_idx, \"Label: \", labels[neighbour_idx], \"dist_orig_parent: \", dist_orig_parent, \"dist_prob_parent: \", dist_prob_parent)\n",
    "                        if (dist_orig_parent) <= (dist_prob_parent) or numerator/denominator > thresh: ## and (dist_orig_parent) <= thresh_5:\n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                            # Everything is fine. Nothing needs to be changed\n",
    "                            # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                            # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                            continue\n",
    "\n",
    "                        else:\n",
    "                            # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                            # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                            #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                            # Plot to check whats happening\n",
    "                            # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", Inc_Dp)\n",
    "                            # PlotF2(neighbour_dp, parent_of_neighbour, Inc_Dp, Dimension, cluster_num, data1, \\\n",
    "                            #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "\n",
    "                            if Inc_Dp_Idx in Tree_Structure:\n",
    "                                Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "                            else:\n",
    "                                Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "                            if parent_of_neighbour_idx != None:\n",
    "                                Tree_Structure[parent_of_neighbour_idx].remove(neighbour_idx)\n",
    "\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "\n",
    "                print(\"--> COMPLETED ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                \n",
    "                \n",
    "                ## Assess the Anomaly neighbour points\n",
    "                neighbour_anomalies = GetNeighbourAnomalies(Inc_Dp_Idx, LLE_lookup, data1)\n",
    "                print(\"--> ASSESSING POTENTIAL ANOMALIES\")\n",
    "                # data1, Roots\n",
    "                data1, Roots, Tree_Structure, Store_Radius_Density, Store_Parents, Store_Self_Density,  cluster_centers, _ = RunStreamingData(data1, \n",
    "                         neighbour_anomalies, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                         Store_Parents, Hash_Map, thresh, anomaly_list, countimg, Store_Self_Density, cluster_num,\n",
    "                                        Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, \n",
    "                                         cluster_centers, algo, alpha_weight_avg, incoming_time_dur)\n",
    "                print(\"--> COMPLETED ASSESSING POTENTIAL ANOMALIES\")\n",
    "                print(\"----------------------------------Rearranging4----------------------------------\")\n",
    "                RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Incoming_points, Roots)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "            # print(\"Incoming Data point Idx: \", Inc_Dp_Idx)\n",
    "            # print(\"ALREADY LABELLED AS INLIER SO SKIPPED!!!!\")\n",
    "            \n",
    "        \n",
    "        ## For saving Individual plots\n",
    "        # if labels[Inc_Dp_Idx] == -1:\n",
    "        #     countimg = Plot(countimg)\n",
    "\n",
    "\n",
    "        # total_num_pts += 1\n",
    "        # Update the data1 as new point enters\n",
    "        # print(\"Label of\", Inc_Dp, Inc_Dp_Idx, \"is: \", labels[Inc_Dp_Idx])\n",
    "        \n",
    "        \n",
    "        anomaly_idxs = getanomalyindexes(anomaly_list, data1)\n",
    "        for label in labels:\n",
    "            if labels[label] == -1 and (label not in set(anomaly_idxs)):\n",
    "                anomaly_idxs.append(label)\n",
    "        for anomaly_idx in anomaly_idxs:\n",
    "            if labels[anomaly_idx] != -1:\n",
    "                anomaly_idxs.remove(anomaly_idx)\n",
    "                \n",
    "                \n",
    "                \n",
    "        print(\"Current Number of Anomalies: \", len(anomaly_idxs))\n",
    "        if len(anomaly_idxs) >= Concept_Evolution_Thresh:\n",
    "            print(\"*************************PERFORMING CONCEPT EVOLUTION**************************\")\n",
    "            ## data1, Roots, cluster_centers, labels, updated_anomaly_dps, updated_anomaly_idxs, Tree_Structure, \\\n",
    "            ## Store_Self_Density, cluster_num, Store_Radius_Density, Store_Parents, Hash_Map = \n",
    "            data1, Roots, cluster_centers, labels, updated_anomaly_dps, updated_anomaly_idxs, Tree_Structure, \\\n",
    "            Store_Self_Density, cluster_num, Store_Radius_Density, Store_Parents, Hash_Map = \\\n",
    "            RunConceptEvolution(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                         Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "                        Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "                       algo, alpha_weight_avg)\n",
    "            print(\"********************COMPLETED PERFORMING CONCEPT EVOLUTION*********************\")\n",
    "            \n",
    "            \n",
    "        # clear_output(wait=True)    \n",
    "        \n",
    "        # Plotting just to see\n",
    "        #### FinalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n",
    "        \n",
    "        # Check which are the nodes having no parents. Should be only the roots, if any more then that's wrong\n",
    "        Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "        # Check if cluster_num is same as the Supposed_Roots: if not then this function corrects it.\n",
    "        # I am considering that len(Supposed_Roots) > or = cluster_num. Is this correct?\n",
    "        if len(Supposed_Roots) != cluster_num:\n",
    "          Tree_Structure = MakeSupposedRootsandClusternumsamelength(Supposed_Roots, labels, Tree_Structure, Store_Self_Density)\n",
    "          Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "\n",
    "        Roots = AddRootsFromSupposedRoots(Supposed_Roots, Roots, data1, Store_Self_Density)\n",
    "        # print(\"Supposed Roots: \", Supposed_Roots)\n",
    "        if len(list(Supposed_Roots)) > len(Roots):\n",
    "            pass\n",
    "            # print(\"Error at RunStreamingData\")\n",
    "            # print(\"\\nTree_Structure: \\n\", Tree_Structure)\n",
    "            # raise Exception(\"Number of Supposed Roots has become more than the actual number of roots\")\n",
    "        \n",
    "        # PlotF1(Inc_Dp, Inc_Dp_Idx, Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n",
    "        \n",
    "        \n",
    "        print(\"******************************DATAPOINT PROCESSED******************************\")\n",
    "        if Bool == False:\n",
    "            print(\"Total Datapoints now: \", len(data1))\n",
    "            \n",
    "            # Processing Time:\n",
    "            # This was a new data point and we need to calculate its processing time\n",
    "            processing_time = time.time() - pro_start_time\n",
    "            \n",
    "            # Waiting time:\n",
    "            # For the very first incoming point the waiting time is 0\n",
    "            # Once processed, we check the processing time --> If that Processing time > Incoming_time of the new\n",
    "            # point, then that additional time is going to become the waiting time of the new point\n",
    "            if processing_time > incoming_time_dur:\n",
    "                waiting_time = processing_time - incoming_time_dur\n",
    "            \n",
    "            # Total Time: Update the total time as processing_time + waiting_time\n",
    "            Curr_Total = processing_time + curr_waiting_time\n",
    "            Total_Time += Curr_Total\n",
    "            \n",
    "            # Update the waiting time --> as this is for the next incoming data point\n",
    "            curr_waiting_time = waiting_time\n",
    "            \n",
    "            # Display\n",
    "            print(\"-----------------------------Time Taken: {} seconds----------------------------\".format(Curr_Total))\n",
    "            print(\"\\n\")\n",
    "\n",
    "     \n",
    "    # try:\n",
    "    #     if len(anomaly_idxs) >= Concept_Evolution_Thresh:\n",
    "    #         print(\"*************************PERFORMING CONCEPT EVOLUTION**************************\")\n",
    "    #         data1, Roots, cluster_centers, labels, updated_anomaly_dps, updated_anomaly_idxs, Tree_Structure, \\\n",
    "    #         Store_Self_Density, cluster_num, Store_Radius_Density, Store_Parents, Hash_Map = RunConceptEvolution(data1, \n",
    "    #         Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "    #                      Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "    #                     Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "    #                    algo, alpha_weight_avg)\n",
    "    #         print(\"********************COMPLETED PERFORMING CONCEPT EVOLUTION*********************\")\n",
    "\n",
    "    # except:\n",
    "    #     pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    # print(\"!!!!!!!!!!CURRENT CLUSTER LENGTH: \", cluster_num)\n",
    "    # print(\"!!!!!!!!!!CURRENT ROOT LENGTH: \", len(Roots))\n",
    "    # return data1, Roots\n",
    "    return data1, Roots, Tree_Structure, Store_Radius_Density, Store_Parents, Store_Self_Density,  cluster_centers, Total_Time\n",
    "#     try:\n",
    "#         return data1, anomaly_idxs\n",
    "#     except:\n",
    "#         return data1, [] \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEUASEezoPPD",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKBOa72BoPPD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # def ConceptEvolutionOne(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "# #                          Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "# #                         Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "# #                     algo, alpha_weight_avg):\n",
    "#     countimg = [0]\n",
    "#     # data1, anomaly_idxs = RunStreamingData(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "#     #                      Store_Parents, Hash_Map, thresh, anomaly_list, countimg, Store_Self_Density, cluster_num)\n",
    "\n",
    "#     data1 = RunStreamingData(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "#                          Store_Parents, Hash_Map, thresh, anomaly_list, countimg, Store_Self_Density, cluster_num,\n",
    "#                             Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "#                     algo, alpha_weight_avg)\n",
    "    \n",
    "# #     return data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjd0u1J_oPPD"
   },
   "outputs": [],
   "source": [
    "# Get current Anomaly_idxs and Anomaly_dps\n",
    "def GetCurrentAnomalyIdxsandDps(labels, data1):\n",
    "    anomaly_idxs = []\n",
    "    anomaly_dps = []\n",
    "    for key in labels:\n",
    "        if labels[key] == -1:\n",
    "            anomaly_idxs.append(key)\n",
    "            anomaly_dps.append(data1[key])\n",
    "    \n",
    "    anomaly_dict = dict() # Anomaly_dict --> Anomaly_idx:actual_idx\n",
    "    for i in range(len(anomaly_idxs)):\n",
    "        anomaly_dict[i] = anomaly_idxs[i]\n",
    "        \n",
    "    return anomaly_idxs, anomaly_dps, anomaly_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD2aj1DaoPPE"
   },
   "outputs": [],
   "source": [
    "def PlotSeparationAnomalies(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots):\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "\n",
    "        f = plt.figure(figsize=(16,8))\n",
    "        plt1 = f.add_subplot(121)\n",
    "        plt2 = f.add_subplot(122)\n",
    "\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt1.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt1.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt1.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt1.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        for j in range(len(Incoming_points)):\n",
    "            plt1.scatter(Incoming_points[j][0],Incoming_points[j][1],c='black',marker='*',s=150)    \n",
    "        plt1.set_title(\"Datapoints Visualization For Incoming Data Points before Processing\")\n",
    "\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt2.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt2.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt2.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt2.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt2.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt2.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt2.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt2.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "\n",
    "\n",
    "        plt2.set_title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxssEw5HoPPE"
   },
   "outputs": [],
   "source": [
    "def CorrectRootsandClusterCentersandgetCurrentAnomalies(data1, Roots, cluster_centers, labels, Store_Self_Density):\n",
    "    # Correct Roots and Cluster_Centers and get Current Anomalies (both idxs and dps)\n",
    "\n",
    "    # print(\"Data1 length: \", len(data1))\n",
    "\n",
    "    toremovepairs1 = []\n",
    "    idxs_1 = list(Roots.keys())\n",
    "    for i in range(len(idxs_1)-1):\n",
    "        for j in range(i+1, len(idxs_1)):\n",
    "            if labels[idxs_1[i]] == labels[idxs_1[j]]:\n",
    "                toremovepairs1.append([idxs_1[i], idxs_1[j]])\n",
    "\n",
    "    for pair in toremovepairs1:\n",
    "        if Store_Self_Density[pair[0]] < Store_Self_Density[pair[1]]:\n",
    "          try:\n",
    "            del Roots[pair[0]]\n",
    "          except:\n",
    "            pass\n",
    "        else:\n",
    "          try:\n",
    "            del Roots[pair[1]]\n",
    "          except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    for i in range(len(cluster_centers)):\n",
    "        for key in Roots:\n",
    "            if labels[key] == cluster_centers[i][1]:\n",
    "                cluster_centers[i][0] = Roots[key][0].tolist()\n",
    "                break\n",
    "\n",
    "                \n",
    "    def CheckIfSame_1(item1, item2):\n",
    "        comparison = np.array(item1) == np.array(item2)\n",
    "        equals_array = comparison.all()\n",
    "        if equals_array == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    # Remove duplicate values from cluster_centers\n",
    "    def Removeduplicatesfromclustercenters_1(cluster_centers):\n",
    "        new_cluster_centers = []\n",
    "        duplicate_at_idx = set()\n",
    "        for i in range(len(cluster_centers)-1):\n",
    "            boolean_f = False\n",
    "            for j in range(i+1, len(cluster_centers)):\n",
    "                if CheckIfSame_1(cluster_centers[i][0], cluster_centers[j][0]) == True:\n",
    "                    duplicate_at_idx.add(j)\n",
    "        for i in range(len(cluster_centers)):\n",
    "            if i not in duplicate_at_idx:\n",
    "                new_cluster_centers.append(cluster_centers[i])\n",
    "        cluster_centers = new_cluster_centers\n",
    "        return cluster_centers\n",
    "\n",
    "        \n",
    "    if len(cluster_centers) > 1:\n",
    "        cluster_centers = Removeduplicatesfromclustercenters_1(cluster_centers)\n",
    "    \n",
    "    anomaly_idxs, anomaly_dps, anomaly_dict = GetCurrentAnomalyIdxsandDps(labels, data1)\n",
    "\n",
    "\n",
    "    # print(\"Updated Roots: \", Roots)\n",
    "    # print(\"Cluster_Centers: \", cluster_centers)\n",
    "    # print(\"Number of Anomalies: \", len(anomaly_idxs))\n",
    "    \n",
    "    return Roots, cluster_centers, anomaly_idxs, anomaly_dps, anomaly_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xk1N8rSqoPPF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35L26X5noPPF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efTRQ6WsoPPF"
   },
   "outputs": [],
   "source": [
    "# print(labels)\n",
    "# print(Tree_Structure)\n",
    "# print(CheckTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, K=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsE_dcYBoPPF"
   },
   "outputs": [],
   "source": [
    "# len(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_GMBUgVoPPF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DykBdwv5oPPF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# When the total number of amonalies reach a certain threshold, we initiate the new cluster formation algorithm\n",
    "# How it works:\n",
    "# 1. Select the most dense point from the present anomalies and label it as the new root\n",
    "# 2. Build a Tree Structure similar to what we have done before\n",
    "\n",
    "def ReturnIdxs(data1, set_idxs, anomaly_dps, anomaly_dict):\n",
    "    # idx 1 --> anomaly_dps\n",
    "    # Need to convert it to idx in data1\n",
    "    changed_idxs = []\n",
    "    for idx in set_idxs:\n",
    "        changed_idxs.append(anomaly_dict[idx])\n",
    "        \n",
    "#         item = anomaly_dps[idx]\n",
    "#         for i in range(len(data1)):\n",
    "#             comparison = data1[i] == item\n",
    "#             equal_arrays = comparison.all()\n",
    "#             if equal_arrays == True:\n",
    "#                 corresponding_idx = i\n",
    "#                 break\n",
    "#         changed_idxs.append(corresponding_idx)\n",
    "    return changed_idxs\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def FindMostDensePoint(anomaly_dps, algo, Incoming_points, data1, Hash_Map, LLE_lookup, anomaly_idxs, anomaly_dict):\n",
    "    # print(len(anomaly_idxs))\n",
    "    # print(anomaly_dps)\n",
    "    \n",
    "    # anomaly_dps = []\n",
    "    # for idx in anomaly_idxs:\n",
    "    #     try:\n",
    "    #         anomaly_dps.append(data1[idx])\n",
    "    #     except:\n",
    "    #         anomaly_dps.append(Incoming_points[idx-(train_end-train_start)])\n",
    "    # anomaly_dps = np.asarray(anomaly_dps)\n",
    "    \n",
    "    \n",
    "    # Anomaly_dict--> Anomaly_idx: actual_idx\n",
    "    \n",
    "    \n",
    "    # # Calculate the distance of each point with other points in the dataset\n",
    "    nbrs_ddcal_1 = NearestNeighbors(n_neighbors=len(anomaly_dps), algorithm=algo).fit(anomaly_dps)\n",
    "    distances_ddcal_1, indices_ddcal_1 = nbrs_ddcal_1.kneighbors(anomaly_dps)\n",
    "    print(\"****************Distance Between All Anomalous Datapoints Computed****************\")\n",
    "    # print(\"\\n\") \n",
    "    \n",
    "    ## print(\"DD_2: \", distances_ddcal_2)\n",
    "    ## print(\"ID_2: \", indices_ddcal_2)\n",
    "    ## distances_ddcal_1 = distances_ddcal_2\n",
    "    count_5 = 0\n",
    "    for i in range(len(indices_ddcal_1)):\n",
    "        count_5 += 1\n",
    "        send_idxs = indices_ddcal_1[i]\n",
    "        # print(\"Count: \", count_5)\n",
    "        # print(\"SI: \", send_idxs)\n",
    "        indices_ddcal_1[i] = ReturnIdxs(data1, send_idxs, anomaly_dps, anomaly_dict)\n",
    "        if count_5%50 == 0:\n",
    "            print(\"****************Indexes Changed for {} Anomalous Datapoints****************\".format(count_5))\n",
    "    # print(\"\\n\")    \n",
    "    ## indices_ddcal_1 = ReturnIdxs(data1, indices_ddcal_2, anomaly_dps)\n",
    "    ## print(indices_ddcal_1)\n",
    "\n",
    "    # New Method of finding the densest point\n",
    "    # Hash_Map_1 = dict() #{i:0 for i in range(len(anomaly_dps))}\n",
    "    # labels_1 = {i:0 for i in range(len(anomaly_dps))}\n",
    "    density_1 = []\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    for i in range(len(anomaly_idxs)): #in range(len(data1)):\n",
    "        count += 1\n",
    "        opt_K = LLE_lookup[anomaly_idxs[i]]  # 1 already added to K by the LLE function \n",
    "        distances_1 = distances_ddcal_1[i][:opt_K]\n",
    "        idxs_1 = indices_ddcal_1[i][1:opt_K]\n",
    "        # idxs_1 = ReturnIdxs(data1, idxs_2, anomaly_dps)\n",
    "        # print(\"ID2 :\", idxs_1)\n",
    "        # density.append([sum(distances)/(K-1), data1[i], max(distances), idxs, i])\n",
    "        Radius_1 = max(distances_1)\n",
    "        # print(\"Sum: \", sum(distances))\n",
    "        # print(\"Area: \", (math.pi*Radius*Radius))\n",
    "        # print(\"Density: \", sum(distances)/(math.pi*Radius*Radius))\n",
    "        temp = [(sum(distances_1)/(math.pi*Radius_1*Radius_1)), anomaly_dps[i], Radius_1, idxs_1, anomaly_idxs[i]]\n",
    "        density_1.append(temp)\n",
    "        Hash_Map[anomaly_idxs[i]] = temp\n",
    "        if (count)%100 == 0:\n",
    "            print(\"****************Density Computed for {} Anomalous Datapoints****************\".format(count))\n",
    "            \n",
    "            \n",
    "    density_1.sort(key = itemgetter(0), reverse = True)\n",
    "    \n",
    "    # print(density_1)\n",
    "    return density_1, distances_ddcal_1, indices_ddcal_1, Hash_Map\n",
    "\n",
    "\n",
    "\n",
    "def GetDPsfromIdxs(anomaly_idxs, data1, Incoming_points, train_end, train_start):\n",
    "    anomaly_dps = []\n",
    "    for idx in anomaly_idxs:\n",
    "        try:\n",
    "            anomaly_dps.append(data1[idx])\n",
    "        except:\n",
    "            anomaly_dps.append(Incoming_points[idx-(train_end-train_start)])\n",
    "    return np.asarray(anomaly_dps)\n",
    "\n",
    "def ReLabelAnomaliestoZero(labels, anomaly_idxs):\n",
    "    for idx in anomaly_idxs:\n",
    "        labels[idx] = 0\n",
    "    return labels\n",
    "\n",
    "\n",
    "def RunFindMostDensePoint(anomaly_dps, algo, Incoming_points, data1, Hash_Map, LLE_lookup, anomaly_idxs, anomaly_dict):\n",
    "    \n",
    "    # anomaly_dps = GetDPsfromIdxs(anomaly_idxs, data1, Incoming_points, train_end, train_start)\n",
    "    # print(\"Length of anomaly_dps: \", len(anomaly_dps), \"\\n\")\n",
    "    density_1, distances_ddcal_1, indices_ddcal_1, Hash_Map = FindMostDensePoint(anomaly_dps, algo, \n",
    "                                        Incoming_points, data1, Hash_Map, LLE_lookup, anomaly_idxs, anomaly_dict)\n",
    "    dense_pt = density_1[0][1]\n",
    "    # print(dense_pt)\n",
    "    # print(density_1[0][-1])\n",
    "    \n",
    "    return density_1, distances_ddcal_1, indices_ddcal_1, Hash_Map, dense_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OkU7HsQWoPPG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def PlotWithDensePoint(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots, dense_pt):\n",
    "    if Dimension <= 2:\n",
    "\n",
    "        plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=100 )\n",
    "\n",
    "        plt.scatter(dense_pt[0], dense_pt[1], c='gold', marker = 'o', s=100 )\n",
    "\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzlNvqxPoPPG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sdf1avcioPPG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fltN0H_MoPPG"
   },
   "outputs": [],
   "source": [
    "def ChangeidxLargerdatasettoSmallerdataset(data1, idx, anomaly_dps):\n",
    "    # idx 1 --> anomaly_dps\n",
    "    # Need to convert it to idx in data1\n",
    "    for i in range(len(anomaly_dps)):\n",
    "        comparison = data1[idx] == anomaly_dps[i]\n",
    "        equal_arrays = comparison.all()\n",
    "        if equal_arrays == True:\n",
    "            corresponding_idx = i\n",
    "            return corresponding_idx\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ctug-YroPPH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BFS based Approach: \n",
    "\n",
    "It takes care of the maximum recursion depth constraint in Python programming Language.\n",
    "This however may take more time than DFS\n",
    "\"\"\"\n",
    "def NovelClus_FF_Streaming(Parent, Hash_Map, dataset, K, list_of_parents_queue, labels, thresh, cluster_num, can_form_cluster, \n",
    "                 alpha_weight_avg, Tree_Structure, Roots, Store_Radius_Density, Store_Parents,\n",
    "                distances_ddcal, indices_ddcal, Store_Self_Density, whole_dataset, anomaly_dps, Incoming_points, \n",
    "                   train_end, train_start, LLE_lookup):\n",
    "    ##print(\"Cluster =\", cluster_num)\n",
    "    Hold_Root_Density = Parent[0]\n",
    "    \n",
    "    # Adding only the root\n",
    "    Roots[Parent[4]] = [Parent[1],Parent[0]]\n",
    "    Store_Radius_Density[Parent[4]] = [Parent[2], Parent[0]]\n",
    "    \n",
    "    # Add density of the root to Store_Self_Density\n",
    "    Store_Self_Density[Parent[4]] = Parent[0]\n",
    "    \n",
    "    \n",
    "    # We make use of a Deque for both side operation\n",
    "    Queue = deque()\n",
    "    \n",
    "    # Append from left the Parent Node\n",
    "    Queue.appendleft(Parent)\n",
    "    \n",
    "    \n",
    "    # print(\"IP: \", Parent)\n",
    "    \n",
    "    \n",
    "    # While Queue is not Null:\n",
    "        # Queue extracts the first Node\n",
    "        # We check if k children of Node satisfies the density criterion\n",
    "        # if a child satisfies, it is pushed into the Queue from the end; else it is termed -1 and left\n",
    "    \n",
    "    while Queue:\n",
    "        # Pop from the right end\n",
    "        Parent = Queue.pop()\n",
    "        # print(\"P: \", Parent[-1])\n",
    "            \n",
    "        # Getting important information from Parent\n",
    "        Density_Parent = Parent[0]\n",
    "        Radius = Parent[2]\n",
    "        #Child_Idx = Parent[3][idx] \n",
    "        Child_Idx_array = Parent[3]   # [94,  32, 117, .... 100, 118]\n",
    "        # print(\"CI: \", Child_Idx_array)\n",
    "        #Child = Hash_Map[Child_Idx]\n",
    "        #Child_Datapoint = Child[1]\n",
    "        \n",
    "        # Update the list_of_parents for the weighted moving average\n",
    "        list_of_parents = list_of_parents_queue.pop() + [Density_Parent] \n",
    "        Store_Parents[Parent[4]] = list_of_parents\n",
    "        #print(\"LoP =\", list_of_parents)\n",
    "        \n",
    "        \n",
    "        Child_array = []\n",
    "        Child_Datapoint_array = []\n",
    "        for Child_Idx in Child_Idx_array:\n",
    "            Child_array.append(Hash_Map[Child_Idx])\n",
    "            # print(\"F1: \", Child_Idx, Hash_Map[Child_Idx])\n",
    "            Child_Datapoint_array.append(Hash_Map[Child_Idx][1])  \n",
    "        \n",
    "        # For each Parent find which child satiesfies the density threshold criterion\n",
    "        ##print(\"Number of Children Nodes =\", len(Child_Idx_array), \"\\n\")\n",
    "        for i in range(len(Child_Idx_array)):\n",
    "            \n",
    "            Child = Child_array[i]\n",
    "            Child_Idx = Child[-1]             ##Child_Idx_array[i]\n",
    "            # print(\"Only CIN: \", Child_Idx)\n",
    "            Child_Datapoint = Child[1]        ##Child_Datapoint_array[i]\n",
    "            \n",
    "            # Base case: if child is already labelled in cluster then ignore\n",
    "            if labels[Child_Idx] > 0:\n",
    "                #print(\"Child is already labelled!!\")\n",
    "                continue\n",
    "             \n",
    "            \n",
    "            # # Old\n",
    "            # # print(\"Radius: \", Radius)\n",
    "            # neigh = NearestNeighbors(radius = Radius)\n",
    "            # neigh.fit(dataset)\n",
    "            # rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "            # print(\"Prev: \", len(rng[0][0]))\n",
    "            \n",
    "            \n",
    "            Corresponding_Child_Idx = ChangeidxLargerdatasettoSmallerdataset(whole_dataset, Child_Idx, anomaly_dps)\n",
    "            # print(\"Changed: \", Corresponding_Child_Idx)\n",
    "            \n",
    "            \n",
    "            # New\n",
    "            neigh_dist = []\n",
    "            neigh_idxs = []\n",
    "            rng = [[],[]]\n",
    "            for i in range(1, len(distances_ddcal[Corresponding_Child_Idx])):\n",
    "                if distances_ddcal[Corresponding_Child_Idx][i] < Radius and \\\n",
    "                (labels[indices_ddcal[Corresponding_Child_Idx][i]] == -1 or \\\n",
    "                 labels[indices_ddcal[Corresponding_Child_Idx][i]] == labels[Parent[-1]]):\n",
    "                    \n",
    "                    neigh_dist.append(distances_ddcal[Corresponding_Child_Idx][i])\n",
    "                    # print(\"NIs: \", indices_ddcal[Child_Idx][i])\n",
    "                    neigh_idxs.append(np.asarray(int(indices_ddcal[Corresponding_Child_Idx][i])))\n",
    "                    \n",
    "                    ## print(\"Added: \", labels[indices_ddcal[Corresponding_Child_Idx][i]], \"to: \", labels[Parent[-1]])\n",
    "                    \n",
    "                else:\n",
    "                    break\n",
    "            rng[0].append(np.asarray(neigh_dist))\n",
    "            rng[1].append(np.asarray(neigh_idxs))\n",
    "            rng = tuple(rng)\n",
    "            # print(\"New RNG:\", len(rng[0][0]))\n",
    "            ####\n",
    "            \n",
    "            \n",
    "            if len(rng[0][0]) == 0:\n",
    "                Density_Child = 0\n",
    "            else:\n",
    "                # Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "                Density_Child = sum(rng[0][0])/(math.pi*Radius*Radius)\n",
    "                \n",
    "            #print(\"No. of Parent =\",len(list_of_parents))\n",
    "            \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "            #print(\"Density of Parent =\", weighted_density)\n",
    "            #print(\"Density of Child = \", Density_Child)\n",
    "            numerator = abs(weighted_density - Density_Child)\n",
    "            denominator = weighted_density\n",
    "            #print(\"Weighted Density =\", numerator/denominator)\n",
    "            \n",
    "            \n",
    "            # print(\"CIN (Streaming): \", Child_Idx, \"N: \", numerator, \"D: \", denominator, \"Frac: \", numerator/denominator)\n",
    "            if numerator/denominator <= thresh:\n",
    "                # print(\"Included!!\")\n",
    "                labels[Child_Idx] = cluster_num\n",
    "                \n",
    "                list_of_parents_queue.appendleft(list_of_parents)\n",
    "                       \n",
    "                # Update the information for this child\n",
    "                Child[0] = Density_Child\n",
    "                if len(rng[0][0]) == 0:\n",
    "                    Child[2] = 0\n",
    "                else:\n",
    "                    Child[2] = max(rng[0][0])\n",
    "                Child[3] = rng[1][0]\n",
    "                \n",
    "                \n",
    "                # Store Adaptive Radius and New Density of the Parent Node for latter usage\n",
    "                list_of_parents_1 = list_of_parents + [Density_Child]\n",
    "                data_parent_1 = np.array(list_of_parents_1) #, dtype=np.float64)\n",
    "                weighted_density_1 = numpy_ewma_vectorized_v2(data_parent_1, alpha_weight_avg)  #O()??\n",
    "                Store_Radius_Density[Child[4]] = [Child[2], weighted_density_1]\n",
    "                \n",
    "                # Add density of child to Store_Self_Density\n",
    "                Store_Self_Density[Child[4]] = Density_Child\n",
    "                \n",
    "                # Add Child to the Tree Structure\n",
    "                if Parent[4] not in Tree_Structure:\n",
    "                    Tree_Structure[Parent[4]] = [Child[4]]\n",
    "                else:\n",
    "                    Tree_Structure[Parent[4]].append(Child[4])\n",
    "                    \n",
    "                    \n",
    "                #################################################################################################    \n",
    "                Inc_Dp_Idx = Child_Idx\n",
    "                Inc_Dp = Child[1]\n",
    "                # For the point that is just labelled, compare its distance with its present parent and its neighbours \n",
    "                # which belong to some different cluster. If the distance and density criteria satisfies,\n",
    "                # include it to that neighbour cluster and break\n",
    "                parent_of_curr_pt_idx = GetParentfromTree_F1(Tree_Structure, Inc_Dp_Idx)\n",
    "                neighbour_idx_of_different_clusters = GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, whole_dataset)\n",
    "                print(\"--> ASSESSING WITH ITS NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                for neighbour_idx in neighbour_idx_of_different_clusters:\n",
    "\n",
    "                    neighbour_dp = whole_dataset[neighbour_idx]\n",
    "                    # If the current point has become a root node\n",
    "                    if parent_of_curr_pt_idx == None: \n",
    "                        # print(\"--> ROOT OBTAINED!! with idx: \", neighbour_idx, \"and label:\", labels[neighbour_idx])\n",
    "                        \n",
    "                        # If root is obtained, check the density of the root with the probable parent.\n",
    "                        # It is to be done similar to the method of inclusion of a data point (same threshold)\n",
    "                        # If the root satisfies the density criterion, include it otherwise don't\n",
    "                        \n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx, whole_dataset)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                        \n",
    "                        # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                        cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                        if cardinality_curr_cluster <= 8 or numerator/denominator <= thresh:\n",
    "                            # cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                            # cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                            # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                            # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                            #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, Inc_Dp_Idx, labels[neighbour_idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "                            if neighbour_idx in Tree_Structure:\n",
    "                                Tree_Structure[neighbour_idx].append(Inc_Dp_Idx)\n",
    "                            else:\n",
    "                                Tree_Structure[neighbour_idx] = [Inc_Dp_Idx]\n",
    "                            break\n",
    "                    \n",
    "                    \n",
    "                    else:\n",
    "                        # If the current point is not a root Node\n",
    "                        \n",
    "                        if type(parent_of_curr_pt_idx) != int:\n",
    "                            raise Exception(\"More than 1 idx present(1)\")\n",
    "                        parent_of_curr_pt = whole_dataset[parent_of_curr_pt_idx]\n",
    "                        # parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, Inc_Dp_Idx)\n",
    "                        # parent_of_curr_dp = whole_dataset[parent_of_curr_dp_idx]\n",
    "                        # Not the child's parent but check with the child itself.\n",
    "\n",
    "                        Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                        Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                        list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx, whole_dataset)\n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "                        denominator = weighted_density\n",
    "                        # print(\"Weighted Density =\", numerator/denominator)\n",
    "                        # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "\n",
    "                        # Find distances with the present parent and the probable parent.\n",
    "                        dist_orig_parent = np.linalg.norm(Inc_Dp - parent_of_curr_pt)\n",
    "                        dist_prob_parent = np.linalg.norm(Inc_Dp - neighbour_dp)\n",
    "                        \n",
    "                        # Find density differences with the present parent and the probable parent.\n",
    "                        # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "                        # density_diff_with_prob_parent = Store_Self_Density[Inc_Dp_Idx] - Store_Self_Density[neighbour_idx]\n",
    "                        \n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                        \n",
    "                        ## print(\"Idx: \", neighbour_idx, \"Label: \", labels[neighbour_idx], \"dist_orig_parent: \", dist_orig_parent, \"dist_prob_parent: \", dist_prob_parent)\n",
    "                        \n",
    "                        \n",
    "                        if (dist_orig_parent)*0.3 <= (dist_prob_parent) and numerator/denominator > thresh: # Changed or to and\n",
    "                        ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                            # Everything is fine. Nothing needs to be changed\n",
    "                            # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                            # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                            # print(\"dist_orig_parent\", dist_orig_parent, \"dist_prob_parent\", dist_prob_parent, \"frac\" , numerator/denominator)\n",
    "                            continue\n",
    "\n",
    "                        else:\n",
    "                            # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                            # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                            #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                            # Plot to check whats happening\n",
    "                            # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", Inc_Dp)\n",
    "                            # PlotF2(Inc_Dp, parent_of_curr_pt, neighbour_dp, Dimension, cluster_num, whole_dataset, \\\n",
    "                            #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "\n",
    "                            if neighbour_idx in Tree_Structure:\n",
    "                                Tree_Structure[neighbour_idx].append(Inc_Dp_Idx)\n",
    "                            else:\n",
    "                                Tree_Structure[neighbour_idx] = [Inc_Dp_Idx]\n",
    "                            if parent_of_curr_pt_idx != None:\n",
    "                                Tree_Structure[parent_of_curr_pt_idx].remove(Inc_Dp_Idx)\n",
    "\n",
    "                            ChangeAllLabels_F(labels, Tree_Structure, Inc_Dp_Idx, labels[neighbour_idx])\n",
    "                            RemoveNodesWithNoChild(Tree_Structure)\n",
    "                            break\n",
    "\n",
    "                print(\"--> COMPLETED ASSESSING WITH ITS NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "                #################################################################################################    \n",
    "                       \n",
    "                \n",
    "                Queue.appendleft(Child)\n",
    "                \n",
    "                # Update the hashmap OLD\n",
    "                ##Hash_Map[Child[-1]] = Child\n",
    "                \n",
    "                # Update the hashmap NEW\n",
    "                temp_dict = {Child[-1]:Child}\n",
    "                Hash_Map.update(temp_dict)\n",
    "                ##################################\n",
    "                \n",
    "            \n",
    "                # The Parent can form a cluster of its own\n",
    "                can_form_cluster[0] = [True]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #print(\"Possible Anomaly!!\")\n",
    "                labels[Child_Idx] = -1\n",
    "                \n",
    "            \n",
    "            # Comment this later\n",
    "            Dimension = 2\n",
    "            # PlotF1(Child_Datapoint, Child_Idx, Dimension, cluster_num, whole_dataset, labels, Incoming_points, \n",
    "            #        train_end, train_start, Roots)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxyPdiySoPPI"
   },
   "outputs": [],
   "source": [
    "def BuildModel_Streaming(data1, density, Hash_Map, labels, thresh, alpha_weight_avg, distances_ddcal, indices_ddcal, \n",
    "                         start_with, whole_dataset, anomaly_dps, LLE_lookup, Store_Self_Density, train_end, train_start, \n",
    "                         Store_Radius_Density, Store_Parents):\n",
    "    \n",
    "    data_idx = 0\n",
    "    cluster_num = start_with\n",
    "    cluster_centers = []\n",
    "    hold_cluster_val = cluster_num ####\n",
    "\n",
    "    # For Storing Tree Structure\n",
    "    Tree_Structure = dict()\n",
    "    # Only Roots\n",
    "    Roots = dict()\n",
    "    # For Storing Radius and the Weighted Density of Ancestors including itself\n",
    "    # Store_Radius_Density = dict()\n",
    "    # For Storing parents\n",
    "    # Store_Parents = dict()\n",
    "    # For Storing Self Density\n",
    "    ## Store_Self_Density = dict()\n",
    "\n",
    "\n",
    "    for data_idx in range(len(data1)):  #O(n)\n",
    "\n",
    "        if (data_idx + 1) % 100 == 0:\n",
    "            print(\"**************************Computed {} Datapoints**************************\".format(data_idx+1))\n",
    "        idx = 0\n",
    "        list_of_parents_queue = deque()\n",
    "        list_of_parents_queue.appendleft([])\n",
    "        \n",
    "        \n",
    "        if labels[density[data_idx][-1]] == 0: #<= 0:\n",
    "            cluster_num += 1\n",
    "            labels[density[data_idx][-1]] = cluster_num\n",
    "            # print(\"CHECK1: \", density[data_idx][-1], labels[density[data_idx][-1]])\n",
    "            can_form_cluster = [False]\n",
    "\n",
    "\n",
    "            # Update information for the Parent node\n",
    "            #   0                         1         2        3                    4\n",
    "            # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "\n",
    "            K_mod = LLE_lookup[density[data_idx][-1]]      #O(1) Optimized One\n",
    "            #print(\"K modified: \",K_mod) # Can be uncommented for better intuition\n",
    "\n",
    "            #########################################\n",
    "            # # New One\n",
    "            # density[data_idx] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "            # temp_dict = {density[data_idx][-1]:density[data_idx]}\n",
    "            # #########################################\n",
    "        \n",
    "            NovelClus_FF_Streaming(density[data_idx], Hash_Map, data1, K_mod, list_of_parents_queue, labels, thresh, \n",
    "                                   cluster_num, can_form_cluster, alpha_weight_avg, Tree_Structure, Roots, \n",
    "                                   Store_Radius_Density, Store_Parents, distances_ddcal, indices_ddcal, Store_Self_Density,\n",
    "                                  whole_dataset, anomaly_dps, data1, train_end, train_start, LLE_lookup)\n",
    "            \n",
    "                \n",
    "            # # Visualization: For higher dimensions this time complexity can be ignored\n",
    "            # if Dimension <= 2:\n",
    "            #     plt.figure(figsize=(8,8))\n",
    "            #     for i in range(len(data1)):\n",
    "            #         if labels[i] == cluster_num:\n",
    "            #             plt.scatter(data1[i][0],data1[i][1],c='pink')\n",
    "            #         elif labels[i] == -1:\n",
    "            #             plt.scatter(data1[i][0],data1[i][1], c='yellow')\n",
    "            #         else:\n",
    "            #             plt.scatter(data1[i][0], data1[i][1],c='blue')\n",
    "            #     plt.scatter(density[data_idx][1][0], density[data_idx][1][1], c='red',s=200, marker = '+')\n",
    "            #     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            # If the last Parent Node cannot form a clusterof its own then it is a part of a cluster with only 1 datapoint\n",
    "            # which means \" a possible Anomaly\"\n",
    "            # then change its label to -1 and reduce the cluster num by 1\n",
    "            if can_form_cluster[0] == False:\n",
    "                labels[density[data_idx][-1]] = -1\n",
    "                cluster_num = hold_cluster_val  ####\n",
    "            else:\n",
    "                cluster_centers.append([density[data_idx][1].tolist(), cluster_num])\n",
    "                hold_cluster_val += 1  ####\n",
    "                \n",
    "            ## print(\"CC: \", cluster_centers)\n",
    "\n",
    "    return Roots, cluster_centers, Store_Radius_Density, Store_Parents, Store_Self_Density, Tree_Structure, density             \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upWwqjCioPPJ"
   },
   "outputs": [],
   "source": [
    "def OutlierPostProcessing_Streaming(LLE_lookup, algo, anomaly_dps, anomaly_idxs, labels, percent5, alpha_weight_avg, \n",
    "                          Store_Radius_Density, distances_ddcal_1, indices_ddcal_1, whole_dataset, \n",
    "                                    Store_Parents, Store_Self_Density):\n",
    "    # Select a predetermined value of K\n",
    "    # Find the K nearest Neighbors of an outlier\n",
    "    # For each of those neighbors:\n",
    "    #     Find their Intracluster distance with the same K value\n",
    "    #     If the Intracluster distance of outlier is within +-5% of the intracuster distance of the neighbor,\n",
    "    #     and the neighbor point is not an outlier, \n",
    "    #     then consider the outlier to be a part of the smae cluster that the neighbor point belongs.\n",
    "    \n",
    "    ##nbrs = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "    ##distances, indices = nbrs_ddcal.kneighbors(data1)\n",
    "    \n",
    "    for idx in anomaly_idxs:\n",
    "        if labels[idx] == -1:\n",
    "            # print(\"Here i: \", i)\n",
    "            # Predicted Outlier, so find its K nearest neighbors\n",
    "            #############################################################\n",
    "            Opt_K = LLE_lookup[idx]\n",
    "            # print(\"Opt k: \", Opt_K)\n",
    "            \n",
    "            Corresponding_Idx = ChangeidxLargerdatasettoSmallerdataset(whole_dataset, idx, anomaly_dps)\n",
    "            ## print(\"Changed: \", Corresponding_Idx)\n",
    "            \n",
    "            indices = indices_ddcal_1[Corresponding_Idx][1:Opt_K]\n",
    "            # print(\"Indices: \", indices)\n",
    "            # nbrs = NearestNeighbors(n_neighbors=Opt_K, algorithm=algo).fit(data1)              ########################\n",
    "            # _, indices = nbrs.kneighbors(data1)\n",
    "            #############################################################\n",
    "            \n",
    "            PredOutlierNNidx = indices               ##[i]\n",
    "            # print(\"IDXs: \", PredOutlierNNidx)\n",
    "            PredOutlierNN = []\n",
    "            for idx in PredOutlierNNidx:\n",
    "                PredOutlierNN.append(whole_dataset[idx])\n",
    "            # Find the intracluster distance for the Predicted Outlier\n",
    "            PredOutlierICD = IntraClusterDistance(PredOutlierNN)\n",
    "            # print(\"Outlier\", i, PredOutlierICD)\n",
    "            \n",
    "            \n",
    "            # For each of the outlier's K neighbors\n",
    "            for idx1 in PredOutlierNNidx[1:]:\n",
    "                \n",
    "                # Check if the neighbor is an outlier or inlier\n",
    "                if labels[idx1] != -1:\n",
    "                    # print(\"Not an Outlier\")\n",
    "                    pruned_k = LLE_lookup[idx1]\n",
    "                    Corresponding_Idx1 = ChangeidxLargerdatasettoSmallerdataset(whole_dataset, idx, anomaly_dps)\n",
    "                    # print(\"Changed: \", Corresponding_Idx)\n",
    "                    NNeighboridx = indices_ddcal_1[Corresponding_Idx1][1:pruned_k]\n",
    "                    NNeighbor = []\n",
    "                    for idx2 in NNeighboridx:\n",
    "                        NNeighbor.append(whole_dataset[idx2])\n",
    "                        \n",
    "                    # Find each neighbor's IntraClusterDistance(ICD)\n",
    "                    NNICD = IntraClusterDistance(NNeighbor)\n",
    "                    #print(\"NNICD: \", NNICD)\n",
    "                    \n",
    "                    # Check if outlier's ICD is about +-5% of neighbor's ICD\n",
    "                    # print(\"Lower: \",(NNICD-percent5*NNICD), \"Upper: \",(NNICD+percent5*NNICD), \"Self: \",PredOutlierICD)\n",
    "                    if (NNICD-percent5*NNICD) <= PredOutlierICD <= (NNICD+percent5*NNICD):\n",
    "                        # Change the label of the Predicted Outlier 'i' to the \n",
    "                        # label of the 1st Nearest Neighbor which satifies the condition\n",
    "                        labels[idx] = labels[idx1]\n",
    "                        # print(\"Changed\\n\")\n",
    "                        if idx1 not in Tree_Structure:\n",
    "                            Tree_Structure[idx1] = [idx]\n",
    "                        else:\n",
    "                            Tree_Structure[idx1].append(idx)\n",
    "                            \n",
    "                        # Update its Store_Radius_Density Value\n",
    "                        # print(\"IDX:\",idx)\n",
    "                        \n",
    "                        # # Old\n",
    "                        # Radius = Store_Radius_Density[idx][0]\n",
    "                        # neigh = NearestNeighbors(radius = Radius)\n",
    "                        # neigh.fit(data1)\n",
    "                        # rng = neigh.radius_neighbors([data1[i]])\n",
    "                        \n",
    "                        # New\n",
    "                        Radius = Store_Radius_Density[idx1][0]\n",
    "                        neigh_dist = []\n",
    "                        neigh_idxs = []\n",
    "                        rng = [[],[]]\n",
    "                        for k in range(1, len(distances_ddcal_1[Corresponding_Idx])):\n",
    "                            if distances_ddcal_1[Corresponding_Idx][k] < Radius:\n",
    "                                neigh_dist.append(distances_ddcal_1[Corresponding_Idx][k])\n",
    "                                # print(\"NIs: \", indices_ddcal[Child_Idx][i])\n",
    "                                neigh_idxs.append(np.asarray(int(indices_ddcal_1[Corresponding_Idx][k])))\n",
    "                            else:\n",
    "                                break\n",
    "                        rng[0].append(np.asarray(neigh_dist))\n",
    "                        rng[1].append(np.asarray(neigh_idxs))\n",
    "                        rng = tuple(rng)\n",
    "                        # print(\"New RNG:\", len(rng[0][0]))\n",
    "                        ####\n",
    "                        \n",
    "                        \n",
    "                        # Check if there is no neighbour within the radius\n",
    "                        # If yes then the density of the recently changed datapoint is 0\n",
    "                        if len(rng[0][0]) == 0:\n",
    "                            Density_Changed_Dp = 0\n",
    "                        else:\n",
    "                            #Density_Parent = sum(rng[0][0])/len(rng[0][0])       ########################\n",
    "                            Radius = max(rng[0][0])\n",
    "                            Density_Changed_Dp = sum(rng[0][0])/(math.pi*Radius*Radius)\n",
    "                            \n",
    "                        list_of_parents = Store_Parents[idx1] + [Density_Changed_Dp] \n",
    "                        data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                        weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                        if len(rng[0][0]) == 0:\n",
    "                            Radius_new = 0\n",
    "                        else:\n",
    "                            Radius_new = max(rng[0][0])\n",
    "                        Store_Radius_Density[idx] = [Radius_new, weighted_density]\n",
    "                        Store_Parents[idx] = list_of_parents\n",
    "                        Store_Self_Density[idx] = Density_Changed_Dp\n",
    "                        \n",
    "                        break\n",
    "\n",
    "            \n",
    "def IntraClusterDistance(X):\n",
    "        Intra_Clus_Dist = 0\n",
    "        for i in range(len(X)):\n",
    "            Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "        return Intra_Clus_Dist/(len(X)-1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypots1WzoPPJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KoNThFkoPPJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ChangelabelsandRunBuildModelStreaming(labels, anomaly_idxs, anomaly_dps, density_1, Hash_Map, thresh, \n",
    "            alpha_weight_avg, distances_ddcal_1, indices_ddcal_1, data1, LLE_lookup, Store_Self_Density, \n",
    "                                          train_end, train_start, Roots, Store_Radius_Density, Store_Parents):\n",
    "\n",
    "    # Changed labels of anomalies to 0 for remodeling\n",
    "    labels = ReLabelAnomaliestoZero(labels, anomaly_idxs)\n",
    "    # print(labels)\n",
    "\n",
    "    # print(len(Hash_Map))\n",
    "    # print(Hash_Map)\n",
    "    ## print(len(distances_ddcal_1), len(indices_ddcal_1), len(anomaly_dps))\n",
    "\n",
    "    # distances_ddcal = distances_ddcal + distances_ddcal_1\n",
    "    # indices_ddcal = indices_ddcal + indices_ddcal_1\n",
    "    # print(len(distances_ddcal), len(indices_ddcal), len(anomaly_dps))\n",
    "\n",
    "    listofkeys = []\n",
    "    for key in labels:\n",
    "        if labels[key] != -1 and labels[key] not in set(listofkeys):\n",
    "            listofkeys.append(labels[key])\n",
    "        \n",
    "    start_with = len(Roots) # len(listofkeys)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    Roots_1, cluster_centers_1, Store_Radius_Density_1, Store_Parents_1, Store_Self_Density_1, Tree_Structure_1, density_1 = BuildModel_Streaming(anomaly_dps, \n",
    "    density_1, Hash_Map, labels, thresh, alpha_weight_avg, distances_ddcal_1, indices_ddcal_1, start_with, data1, \n",
    "    anomaly_dps, LLE_lookup, Store_Self_Density, train_end, train_start, Store_Radius_Density, Store_Parents)\n",
    "\n",
    "\n",
    "    # print(cluster_centers_1)\n",
    "    # print(\"\\n\", len(cluster_centers_1))\n",
    "    \n",
    "    return labels, Roots_1, cluster_centers_1, Store_Radius_Density_1, Store_Parents_1, Store_Self_Density_1, Tree_Structure_1, density_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HTypttdVoPPK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KT97FRy6oPPO"
   },
   "outputs": [],
   "source": [
    "def CheckIfSame(item1, item2):\n",
    "        comparison = np.array(item1) == np.array(item2)\n",
    "        equals_array = comparison.all()\n",
    "        if equals_array == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "def CheckIfPresent1(cluster_centers, clus_cen):\n",
    "    for item in cluster_centers:\n",
    "        # print(\"F1: \", item[0], clus_cen[0])\n",
    "        comparison = np.array(item[0]) == np.array(clus_cen[0])\n",
    "        equals_array = comparison.all()\n",
    "        if equals_array == True:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def GetPresentClusterInformation1(cluster_centers_1, cluster_centers):\n",
    "    \n",
    "    if cluster_centers is not None:\n",
    "        # Remove duplicate values from cluster_centers\n",
    "        new_cluster_centers = []\n",
    "        duplicate_at_idx = set()\n",
    "        for i in range(len(cluster_centers)-1):\n",
    "            boolean_f = False\n",
    "            for j in range(i+1, len(cluster_centers)):\n",
    "                if CheckIfSame(cluster_centers[i][0], cluster_centers[j][0]) == True:\n",
    "                    duplicate_at_idx.add(j)\n",
    "\n",
    "        for i in range(len(cluster_centers)):\n",
    "            if i not in duplicate_at_idx:\n",
    "                new_cluster_centers.append(cluster_centers[i])\n",
    "\n",
    "        cluster_centers = new_cluster_centers\n",
    "\n",
    "\n",
    "    for clus_cen in cluster_centers_1:\n",
    "        # print(clus_cen[1])\n",
    "        boolean = False\n",
    "        for present_clus_cen in cluster_centers:\n",
    "            # print(present_clus_cen[1], clus_cen[1])\n",
    "            if present_clus_cen[1] == clus_cen[1]:\n",
    "                boolean = True\n",
    "                break\n",
    "        present = CheckIfPresent1(cluster_centers, clus_cen)\n",
    "        # print(\"Present: \", present)\n",
    "        if boolean == False and present == False:\n",
    "            cluster_centers.append(clus_cen)\n",
    "\n",
    "\n",
    "    # print(\"Present Cluster Centers: \\n\")\n",
    "    # print(cluster_centers)\n",
    "    \n",
    "    return cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBbQLsnzoPPO"
   },
   "outputs": [],
   "source": [
    "def AdjustRoot_Streaming(Roots, labels):\n",
    "    seen = set()\n",
    "    idxs_to_remove = []\n",
    "    for idx in Roots:\n",
    "        if labels[idx] in seen or labels[idx] == -1:\n",
    "            idxs_to_remove.append(idx)\n",
    "        else:\n",
    "            seen.add(labels[idx])\n",
    "\n",
    "    for idx in idxs_to_remove:\n",
    "        del Roots[idx]\n",
    "    \n",
    "    return Roots\n",
    "    \n",
    "\n",
    "# print(Roots)\n",
    "# for key in Roots:\n",
    "#     print(labels[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9VzS9MkoPPO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RunCorrectingwrongclusteringAndcorrectthelabels(labels, cluster_centers, data1, Roots_1):\n",
    "    #######Roots_1, labels, Tree_Structure_1 = Correcting_wrong_clustering(labels, cluster_centers, data1, Roots_1)\n",
    "    #### For the time being I have commented the above function, so no need of returning anything\n",
    "    correct_the_idx(labels)\n",
    "    \n",
    "    ##return Roots_1, labels, Tree_Structure_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBFZNI_3oPPP"
   },
   "outputs": [],
   "source": [
    "def UpdateRootsandCluster1(cluster_centers, Roots, Roots_1, labels):\n",
    "    \n",
    "    # Updating the Cluster_Centers\n",
    "\n",
    "    toremove = []\n",
    "    for item in cluster_centers:\n",
    "        boolean = False\n",
    "        item1 = np.array(item[0])\n",
    "        for key in Roots:\n",
    "            item2 = np.array(Roots[key][0])\n",
    "            comparison1 = item1 == item2\n",
    "            equals_array1 = comparison1.all()\n",
    "            if equals_array1 == True:\n",
    "                boolean = True\n",
    "                break\n",
    "        if boolean == False:\n",
    "            for key1 in Roots_1:\n",
    "                item3 = np.array(Roots_1[key1][0])\n",
    "                comparison2 = item1 == item3\n",
    "                equals_array2 = comparison2.all()\n",
    "                if equals_array2 == True:\n",
    "                    boolean = True\n",
    "                    break\n",
    "        if boolean == False:\n",
    "            toremove.append(item)\n",
    "\n",
    "    new_cluster_centers = []\n",
    "    for i in range(len(cluster_centers)):\n",
    "        boolean1 = False\n",
    "        for j in range(len(toremove)):\n",
    "            item4 = np.array(cluster_centers[i][0])\n",
    "            item5 = np.array(toremove[j][0])\n",
    "            comparison4 = item4 == item5\n",
    "            equals_array4 = comparison4.all()\n",
    "            if equals_array4 == True:\n",
    "                boolean1 = True\n",
    "                break\n",
    "        if boolean1 == False:\n",
    "            new_cluster_centers.append(cluster_centers[i])\n",
    "\n",
    "    cluster_centers = new_cluster_centers\n",
    "\n",
    "    # toadd = []\n",
    "    for key2 in Roots_1:\n",
    "        boolean5 = False\n",
    "        item6 = Roots_1[key2][0]\n",
    "        for item in cluster_centers:\n",
    "            comparison5 = item[0] == np.array(item6)\n",
    "            equals_array5 = comparison5.all()\n",
    "            if equals_array5 == True:\n",
    "                boolean5 = True\n",
    "                break\n",
    "        if boolean5 == False and labels[key2] != -1:\n",
    "            cluster_centers.append([Roots_1[key2][0].tolist(), labels[key2]])\n",
    "\n",
    "\n",
    "    # print(\"Roots: \", Roots)\n",
    "    # print(\"Roots_1: \", Roots_1)\n",
    "    # print(\"Cluster Centers: \", cluster_centers)\n",
    "    \n",
    "    return Roots, Roots_1, cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjmwU2z4oPPP"
   },
   "outputs": [],
   "source": [
    "# labels[271]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gw14tRK7oPPP"
   },
   "outputs": [],
   "source": [
    "# for idx in anomaly_idxs:\n",
    "#     print(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7pM7A4Y-oPPP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MergeAll(Tree_Structure, Tree_Structure_1, Store_Self_Density, Store_Self_Density_1, Store_Radius_Density, \n",
    "             Store_Radius_Density_1, Store_Parents, Store_Parents_1, Roots, Roots_1, distances_ddcal_1, indices_ddcal_1,\n",
    "            LLE_lookup, algo, anomaly_dps, anomaly_idxs, labels, percent5, alpha_weight_avg):\n",
    "\n",
    "    # print(\"Merging Tree Structures\")\n",
    "    # print(len(Tree_Structure))\n",
    "    # print(len(Tree_Structure_1))\n",
    "    Tree_Structure = {**Tree_Structure, **Tree_Structure_1}\n",
    "    # print(len(Tree_Structure))\n",
    "\n",
    "    # print(\"\\nMerging Store_Self_Density\")\n",
    "    # print(len(Store_Self_Density))\n",
    "    # print(len(Store_Self_Density_1))\n",
    "    Store_Self_Density = {**Store_Self_Density, **Store_Self_Density_1}\n",
    "    # print(len(Store_Self_Density))\n",
    "\n",
    "    # print(\"\\nMerging Store_Radius_Density\")\n",
    "    # print(len(Store_Radius_Density))\n",
    "    # print(len(Store_Radius_Density_1))\n",
    "    Store_Radius_Density = {**Store_Radius_Density, **Store_Radius_Density_1}\n",
    "    # print(len(Store_Radius_Density))\n",
    "\n",
    "    # print(\"\\nMerging Store_Parents\")\n",
    "    # print(len(Store_Parents))\n",
    "    # print(len(Store_Parents_1))\n",
    "    Store_Parents = {**Store_Parents, **Store_Parents_1}\n",
    "    # print(len(Store_Parents))\n",
    "\n",
    "    # print(\"\\nMerging Roots\")\n",
    "    # print(len(Roots))\n",
    "    # print(len(Roots_1))\n",
    "    Roots = {**Roots, **Roots_1}\n",
    "    # print(len(Roots))\n",
    "    # Roots = AdjustRoot_Streaming(Roots, labels)\n",
    "    # print(\"Current Roots (will be updated again): \", len(Roots))\n",
    "\n",
    "    # print(\"\\n\")\n",
    "    cluster_num = len(Roots)\n",
    "    # print(\"Cluster Num: \", cluster_num)\n",
    "\n",
    "    ######### I have uncommented this\n",
    "#     OutlierPostProcessing_Streaming(LLE_lookup, algo, anomaly_dps, anomaly_idxs, labels, percent5, alpha_weight_avg, \n",
    "#                               Store_Radius_Density, distances_ddcal_1, indices_ddcal_1, data1, \n",
    "#                                     Store_Parents, Store_Self_Density)\n",
    "\n",
    "    \n",
    "    return Tree_Structure, Store_Self_Density, Store_Radius_Density, Store_Parents, Roots, cluster_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDNe3sqwoPPQ"
   },
   "outputs": [],
   "source": [
    "def UPDATEROOTS(Roots, cluster_centers, Store_Self_Density):\n",
    "    toremovepairs1 = []\n",
    "    idxs_1 = list(Roots.keys())\n",
    "    for i in range(len(idxs_1)-1):\n",
    "        for j in range(i+1, len(idxs_1)):\n",
    "            if labels[idxs_1[i]] == labels[idxs_1[j]]:\n",
    "                toremovepairs1.append([idxs_1[i], idxs_1[j]])\n",
    "    # print(\"To Remove: \", toremovepairs1, \"Roots: \", Roots)\n",
    "    for pair in toremovepairs1:\n",
    "        if Store_Self_Density[pair[0]] < Store_Self_Density[pair[1]]:\n",
    "            if pair[0] in Roots:\n",
    "                del Roots[pair[0]]\n",
    "        else:\n",
    "            if pair[1] in Roots:\n",
    "                del Roots[pair[1]]\n",
    "            \n",
    "    # If one root has become an anomaly\n",
    "    toremove2 = []\n",
    "    for key in Roots:\n",
    "        if labels[key] == -1:\n",
    "            toremove2.append(key)\n",
    "    for key_item in toremove2:\n",
    "        del Roots[key_item]\n",
    "        \n",
    "            \n",
    "    for i in range(len(cluster_centers)):\n",
    "        for key in Roots:\n",
    "            if labels[key] == cluster_centers[i][1]:\n",
    "                cluster_centers[i][0] = Roots[key][0].tolist()\n",
    "                break\n",
    "            \n",
    "                \n",
    "    return Roots, cluster_centers\n",
    "\n",
    "def UpdateRootsAndClusterCentersAgain(Roots, cluster_centers, Store_Self_Density):\n",
    "    Roots, cluster_centers = UPDATEROOTS(Roots, cluster_centers, Store_Self_Density)\n",
    "    \n",
    "    def CheckIfSame_2(item1, item2):\n",
    "        comparison = np.array(item1) == np.array(item2)\n",
    "        equals_array = comparison.all()\n",
    "        if equals_array == True:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    # Remove duplicate values from cluster_centers\n",
    "    def Removeduplicatesfromclustercenters_2(cluster_centers):\n",
    "        new_cluster_centers = []\n",
    "        duplicate_at_idx = set()\n",
    "        for i in range(len(cluster_centers)-1):\n",
    "            boolean_f = False\n",
    "            for j in range(i+1, len(cluster_centers)):\n",
    "                if CheckIfSame_2(cluster_centers[i][0], cluster_centers[j][0]) == True:\n",
    "                    duplicate_at_idx.add(j)\n",
    "        for i in range(len(cluster_centers)):\n",
    "            if i not in duplicate_at_idx:\n",
    "                new_cluster_centers.append(cluster_centers[i])\n",
    "        cluster_centers = new_cluster_centers\n",
    "        return cluster_centers\n",
    "\n",
    "        \n",
    "    if len(cluster_centers) > 1:\n",
    "        cluster_centers = Removeduplicatesfromclustercenters_2(cluster_centers)\n",
    "        \n",
    "    \n",
    "    # Make Cluster Center and Roots same\n",
    "    new_cluster_center2 = []\n",
    "    for item in cluster_centers:\n",
    "        boolean = False\n",
    "        item1 = item[0]\n",
    "        for root in Roots:\n",
    "            item2 = Roots[root][0]\n",
    "            comparison = np.array(item1) == np.array(item2)\n",
    "            equals_array = comparison.all()\n",
    "            if equals_array == True:\n",
    "                boolean = True\n",
    "                break\n",
    "        if boolean == True:\n",
    "            new_cluster_center2.append(item)\n",
    "    \n",
    "    cluster_centers = new_cluster_center2\n",
    "    \n",
    "    \n",
    "    # print(\"Current Updated Roots: \", Roots)\n",
    "    # print(\"Current Updated Cluster Centers: \", cluster_centers)\n",
    "    cluster_num = len(Roots)\n",
    "    # print(\"Current Updated cluster_num: \", cluster_num)\n",
    "    \n",
    "    return Roots, cluster_centers, cluster_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB4AWwWEoPPQ"
   },
   "outputs": [],
   "source": [
    "# print(\"Current Updated Roots: \", Roots)\n",
    "# print(\"Current Updated Cluster Centers: \", cluster_centers)\n",
    "# cluster_num = len(Roots)\n",
    "# print(\"Current Updated cluster_num: \", cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M81ZBQumoPPQ"
   },
   "outputs": [],
   "source": [
    "def InitalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots):\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='black', marker = 'o', s=200 )\n",
    "\n",
    "        # plt.scatter(dense_pt[0], dense_pt[1], c='gold', marker = 'o', s=100 )\n",
    "\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Concept Evolution\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ys17RYK3oPPR"
   },
   "outputs": [],
   "source": [
    "def RunRearranging5(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots):\n",
    "    print(\"----------------------------------Rearranging5----------------------------------\")\n",
    "    RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, data1, Roots)\n",
    "    correct_the_idx(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "meRMEtbWoPPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmaJ2b78oPPR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhGhggSMoPPS"
   },
   "outputs": [],
   "source": [
    "# For all the anomaly points:\n",
    "# 1. Find their neighbours using pruned k\n",
    "# 2. If all neighbours belong to the same cluster as itself, then do nothing\n",
    "# 3. If neighbour is an anomaly then do the same thing while building Tree in Streaming for anomaly\n",
    "# 4. If neighbour belongs to a different cluster, check its distance with its parent and the parent of the point\n",
    "#    under consideration. Whichever is lesser insert it into that.\n",
    "\n",
    "\n",
    "# When a neighbour is the root of a cluster, check its cardinality.\n",
    "# If that is less than a percent include it with the curr_idx label along with its children nodes\n",
    "#\n",
    "\n",
    "\n",
    "# Function to return the set of labels for all neighbour points\n",
    "def ReturnSetofLabels(labels, neighbour_idxs):\n",
    "    return_labels = set()\n",
    "    for idx in neighbour_idxs:\n",
    "        if labels[idx] not in return_labels:\n",
    "            return_labels.add(labels[idx])\n",
    "    return return_labels\n",
    "\n",
    "\n",
    "def GetParentfromTree(Tree_Structure, idx):\n",
    "    send_key = None\n",
    "    for key in Tree_Structure:\n",
    "        if idx in set(Tree_Structure[key]):\n",
    "            send_key = key\n",
    "            break\n",
    "    if send_key is None:\n",
    "        return send_key\n",
    "    else:\n",
    "        return int(send_key)\n",
    "    \n",
    "\n",
    "def GetCardinality(labels, idx):\n",
    "    cardinality = 0\n",
    "    label_idx = labels[idx]\n",
    "    for key in labels:\n",
    "        if labels[key] == label_idx:\n",
    "            cardinality += 1\n",
    "    return cardinality\n",
    "\n",
    "\n",
    "# def ChangeAllLabels(labels, Tree_Structure, idx, changetolabel):\n",
    "#     ToChange = deque()\n",
    "#     ToChange.append(idx)\n",
    "#     while ToChange:\n",
    "#         curr_idx = ToChange.pop()\n",
    "#         labels[curr_idx] = changetolabel\n",
    "#         if curr_idx in Tree_Structure:\n",
    "#             for child_idx in Tree_Structure[curr_idx]:\n",
    "#                 ToChange.append(child_idx)\n",
    "#     return labels\n",
    "\n",
    "\n",
    "\n",
    "def CheckandCorrectStreaming(data1, anomaly_idxs, LLE_lookup, anomaly_dps, Roots, Store_Radius_Density, \n",
    "                             Store_Parents, Hash_Map, thresh, Store_Self_Density, Tree_Structure, labels, \n",
    "                             Concept_Evolution_Thresh, cluster_centers, Dimension, train_end, train_start,\n",
    "                              algo, alpha_weight_avg):\n",
    "    \n",
    "    thresh_5 = 10 #(for corners)\n",
    "    percent7 = 0.05 # 0.5  #0.05(For Correct One till now)\n",
    "    \n",
    "    for i in range(len(anomaly_dps)):\n",
    "        curr_idx = anomaly_idxs[i]\n",
    "        curr_dp = anomaly_dps[i]\n",
    "        \n",
    "        if labels[curr_idx] != -1:\n",
    "            ## 1. Find their neighbours using pruned k\n",
    "            opt_k = LLE_lookup[anomaly_idxs[i]]\n",
    "            nearest_neighbour_idxs = GetNeighbours(anomaly_dps[i], data1, opt_k)\n",
    "            # print(\"NNI: \", nearest_neighbour_idxs)\n",
    "\n",
    "\n",
    "            SetofLabels = ReturnSetofLabels(labels, nearest_neighbour_idxs)\n",
    "            ## 2. If all neighbours belong to the same cluster as itself, then do nothing\n",
    "            if len(SetofLabels) == 1 and SetofLabels.pop() != -1:\n",
    "                # print(\"--> All neighbours belong to the same cluster and are not anomalies\")\n",
    "                continue\n",
    "\n",
    "\n",
    "            ## 3. If neighbour belongs to a different cluster, check its distance with its parent and the parent of the point\n",
    "            ##    under consideration. Whichever is lesser insert it into that.\n",
    "            elif len(SetofLabels) > 1:\n",
    "                # print(\"--> Neighbours belong to different clusters\")\n",
    "                # To do\n",
    "                # Neighbours can either belong to different cluster or can be anomalies\n",
    "                # For neighbours belonging to other clusters:\n",
    "                for neighbour_idx in nearest_neighbour_idxs:\n",
    "                    # print(\"Label of \", neighbour_idx, \"is \", labels[neighbour_idx], \"Curr idx label: \", labels[curr_idx])\n",
    "                    if labels[curr_idx] == labels[neighbour_idx]:\n",
    "                        # Same label as the point under consideration so do nothing\n",
    "                        continue\n",
    "\n",
    "                    elif labels[curr_idx] != labels[neighbour_idx] and labels[neighbour_idx] != -1:\n",
    "                        # print(\"Label of neighbour: \", labels[neighbour_idx], neighbour_idx)\n",
    "                        # neighbour belongs to some other cluster\n",
    "                        # print(\"NI: \", neighbour_idx)\n",
    "                        neighbour_dp = data1[neighbour_idx]\n",
    "                        parent_of_neighbour_idx = GetParentfromTree(Tree_Structure, neighbour_idx)\n",
    "                        \n",
    "                        \n",
    "                        # When a neighbour is the root of a cluster, check its cardinality.\n",
    "                        # If that is less than a percent include it with the curr_idx label along with its children nodes\n",
    "                        if parent_of_neighbour_idx == None: # or parent_of_curr_dp_idx == None:\n",
    "                            # print(\"--> ROOT OBTAINED!!\")\n",
    "                            \n",
    "                            Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                            Density_Inc_Dp = Store_Self_Density[curr_idx]\n",
    "                            list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, curr_idx, data1)\n",
    "                            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                            numerator = abs(weighted_density - Density_neighbour)\n",
    "                            denominator = weighted_density\n",
    "                            # print(\"Weighted Density =\", numerator/denominator)\n",
    "                            # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                            \n",
    "                            # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                            cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                            if cardinality_neighbour_cluster <= 8 or numerator/denominator <= thresh:\n",
    "                                # cardinality_neighbour_cluster = GetCardinality(labels, neighbour_idx)\n",
    "                                # cardinality_curr_cluster = GetCardinality(labels, curr_idx)\n",
    "                                # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                                # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                                #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                                ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[curr_idx])\n",
    "                                RemoveNodesWithNoChild(Tree_Structure)\n",
    "                                if curr_idx in Tree_Structure:\n",
    "                                    Tree_Structure[curr_idx].append(neighbour_idx)\n",
    "                                else:\n",
    "                                    Tree_Structure[curr_idx] = [neighbour_idx]\n",
    "                                # labels[neighbour_idx] = labels[curr_idx]\n",
    "                                # if neighbour_idx in Tree_Structure:\n",
    "                                #     for child_idx in Tree_Structure[neighbour_idx]:\n",
    "                                #         labels[child_idx] = labels[curr_idx]\n",
    "                        \n",
    "                        \n",
    "                        else:\n",
    "                            # If the neighbour is not a root node\n",
    "                            # print(\"PNI: \", parent_of_neighbour_idx)\n",
    "                            if type(parent_of_neighbour_idx) != int:\n",
    "                                raise Exception(\"More than 1 idx present(2)\")\n",
    "                            parent_of_neighbour = data1[parent_of_neighbour_idx]\n",
    "                            # parent_of_curr_dp_idx = GetParentfromTree(Tree_Structure, curr_idx)\n",
    "                            # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "\n",
    "                            # print(\"PI: \", parent_of_neighbour_idx)\n",
    "\n",
    "                            Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                            Density_Inc_Dp = Store_Self_Density[curr_idx]\n",
    "                            list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, curr_idx, data1)\n",
    "                            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                            numerator = abs(weighted_density - Density_neighbour)\n",
    "                            denominator = weighted_density\n",
    "                            # print(\"Weighted Density =\", numerator/denominator)\n",
    "                            # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                                \n",
    "                            # Find distances with the present parent and the probable parent.\n",
    "                            dist_orig_parent = np.linalg.norm(neighbour_dp - parent_of_neighbour)\n",
    "                            dist_prob_parent = np.linalg.norm(neighbour_dp - curr_dp)\n",
    "                            \n",
    "                            # Find density differences with the present parent and the probable parent.\n",
    "                            # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "                            # density_diff_with_prob_parent = Store_Self_Density[curr_idx] - Store_Self_Density[neighbour_idx]\n",
    "                        \n",
    "                            ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                        \n",
    "                            if (dist_orig_parent)*0.3 <= (dist_prob_parent) or numerator/denominator > thresh: ## and (dist_orig_parent) <= thresh_5:\n",
    "                            ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                                # Everything is fine. Nothing needs to be changed\n",
    "                                # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                                # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                                continue\n",
    "\n",
    "                            else:\n",
    "                                # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                                # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                                #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                                # Plot to check whats happening\n",
    "                                # Plot to check whats happening\n",
    "                                # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", curr_dp)\n",
    "                                # PlotF2(neighbour_dp, parent_of_neighbour, curr_dp, Dimension, cluster_num, data1, \\\n",
    "                                #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "                                \n",
    "                                if curr_idx in Tree_Structure:\n",
    "                                    Tree_Structure[curr_idx].append(neighbour_idx)\n",
    "                                else:\n",
    "                                    Tree_Structure[curr_idx] = [neighbour_idx]\n",
    "                                if parent_of_neighbour_idx != None:\n",
    "                                    Tree_Structure[parent_of_neighbour_idx].remove(neighbour_idx)\n",
    "\n",
    "                                ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[curr_idx])\n",
    "                                RemoveNodesWithNoChild(Tree_Structure)\n",
    "                                # labels[neighbour_idx] = labels[curr_idx]\n",
    "                                # if neighbour_idx in Tree_Structure:\n",
    "                                #     for child_idx in Tree_Structure[neighbour_idx]:\n",
    "                                #         labels[child_idx] = labels[curr_idx]\n",
    "\n",
    "                            \n",
    "                    else:\n",
    "                        ## 3. If neighbour is an anomaly then do the same thing while building Tree in Streaming for anomaly\n",
    "                        # print(\"--> Anomaly neighbour faced!!\")\n",
    "                        pass\n",
    "                        # To do\n",
    "\n",
    "    \n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # Get the remaining anomalies\n",
    "    updated_anomaly_idxs = []\n",
    "    updated_anomaly_dps = [] \n",
    "    ########### I changed this\n",
    "#     for key in labels:\n",
    "#         if labels[key] == -1:\n",
    "#             updated_anomaly_idxs.append(key)\n",
    "#             updated_anomaly_dps.append(data1[key])\n",
    "#     updated_anomaly_dps = np.asarray(updated_anomaly_dps)\n",
    "    \n",
    "    for idx5 in anomaly_idxs:\n",
    "        if labels[idx5] == -1:\n",
    "            updated_anomaly_idxs.append(idx5)\n",
    "            updated_anomaly_dps.append(data1[idx5])\n",
    "    updated_anomaly_dps = np.asarray(updated_anomaly_dps)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"--> ASSESSING POTENTIAL ANOMALIES\")\n",
    "    correct_the_idx(labels)\n",
    "    Roots = AdjustRoot_Streaming(Roots, labels)\n",
    "    ## Roots, cluster_centers = UPDATEROOTS(Roots, cluster_centers, Store_Self_Density)\n",
    "    countimg = [0]   ## Not interest in plotting now. Afterwards it can be uncommented\n",
    "    cluster_num_1 = len(Roots)\n",
    "    # print(\"Cluster Num2: \", cluster_num_1)\n",
    "    \n",
    "    # data1, Roots\n",
    "    data1, Roots, Tree_Structure, Store_Radius_Density, Store_Parents, Store_Self_Density,  cluster_centers, _ = RunStreamingData(data1, updated_anomaly_dps, Roots, labels, Tree_Structure, \n",
    "                                           Store_Radius_Density, Store_Parents, Hash_Map, thresh, \n",
    "                                           updated_anomaly_dps, countimg, Store_Self_Density, cluster_num_1,\n",
    "                                          Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start,\n",
    "                                         cluster_centers, algo, alpha_weight_avg, incoming_time_dur)\n",
    "    \n",
    "    \n",
    "    print(\"--> COMPLETED ASSESSING POTENTIAL ANOMALIES\")\n",
    "    print(\"----------------------------------Rearranging6----------------------------------\")\n",
    "    RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num_1, data1, Roots)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the remaining anomalies\n",
    "    updated_anomaly_idxs = []\n",
    "    updated_anomaly_dps = []\n",
    "    ########### I changed this\n",
    "#     for key in labels:\n",
    "#         if labels[key] == -1:\n",
    "#             updated_anomaly_idxs.append(key)\n",
    "#             updated_anomaly_dps.append(data1[key])\n",
    "    for idx6 in anomaly_idxs:\n",
    "        if labels[idx6] == -1:\n",
    "            updated_anomaly_idxs.append(idx6)\n",
    "            updated_anomaly_dps.append(data1[idx6])\n",
    "    updated_anomaly_dps = np.asarray(updated_anomaly_dps)\n",
    "\n",
    "    return updated_anomaly_idxs, updated_anomaly_dps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMs5ZyoGoPPS",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def RunCheckandCorrectStreaming(data1, anomaly_idxs, LLE_lookup, anomaly_dps, \n",
    "                                Roots, Store_Radius_Density, Store_Parents, Hash_Map, thresh,\n",
    "                                Store_Self_Density, Tree_Structure, labels, Concept_Evolution_Thresh, cluster_centers,\n",
    "                               Dimension, train_end, train_start, algo, alpha_weight_avg):\n",
    "    \n",
    "    updated_anomaly_idxs, updated_anomaly_dps = CheckandCorrectStreaming(data1, anomaly_idxs, LLE_lookup, anomaly_dps, \n",
    "                                Roots, Store_Radius_Density, Store_Parents, Hash_Map, thresh,\n",
    "                                Store_Self_Density, Tree_Structure, labels, Concept_Evolution_Thresh, cluster_centers,\n",
    "                                Dimension, train_end, train_start, algo, alpha_weight_avg)\n",
    "    \n",
    "    return updated_anomaly_idxs, updated_anomaly_dps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zGroOYxoPPT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGPXnLbioPPU"
   },
   "outputs": [],
   "source": [
    "def UpdateRootsandCluster2(Roots, cluster_centers, Store_Self_Density):\n",
    "    # Roots = AdjustRoot_Streaming(Roots, labels)\n",
    "    Roots, cluster_centers = UPDATEROOTS(Roots, cluster_centers, Store_Self_Density)\n",
    "    # print(\"Current Updated Roots: \", Roots)\n",
    "\n",
    "    # print(\"\\n\")\n",
    "    cluster_num = len(Roots)\n",
    "    # print(\"Current Cluster Num: \", cluster_num)\n",
    "    \n",
    "    return Roots, cluster_centers, cluster_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFlzrzS4oPPU"
   },
   "outputs": [],
   "source": [
    "def RunRearranging7(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots):\n",
    "    print(\"----------------------------------Rearranging7----------------------------------\")\n",
    "    RearrangeTreeStructure(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, data1, Roots)\n",
    "    correct_the_idx(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GyFk5DYWoPPV"
   },
   "outputs": [],
   "source": [
    "def PlotF1(Inc_Dp, Inc_Dp_Idx, Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots):\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        \n",
    "        # ax = plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        ax.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        ax.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        ax.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        ax.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        ax.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        ax.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        ax.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        ax.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        ax.scatter(Inc_Dp[0], Inc_Dp[1], c='black', marker = 'o')\n",
    "        ax.annotate('(%.1f)'%(Inc_Dp_Idx), xy=(Inc_Dp[0], Inc_Dp[1]))\n",
    "\n",
    "\n",
    "\n",
    "        ax.set_title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPQ1WzYaoPPV"
   },
   "outputs": [],
   "source": [
    "def PlotF2(Child_Dp, Orig_Parent_Dp, Prob_Parent_Dp, Dimension, cluster_num, data1, \\\n",
    "           labels, Incoming_points, train_end, train_start, Roots):\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "                \n",
    "        plt.figure() #figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        plt.scatter(Child_Dp[0], Child_Dp[1], c='black', marker = 'X', label = 'Child')\n",
    "        plt.scatter(Orig_Parent_Dp[0], Orig_Parent_Dp[1], c='black', marker = 'o', label = 'Orig_Parent')\n",
    "        plt.scatter(Prob_Parent_Dp[0], Prob_Parent_Dp[1], c='black', marker = '^', label = 'Prob Parent')\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WuUcR02oPPW"
   },
   "outputs": [],
   "source": [
    "def FinalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots):\n",
    "    \n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        # for root in Roots:\n",
    "        #     plt.scatter(Roots[root][0][0], Roots[root][0][1], c='black', marker = 'o', s=200 )\n",
    "\n",
    "\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4bJk0PaoPPW"
   },
   "outputs": [],
   "source": [
    "def DisplayInformation(updated_anomaly_idxs, Roots):\n",
    "    print(\"Number of current Anomalies: \", len(updated_anomaly_idxs))\n",
    "    # Roots = AdjustRoot_Streaming(Roots, labels)\n",
    "    # print(\"Roots: \", Roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weTSRJjWoPPX"
   },
   "outputs": [],
   "source": [
    "def ComputeDistIdxs(data1, algo):\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "    print(\"****************Completed Computing Distance Between All Datapoints****************\")\n",
    "    return distances_ddcal, indices_ddcal\n",
    "\n",
    "def GetCardinalityofIdx(labels, label_idx):\n",
    "    Cardinality_idx = 0\n",
    "    for key in labels:\n",
    "        if labels[key] == label_idx:\n",
    "            Cardinality_idx += 1\n",
    "    return Cardinality_idx\n",
    "\n",
    "\n",
    "def IdxofDp(dp, data1):\n",
    "    for i in range(len(data1)):\n",
    "        comparison = dp == data1[i]\n",
    "        equals_array = comparison.all()\n",
    "        if equals_array == True:\n",
    "            return i\n",
    "        \n",
    "        \n",
    "def GetParentfromTree_F1(Tree_Structure, idx):\n",
    "    send_key = None\n",
    "    for key in Tree_Structure:\n",
    "        if idx in set(Tree_Structure[key]):\n",
    "            send_key = key\n",
    "            break\n",
    "    # print(\"Sending: \", send_key)\n",
    "    if send_key is None:\n",
    "        return send_key\n",
    "    else:\n",
    "        return int(send_key)\n",
    "\n",
    "\n",
    "# This function is already defined as ChangeAllLabels_F\n",
    "# def ChangeAllLabels_F1(labels, Tree_Structure, idx, changetolabel):\n",
    "#     ToChange = deque()\n",
    "#     ToChange.append(idx)\n",
    "#     visited = {key:False for key in labels}\n",
    "#     while True:\n",
    "#         # print(\"ToChange: \", ToChange)\n",
    "#         if len(ToChange) == 0:\n",
    "#             break\n",
    "#         curr_idx = ToChange.pop()\n",
    "#         labels[curr_idx] = changetolabel\n",
    "#         if curr_idx in Tree_Structure:\n",
    "#             for child_idx in Tree_Structure[curr_idx]:\n",
    "#                 if child_idx not in set(ToChange) and visited[child_idx] == False:\n",
    "#                     ToChange.appendleft(child_idx)\n",
    "#                     visited[child_idx] = True\n",
    "                    \n",
    "#     return labels\n",
    "\n",
    "\n",
    "def CorrectSpecificLabel(dps, data1, labels, indices_ddcal, LLE_lookup, Tree_Structure):\n",
    "    \n",
    "    Tree_Structure = RemoveDulplicatePositions(Tree_Structure, Store_Self_Density)\n",
    "    Tree_Structure = CorrectOneChildTwoParents(Tree_Structure, Store_Self_Density)\n",
    "    \n",
    "    percent7 = 0.05 #0.05\n",
    "    \n",
    "    thresh_5 = 10 #(for corners)\n",
    "    \n",
    "    for i in range(len(dps)):\n",
    "        curr_dp = dps[i]\n",
    "        idxofdp = IdxofDp(dps[i], data1)\n",
    "\n",
    "        # Check only if curr data point is not an anomaly\n",
    "        if labels[idxofdp] != -1:\n",
    "            \n",
    "          opt_K = LLE_lookup[idxofdp]\n",
    "          neighbour_idxs = indices_ddcal[i][1:opt_K]\n",
    "\n",
    "          for neighbour_idx in neighbour_idxs:\n",
    "              if labels[neighbour_idx] != labels[idxofdp] and labels[neighbour_idx] != -1:\n",
    "                  # print(\"NEED WORK!!\")\n",
    "                  # To do\n",
    "                  \n",
    "                  # print(\"NI: \", neighbour_idx)\n",
    "                  neighbour_dp = data1[neighbour_idx]\n",
    "                  parent_of_neighbour_idx = GetParentfromTree_F1(Tree_Structure, neighbour_idx)\n",
    "                  \n",
    "                  \n",
    "                  if parent_of_neighbour_idx == None: # or parent_of_curr_dp_idx == None:\n",
    "                      # print(\"--> ROOT OBTAINED!!\")\n",
    "                      \n",
    "                      Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                      Density_Inc_Dp = Store_Self_Density[idxofdp]\n",
    "                      list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, idxofdp, data1)\n",
    "                      data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                      weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                      numerator = abs(weighted_density - Density_neighbour)\n",
    "                      denominator = weighted_density\n",
    "                      # print(\"Weighted Density =\", numerator/denominator)\n",
    "                      # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                      \n",
    "                      cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                      if cardinality_neighbour_cluster <= 10 or numerator/denominator <= thresh:\n",
    "                          # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                          # cardinality_neighbour_cluster = GetCardinalityofIdx(labels, labels[neighbour_idx])\n",
    "                          # cardinality_curr_cluster = GetCardinalityofIdx(labels, labels[idxofdp])\n",
    "                          # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                          # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                          #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                          ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[idxofdp])\n",
    "                          RemoveNodesWithNoChild(Tree_Structure)\n",
    "                          if idxofdp in Tree_Structure:\n",
    "                              Tree_Structure[idxofdp].append(neighbour_idx)\n",
    "                          else:\n",
    "                              Tree_Structure[idxofdp] = [neighbour_idx]\n",
    "                      \n",
    "                  \n",
    "                  else:\n",
    "                      # If neighbour is not a root node\n",
    "                      if type(parent_of_neighbour_idx) != int:\n",
    "                          raise Exception(\"More than 1 idx present(3)\")\n",
    "                      parent_of_neighbour = data1[parent_of_neighbour_idx]\n",
    "                      # parent_of_curr_dp_idx = GetParentfromTree_F1(Tree_Structure, idxofdp)\n",
    "                      # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "\n",
    "\n",
    "                      Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                      Density_Inc_Dp = Store_Self_Density[idxofdp]\n",
    "                      list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, idxofdp, data1)\n",
    "                      data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                      weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                      numerator = abs(weighted_density - Density_neighbour)\n",
    "                      denominator = weighted_density\n",
    "                      # print(\"Weighted Density =\", numerator/denominator)\n",
    "                      # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "      \n",
    "                      # Find distances with the present parent and the probable parent.\n",
    "                      dist_orig_parent = np.linalg.norm(neighbour_dp - parent_of_neighbour)\n",
    "                      dist_prob_parent = np.linalg.norm(neighbour_dp - curr_dp)\n",
    "                      \n",
    "                      # Find density differences with the present parent and the probable parent.\n",
    "                      # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "                      # density_diff_with_prob_parent = Store_Self_Density[idxofdp] - Store_Self_Density[neighbour_idx]\n",
    "          \n",
    "                      ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                          \n",
    "                      if (dist_orig_parent)*0.3 <= (dist_prob_parent) and numerator/denominator > thresh: # and (dist_orig_parent) <= thresh_5:\n",
    "                      ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                          # Everything is fine. Nothing needs to be changed\n",
    "                          # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                          # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                        \n",
    "                          continue\n",
    "\n",
    "                      else:\n",
    "                          # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                          # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                          #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                          # Plot to check whats happening\n",
    "                          # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", curr_dp)\n",
    "                          # PlotF2(neighbour_dp, parent_of_neighbour, curr_dp, Dimension, cluster_num, data1, \\\n",
    "                          #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "                            \n",
    "                          if idxofdp in Tree_Structure:\n",
    "                              Tree_Structure[idxofdp].append(neighbour_idx)\n",
    "                          else:\n",
    "                              Tree_Structure[idxofdp] = [neighbour_idx]\n",
    "                          if parent_of_neighbour_idx != None:\n",
    "                              Tree_Structure[parent_of_neighbour_idx].remove(neighbour_idx)\n",
    "                          \n",
    "                          ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[idxofdp])\n",
    "                          RemoveNodesWithNoChild(Tree_Structure)\n",
    "                    \n",
    "\n",
    "                  \n",
    "                  \n",
    "              else:\n",
    "                  # Same label so do nothing\n",
    "                  continue\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMWjdxWWoPPX"
   },
   "outputs": [],
   "source": [
    "def Correct1(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh):\n",
    "\n",
    "  # Check neighbours of all datapoints. If there exists members from other clusters, check there distance with their original parent \n",
    "  # and the curr point under consideration. Whichever is smaller, include in that.\n",
    "\n",
    "  for Inc_Dp_Idx in range(len(data1)):\n",
    "    Inc_Dp = data1[Inc_Dp_Idx]\n",
    "\n",
    "    # Check only if curr data point is not an anomaly\n",
    "    if labels[Inc_Dp_Idx] != -1:\n",
    "      neighbour_idx_of_different_clusters = GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, data1)\n",
    "\n",
    "      if len(neighbour_idx_of_different_clusters) > 0:\n",
    "        # There are neighbours belonging to different cluster, so need to check\n",
    "        print(\"--> ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "\n",
    "        for neighbour_idx in neighbour_idx_of_different_clusters:\n",
    "\n",
    "            neighbour_dp = data1[neighbour_idx]\n",
    "            parent_of_neighbour_idx = GetParentfromTree_F1(Tree_Structure, neighbour_idx)\n",
    "            \n",
    "            \n",
    "            # If the neighbour is a root node\n",
    "            if parent_of_neighbour_idx == None: # or parent_of_curr_dp_idx == None:\n",
    "                # print(\"--> ROOT OBTAINED!! with idx: \", neighbour_idx, \"and label:\", labels[neighbour_idx])\n",
    "                \n",
    "                # If root is obtained, check the density of the root with the probable parent.\n",
    "                # It is to be done similar to the method of inclusion of a data point (same threshold)\n",
    "                # If the root satisfies the density criterion, include it otherwise don't\n",
    "                \n",
    "                Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "                Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "                list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx, data1)\n",
    "                data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                numerator = abs(weighted_density - Density_neighbour)\n",
    "                denominator = weighted_density\n",
    "                # print(\"Weighted Density =\", numerator/denominator)\n",
    "                # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                \n",
    "                # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "                cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                if cardinality_neighbour_cluster <= 8 and numerator/denominator <= thresh:\n",
    "                    # cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "                    # cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "                    # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "                    # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "                    #     # print(neighbour_idx, \"INCLUDED\")\n",
    "                    ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "                    RemoveNodesWithNoChild(Tree_Structure)\n",
    "                    if Inc_Dp_Idx in Tree_Structure:\n",
    "                        Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "                    else:\n",
    "                        Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "            \n",
    "            \n",
    "            else:\n",
    "              # If the neighbour is not a root Node\n",
    "              \n",
    "              if type(parent_of_neighbour_idx) != int:\n",
    "                raise Exception(\"More than 1 idx present(1)\")\n",
    "              parent_of_neighbour = data1[parent_of_neighbour_idx]\n",
    "              # parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, Inc_Dp_Idx)\n",
    "              # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "              # Not the child's parent but check with the child itself.\n",
    "\n",
    "              Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "              Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "              list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx, data1)\n",
    "              data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "              weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "              numerator = abs(weighted_density - Density_neighbour)\n",
    "              denominator = weighted_density\n",
    "              # print(\"Weighted Density =\", numerator/denominator)\n",
    "              # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "\n",
    "              # Find distances with the present parent and the probable parent.\n",
    "              dist_orig_parent = np.linalg.norm(neighbour_dp - parent_of_neighbour)\n",
    "              dist_prob_parent = np.linalg.norm(neighbour_dp - Inc_Dp)\n",
    "              \n",
    "              # Find density differences with the present parent and the probable parent.\n",
    "              # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "              # density_diff_with_prob_parent = Store_Self_Density[Inc_Dp_Idx] - Store_Self_Density[neighbour_idx]\n",
    "              \n",
    "              ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "              \n",
    "              ## print(\"Idx: \", neighbour_idx, \"Label: \", labels[neighbour_idx], \"dist_orig_parent: \", dist_orig_parent, \"dist_prob_parent: \", dist_prob_parent)\n",
    "              ## For the Correct1 I am not including the density criteria\n",
    "              if (dist_orig_parent)*0.3 <= (dist_prob_parent) and numerator/denominator > thresh: ## and (dist_orig_parent) <= thresh_5:\n",
    "              ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "                # print(\"HERE1\")\n",
    "                # Everything is fine. Nothing needs to be changed\n",
    "                # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "                # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "                # print(\"HERE2\")\n",
    "                continue\n",
    "\n",
    "              else:\n",
    "                # print(\"HERE3\")\n",
    "                # 1. Change the neighbour's parent as the parent of the current data point\n",
    "                # 2. If there are children of the neigbour, include them all into this cluster\n",
    "                #    and under the current data point in the Tree_Structure\n",
    "\n",
    "                # Plot to check whats happening\n",
    "                # print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", Inc_Dp)\n",
    "                # PlotF2(neighbour_dp, parent_of_neighbour, Inc_Dp, Dimension, cluster_num, data1, \\\n",
    "                #       labels, Incoming_points, train_end, train_start, Roots)\n",
    "\n",
    "                if Inc_Dp_Idx in Tree_Structure:\n",
    "                  Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "                else:\n",
    "                  Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "                if parent_of_neighbour_idx != None:\n",
    "                  Tree_Structure[parent_of_neighbour_idx].remove(neighbour_idx)\n",
    "\n",
    "                # print(\"HERE4\")\n",
    "                ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "                RemoveNodesWithNoChild(Tree_Structure)\n",
    "                # print(\"HERE5\")\n",
    "\n",
    "        print(\"--> COMPLETED ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "        \n",
    "  # clear_output(wait=True)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Seen_1 = set()\n",
    "    # for key in labels:\n",
    "    #     if labels[key] not in Seen_1 and labels[key] != -1:\n",
    "    #         Seen_1.add(labels[key])\n",
    "    # print(\"Labels: \", Seen_1)\n",
    "\n",
    "    # See_1_list = list(Seen_1)\n",
    "\n",
    "\n",
    "    # Cardinality_list = []\n",
    "    # for idx in Seen_1:\n",
    "    #     Cardinality_idx = GetCardinalityofIdx(labels, idx)\n",
    "    #     Cardinality_list.append(Cardinality_idx)\n",
    "\n",
    "    # print(\"Cardinality: \", Cardinality_list)\n",
    "\n",
    "    # Joined = []\n",
    "    # for i in range(len(Cardinality_list)):\n",
    "    #     Joined.append([Cardinality_list[i], See_1_list[i]])\n",
    "\n",
    "    # # print(Joined)\n",
    "\n",
    "    # Joined.sort(key=itemgetter(0), reverse = True)\n",
    "\n",
    "    # print(\"Combined: \", Joined)\n",
    "\n",
    "\n",
    "    # distances_ddcal, indices_ddcal = ComputeDistIdxs(data1, algo)\n",
    "\n",
    "    # for pair in Joined:\n",
    "    #     curr_label = pair[1]\n",
    "    #     send_dps = []\n",
    "    #     for dp in data1:\n",
    "    #         idxofdp = IdxofDp(dp, data1)\n",
    "    #         if labels[idxofdp] == curr_label:\n",
    "    #             send_dps.append(dp)\n",
    "\n",
    "    #     CorrectSpecificLabel(send_dps, data1, labels, indices_ddcal, LLE_lookup, Tree_Structure)\n",
    "\n",
    "\n",
    "# Roots, cluster_centers, cluster_num = UpdateRootsandCluster2(Roots, cluster_centers, Store_Self_Density)\n",
    "# labels = RunRearranging7(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots)\n",
    "# FinalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tza5bJQDOM92"
   },
   "outputs": [],
   "source": [
    "# def Correct2(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh):\n",
    "\n",
    "#   # Check neighbours of all datapoints. If there exists members from other clusters, check there distance with their original parent \n",
    "#   # and the curr point under consideration. Whichever is smaller, include in that.\n",
    "\n",
    "#   for Inc_Dp_Idx in range(len(data1)):\n",
    "#     Inc_Dp = data1[Inc_Dp_Idx]\n",
    "\n",
    "#     # Check only if curr data point is not an anomaly\n",
    "#     if labels[Inc_Dp_Idx] != -1:\n",
    "#       neighbour_idx_of_different_clusters = GetNeighboursofDifferentCluster(Inc_Dp_Idx, LLE_lookup, labels, data1)\n",
    "\n",
    "#       if len(neighbour_idx_of_different_clusters) > 0:\n",
    "#         # There are neighbours belonging to different cluster, so need to check\n",
    "#         print(\"--> ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "\n",
    "#         for neighbour_idx in neighbour_idx_of_different_clusters:\n",
    "\n",
    "#             neighbour_dp = data1[neighbour_idx]\n",
    "#             parent_of_neighbour_idx = GetParentfromTree_F1(Tree_Structure, neighbour_idx)\n",
    "            \n",
    "            \n",
    "#             # If the neighbour is a root node\n",
    "#             if parent_of_neighbour_idx == None: # or parent_of_curr_dp_idx == None:\n",
    "#                 # print(\"--> ROOT OBTAINED!! with idx: \", neighbour_idx, \"and label:\", labels[neighbour_idx])\n",
    "                \n",
    "#                 # If root is obtained, check the density of the root with the probable parent.\n",
    "#                 # It is to be done similar to the method of inclusion of a data point (same threshold)\n",
    "#                 # If the root satisfies the density criterion, include it otherwise don't\n",
    "                \n",
    "#                 Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "#                 Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "#                 list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx)\n",
    "#                 data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "#                 weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "#                 numerator = abs(weighted_density - Density_neighbour)\n",
    "#                 denominator = weighted_density\n",
    "#                 # print(\"Weighted Density =\", numerator/denominator)\n",
    "#                 # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "                \n",
    "#                 # print(\"++++Numerator: \", numerator, \"Denominator: \", denominator, \"Frac: \", numerator/denominator)\n",
    "#                 cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "#                 if cardinality_neighbour_cluster <= 8 and numerator/denominator <= thresh:\n",
    "#                     # cardinality_neighbour_cluster = GetCardinality_F(labels, neighbour_idx)\n",
    "#                     # cardinality_curr_cluster = GetCardinality_F(labels, Inc_Dp_Idx)\n",
    "#                     # # print(\"Percent required: \", cardinality_neighbour_cluster/cardinality_curr_cluster)\n",
    "#                     # if cardinality_neighbour_cluster/cardinality_curr_cluster <= percent7:\n",
    "#                     #     # print(neighbour_idx, \"INCLUDED\")\n",
    "#                     ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "#                     RemoveNodesWithNoChild(Tree_Structure)\n",
    "#                     if Inc_Dp_Idx in Tree_Structure:\n",
    "#                         Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "#                     else:\n",
    "#                         Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "            \n",
    "            \n",
    "#             else:\n",
    "#               # If the neighbour is not a root Node\n",
    "              \n",
    "#               if type(parent_of_neighbour_idx) != int:\n",
    "#                 raise Exception(\"More than 1 idx present(1)\")\n",
    "#               parent_of_neighbour = data1[parent_of_neighbour_idx]\n",
    "#               # parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, Inc_Dp_Idx)\n",
    "#               # parent_of_curr_dp = data1[parent_of_curr_dp_idx]\n",
    "#               # Not the child's parent but check with the child itself.\n",
    "\n",
    "#               Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "#               Density_Inc_Dp = Store_Self_Density[Inc_Dp_Idx]\n",
    "#               list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, Inc_Dp_Idx)\n",
    "#               data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "#               weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "#               numerator = abs(weighted_density - Density_neighbour)\n",
    "#               denominator = weighted_density\n",
    "#               # print(\"Weighted Density =\", numerator/denominator)\n",
    "#               # print(\"CIN: \", Child_Idx, \"N: \", numerator, \"D: \", denominator)\n",
    "\n",
    "#               # Find distances with the present parent and the probable parent.\n",
    "#               dist_orig_parent = np.linalg.norm(neighbour_dp - parent_of_neighbour)\n",
    "#               dist_prob_parent = np.linalg.norm(neighbour_dp - Inc_Dp)\n",
    "              \n",
    "#               # Find density differences with the present parent and the probable parent.\n",
    "#               # density_diff_with_orig_parent = Store_Self_Density[parent_of_neighbour_idx] - Store_Self_Density[neighbour_idx]\n",
    "#               # density_diff_with_prob_parent = Store_Self_Density[Inc_Dp_Idx] - Store_Self_Density[neighbour_idx]\n",
    "              \n",
    "#               ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "              \n",
    "#               ## print(\"Idx: \", neighbour_idx, \"Label: \", labels[neighbour_idx], \"dist_orig_parent: \", dist_orig_parent, \"dist_prob_parent: \", dist_prob_parent)\n",
    "#               ## For the Correct1 I am not including the density criteria\n",
    "#               if (dist_orig_parent)*0.3 <= (dist_prob_parent): # or numerator/denominator > thresh: ## and (dist_orig_parent) <= thresh_5:\n",
    "#               ## if (dist_orig_parent+density_diff_with_orig_parent) <= (dist_prob_parent+density_diff_with_prob_parent):\n",
    "#                 # print(\"HERE1\")\n",
    "#                 # Everything is fine. Nothing needs to be changed\n",
    "#                 # print(\"F: \", dist_orig_parent, \"\\t\", dist_prob_parent)\n",
    "#                 # print(\"--> Everything is fine. Nothing needs to be changed\")\n",
    "#                 # print(\"HERE2\")\n",
    "#                 continue\n",
    "\n",
    "#               else:\n",
    "#                 # print(\"HERE3\")\n",
    "#                 # 1. Change the neighbour's parent as the parent of the current data point\n",
    "#                 # 2. If there are children of the neigbour, include them all into this cluster\n",
    "#                 #    and under the current data point in the Tree_Structure\n",
    "\n",
    "#                 # Plot to check whats happening\n",
    "#                 print(\"NDp: \", neighbour_dp, \"PN: \", parent_of_neighbour, \"ProP: \", Inc_Dp)\n",
    "#                 PlotF2(neighbour_dp, parent_of_neighbour, Inc_Dp, Dimension, cluster_num, data1, \\\n",
    "#                       labels, Incoming_points, train_end, train_start, Roots)\n",
    "\n",
    "#                 if Inc_Dp_Idx in Tree_Structure:\n",
    "#                   Tree_Structure[Inc_Dp_Idx].append(neighbour_idx)\n",
    "#                 else:\n",
    "#                   Tree_Structure[Inc_Dp_Idx] = [neighbour_idx]\n",
    "#                 if parent_of_neighbour_idx != None:\n",
    "#                   Tree_Structure[parent_of_neighbour_idx].remove(neighbour_idx)\n",
    "\n",
    "#                 # print(\"HERE4\")\n",
    "#                 ChangeAllLabels_F(labels, Tree_Structure, neighbour_idx, labels[Inc_Dp_Idx])\n",
    "#                 RemoveNodesWithNoChild(Tree_Structure)\n",
    "#                 # print(\"HERE5\")\n",
    "\n",
    "#         print(\"--> COMPLETED ASSESSING NEIGHBOURS BELONGING TO DIFFERENT CLUSTER\")\n",
    "        \n",
    "#   # clear_output(wait=True)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # Seen_1 = set()\n",
    "#     # for key in labels:\n",
    "#     #     if labels[key] not in Seen_1 and labels[key] != -1:\n",
    "#     #         Seen_1.add(labels[key])\n",
    "#     # print(\"Labels: \", Seen_1)\n",
    "\n",
    "#     # See_1_list = list(Seen_1)\n",
    "\n",
    "\n",
    "#     # Cardinality_list = []\n",
    "#     # for idx in Seen_1:\n",
    "#     #     Cardinality_idx = GetCardinalityofIdx(labels, idx)\n",
    "#     #     Cardinality_list.append(Cardinality_idx)\n",
    "\n",
    "#     # print(\"Cardinality: \", Cardinality_list)\n",
    "\n",
    "#     # Joined = []\n",
    "#     # for i in range(len(Cardinality_list)):\n",
    "#     #     Joined.append([Cardinality_list[i], See_1_list[i]])\n",
    "\n",
    "#     # # print(Joined)\n",
    "\n",
    "#     # Joined.sort(key=itemgetter(0), reverse = True)\n",
    "\n",
    "#     # print(\"Combined: \", Joined)\n",
    "\n",
    "\n",
    "#     # distances_ddcal, indices_ddcal = ComputeDistIdxs(data1, algo)\n",
    "\n",
    "#     # for pair in Joined:\n",
    "#     #     curr_label = pair[1]\n",
    "#     #     send_dps = []\n",
    "#     #     for dp in data1:\n",
    "#     #         idxofdp = IdxofDp(dp, data1)\n",
    "#     #         if labels[idxofdp] == curr_label:\n",
    "#     #             send_dps.append(dp)\n",
    "\n",
    "#     #     CorrectSpecificLabel(send_dps, data1, labels, indices_ddcal, LLE_lookup, Tree_Structure)\n",
    "\n",
    "\n",
    "# # Roots, cluster_centers, cluster_num = UpdateRootsandCluster2(Roots, cluster_centers, Store_Self_Density)\n",
    "# # labels = RunRearranging7(Tree_Structure, data1, labels, Store_Self_Density, cluster_num, Roots)\n",
    "# # FinalPlot(Dimension, cluster_num, data1, labels, Incoming_points, train_end, train_start, Roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPShGhMToPPY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3s0lcIoBoPPY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PekMM5nGoPPZ",
    "outputId": "3d5e989b-0fe1-4134-df79-975ea33b8058"
   },
   "outputs": [],
   "source": [
    "# Name: Get Incoming Points\n",
    "\n",
    "if 'Name: Get Incoming Points' not in SM:\n",
    "    data_all = SM['data_all']\n",
    "    test_start = SM['test_start']\n",
    "    test_end = SM['test_end']\n",
    "\n",
    "    # \"\"\"\n",
    "    # print(Roots)\n",
    "    # Incoming_points = np.array([[5,5],\n",
    "    #                    [5,-5],\n",
    "    #                    [-10,5],\n",
    "    #                    [-6,-6],\n",
    "    #                    [1,1],\n",
    "    #                    [1,-1],\n",
    "    #                    [-1,1],\n",
    "    #                    [-1,-1],\n",
    "    #                    [-12,-12],\n",
    "    #                    [3,3],\n",
    "    #                    [10,-3],\n",
    "    #                    [-2,-10],\n",
    "    #                    [-2,12],\n",
    "    #                    [-10,2],\n",
    "    #                    [-5,-5]])\n",
    "\n",
    "    # \"\"\"\n",
    "\n",
    "    Incoming_points = data_all[test_start:test_end+1]\n",
    "    print(\"Testing Length: \", len(Incoming_points))\n",
    "\n",
    "    SM['Incoming_points'] = Incoming_points\n",
    "    SM['Name: Get Incoming Points'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8dU9or7oPPZ"
   },
   "outputs": [],
   "source": [
    "# Name: Display5\n",
    "\n",
    "if 'Name: Display5' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "    Roots = SM['Roots']\n",
    "    \n",
    "    # \"\"\"\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        for j in range(len(Incoming_points)):\n",
    "            plt.scatter(Incoming_points[j][0],Incoming_points[j][1],c='black',marker='*',s=200)\n",
    "\n",
    "        for root in Roots:\n",
    "            plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "\n",
    "\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization After Outlier Post Processing')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "    # \"\"\"\n",
    "    \n",
    "    SM['Name: Display5'] = True\n",
    "    SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O8KxDd8oPPa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jfn48XpwoPPa",
    "outputId": "d78a3b42-c41d-4871-d21e-d3a937033719",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Run Streaming Data\n",
    "\n",
    "if 'Name: Run Streaming Data' not in SM:\n",
    "    data1 = SM['data1']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "    Roots = SM['Roots']\n",
    "    labels = SM['labels']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    Hash_Map = SM['Hash_Map']\n",
    "    thresh = SM['thresh']\n",
    "    anomaly_list = SM['anomaly_list']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Concept_Evolution_Thresh = SM['Concept_Evolution_Thresh']\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    Dimension = SM['Dimension']\n",
    "    train_end = SM['train_end']\n",
    "    train_start = SM['train_start']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    algo = SM['algo']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    incoming_time_dur = SM['incoming_time_dur']\n",
    "    \n",
    "\n",
    "    countimg = [0]\n",
    "\n",
    "    # print(Tree_Structure)\n",
    "    # Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "    # print(Supposed_Roots)\n",
    "\n",
    "\n",
    "    data1, Roots, Tree_Structure, Store_Radius_Density, Store_Parents, Store_Self_Density,  cluster_centers, Total_Time = \\\n",
    "                        RunStreamingData(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                             Store_Parents, Hash_Map, thresh, anomaly_list, countimg, Store_Self_Density, cluster_num,\n",
    "                                Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "                        algo, alpha_weight_avg, incoming_time_dur)\n",
    "\n",
    "    # Correct1(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh)\n",
    "    # Correct2(labels, data1, algo, LLE_lookup, Tree_Structure, Store_Self_Density, alpha_weight_avg, thresh)\n",
    "\n",
    "    SM['data1'] = data1\n",
    "    SM['Roots'] = Roots\n",
    "    SM['Tree_Structure'] = Tree_Structure\n",
    "    SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "    SM['Store_Self_Density'] = Store_Self_Density\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "    SM['Total_Time'] = Total_Time\n",
    "    SM['labels'] = labels\n",
    "    SM['LLE_lookup'] = LLE_lookup\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['Hash_Map'] = Hash_Map\n",
    "    SM['Store_Parents'] = Store_Parents\n",
    "\n",
    "    SM['Name: Run Streaming Data'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: RunConceptEvolution Last Time\n",
    "\n",
    "if 'Name: RunConceptEvolution Last Time' not in SM:\n",
    "    \n",
    "    data1 = SM['data1']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "    Roots = SM['Roots']\n",
    "    labels = SM['labels']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    Hash_Map = SM['Hash_Map']\n",
    "    thresh = SM['thresh']\n",
    "    anomaly_list = SM['anomaly_list']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Concept_Evolution_Thresh = SM['Concept_Evolution_Thresh']\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    Dimension = SM['Dimension']\n",
    "    train_end = SM['train_end']\n",
    "    train_start = SM['train_start']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    algo = SM['algo']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    incoming_time_dur = SM['incoming_time_dur']\n",
    "    \n",
    "    # print(\"Inc:\", incoming_time_dur)\n",
    "\n",
    "    ## Run the Concept Evolution for the last time\n",
    "    data1, Roots, cluster_centers, labels, updated_anomaly_dps, updated_anomaly_idxs, \\\n",
    "    Tree_Structure, Store_Self_Density, cluster_num, Store_Radius_Density, Store_Parents, Hash_Map = \\\n",
    "    RunConceptEvolution(data1, Incoming_points, Roots, labels, Tree_Structure, Store_Radius_Density, \n",
    "                         Store_Parents, Hash_Map, thresh, anomaly_list, Store_Self_Density, cluster_num,\n",
    "                        Concept_Evolution_Thresh, LLE_lookup, Dimension, train_end, train_start, cluster_centers,\n",
    "                       algo, alpha_weight_avg)\n",
    "\n",
    "    SM['data1'] = data1\n",
    "    SM['Roots'] = Roots\n",
    "    SM['Tree_Structure'] = Tree_Structure\n",
    "    SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "    SM['Store_Parents'] = Store_Parents\n",
    "    SM['Store_Self_Density'] = Store_Self_Density\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "    SM['labels'] = labels\n",
    "    SM['LLE_lookup'] = LLE_lookup\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['Hash_Map'] = Hash_Map\n",
    "\n",
    "    SM['Name: RunConceptEvolution Last Time'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7nOiqKwoPP8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for item in Tree_Structure.keys():\n",
    "#     print(\"item: \", item, \"labels: \", labels[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUjKYem-oPP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKwxa0_xoPP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc9GQFrooPP9"
   },
   "outputs": [],
   "source": [
    "# Name: Display6\n",
    "\n",
    "if 'Name: Display6' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "    Roots = SM['Roots']\n",
    "    train_end = SM['train_end']\n",
    "    train_start = SM['train_start']\n",
    "\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        # for root in Roots:\n",
    "        #     plt.scatter(Roots[root][0][0], Roots[root][0][1], c='black', marker = 'o', s=200 )\n",
    "\n",
    "\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "\n",
    "        SM['Name: Display6'] = True\n",
    "        SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOZ5LNM3oPP9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzG_5mVRoPP9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dist_with_parent = []\n",
    "# for key in labels:\n",
    "#     if labels[key] == 1:\n",
    "#         dp = data1[key]\n",
    "#         parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, key)\n",
    "#         if parent_of_curr_dp_idx is not None:\n",
    "#             dist_with_parent.append([np.linalg.norm(dp - data1[parent_of_curr_dp_idx]), key])\n",
    "        \n",
    "# dist_with_parent.sort(key=itemgetter(0), reverse = True)\n",
    "\n",
    "# print(dist_with_parent)\n",
    "\n",
    "\n",
    "# dist_with_parent_1 = []\n",
    "# for key in labels:\n",
    "#     if labels[key] == 2:\n",
    "#         dp = data1[key]\n",
    "#         parent_of_curr_dp_idx = GetParentfromTree_F(Tree_Structure, key)\n",
    "#         if parent_of_curr_dp_idx is not None:\n",
    "#             dist_with_parent_1.append([np.linalg.norm(dp - data1[parent_of_curr_dp_idx]), key])\n",
    "        \n",
    "# dist_with_parent_1.sort(key=itemgetter(0), reverse = True)\n",
    "\n",
    "# print(\"\\n\", dist_with_parent_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I937ZvProPP-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdS8FaL9oPP-"
   },
   "outputs": [],
   "source": [
    "# Name: Check Which Nodes Have No Parent In Tree\n",
    "\n",
    "if 'Name: Check Which Nodes Have No Parent In Tree' not in SM:\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    # Roots\n",
    "    Supposed_Roots = CheckWhichNodesHaveNoParentInTree(Tree_Structure)\n",
    "    print(Supposed_Roots)\n",
    "\n",
    "    SM['Supposed_Roots'] = Supposed_Roots\n",
    "    \n",
    "    SM['Name: Check Which Nodes Have No Parent In Tree'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHNHsadCJ-ei"
   },
   "outputs": [],
   "source": [
    "# Name: Operation 1\n",
    "\n",
    "if 'Name: Operation 1' not in SM:\n",
    "    data1 = SM['data1']\n",
    "    algo = SM['algo']\n",
    "    labels = SM['labels']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "\n",
    "    nbrs_ddcal_updated = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "    distances_ddcal_updated, indices_ddcal_updated = nbrs_ddcal_updated.kneighbors(data1)\n",
    "\n",
    "    for i in range(len(data1)):\n",
    "        if labels[i] == -1:\n",
    "            Store_Radius_Density[i] = [0, 0]\n",
    "            Store_Parents[i] = [0]\n",
    "            Store_Radius_Density[i] = [0]\n",
    "\n",
    "    SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "    SM['Store_Parents'] = Store_Parents\n",
    "    with open('distances_ddcal_updated.p', 'wb') as pfile1:\n",
    "        pickle.dump(distances_ddcal_updated, pfile1, protocol=4)\n",
    "    # SM['distances_ddcal_updated'] = distances_ddcal_updated\n",
    "    with open('indices_ddcal_updated.p', 'wb') as pfile2:\n",
    "        pickle.dump(indices_ddcal_updated, pfile2, protocol=4)\n",
    "    # SM['indices_ddcal_updated'] = indices_ddcal_updated\n",
    "    # SM['nbrs_ddcal_updated'] = nbrs_ddcal_updated\n",
    "    \n",
    "    SM['Name: Operation 1'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbJGGNgFK0sf"
   },
   "outputs": [],
   "source": [
    "# Name: Show1\n",
    "\n",
    "if 'Name: Show1' not in SM:\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    \n",
    "\n",
    "    print(len(Store_Self_Density))\n",
    "    print(len(Store_Parents))\n",
    "    \n",
    "    SM['Name: Show1'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq02cQcFOM-w"
   },
   "outputs": [],
   "source": [
    "# Name: Check If Present1\n",
    "\n",
    "if 'Name: Check If Present1' not in SM:\n",
    "    Roots = SM['Roots']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    \n",
    "    def CheckifPresent(cluster_centers, item1):\n",
    "        boolean = False\n",
    "        for pair in cluster_centers:\n",
    "            item2 = pair[0]\n",
    "            comparison = item1 == item2\n",
    "            equals_array = comparison.all()\n",
    "            if equals_array == True:\n",
    "                boolean = True\n",
    "                break\n",
    "        return boolean\n",
    "\n",
    "\n",
    "    for root in Roots:\n",
    "        item1 = Roots[root][0]\n",
    "        if CheckifPresent(cluster_centers, item1) == False:\n",
    "            idx_item1 = GetIdxFromWholeDataset(data1, item1)\n",
    "            label_item1 = labels[idx_item1]\n",
    "            cluster_centers.append([item1.tolist(), label_item1])\n",
    "\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "\n",
    "    SM['Name: Check If Present1'] = True\n",
    "\n",
    "    # print(cluster_centers)\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5o8MphASOM-w"
   },
   "outputs": [],
   "source": [
    "# Name: Operation2\n",
    "\n",
    "if 'Name: Operation2' not in SM:\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    Roots = SM['Roots']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "    \n",
    "    \n",
    "    # Getting already present labels\n",
    "    curr_center_idxs = set()\n",
    "    for item in cluster_centers:\n",
    "        if item[1] not in curr_center_idxs:\n",
    "            curr_center_idxs.add(item[1])\n",
    "    # print(curr_center_idxs)\n",
    "\n",
    "\n",
    "    # Getting labels which have not been considered as cluster centers\n",
    "    labels_to_add = set()\n",
    "    idxs_to_add = dict()\n",
    "    for key in labels:\n",
    "        if labels[key] not in curr_center_idxs and labels[key] != -1:\n",
    "            labels_to_add.add(labels[key])\n",
    "            if labels[key] not in idxs_to_add:\n",
    "                idxs_to_add[labels[key]] = key\n",
    "    print(labels_to_add)\n",
    "    print(idxs_to_add)\n",
    "\n",
    "\n",
    "    # Adding the left-over labels to cluster_centers\n",
    "    for label in list(labels_to_add):\n",
    "        idx_label = idxs_to_add[label]\n",
    "        item1 = data1[idx_label]\n",
    "        cluster_centers.append([item1.tolist(), label])\n",
    "\n",
    "    # Adding the left-over labels to Roots as well\n",
    "    for label in list(labels_to_add):\n",
    "        idx_label = idxs_to_add[label]\n",
    "        item1 = data1[idx_label]\n",
    "        Roots[idx_label] = [item1, Store_Self_Density[idx_label]]\n",
    "        \n",
    "    SM['Roots'] = Roots\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "    \n",
    "    SM['Name: Operation2'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0h1qOvm4J-yU"
   },
   "outputs": [],
   "source": [
    "# Name: Outlier Post Processing Last Time\n",
    "\n",
    "if 'Name: Outlier Post Processing Last Time' not in SM:\n",
    "    labels = SM['labels']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    data1 = SM['data1']\n",
    "    Roots = SM['Roots']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    algo = SM['algo']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    with open('distances_ddcal_updated.p', 'rb') as pfile1:\n",
    "        distances_ddcal_updated = pickle.load(pfile1)\n",
    "    # distances_ddcal_updated = SM['distances_ddcal_updated']\n",
    "    with open('indices_ddcal_updated.p', 'rb') as pfile2:\n",
    "        indices_ddcal_updated = pickle.load(pfile2)\n",
    "    # indices_ddcal_updated = SM['indices_ddcal_updated']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    Store_Self_Density = SM['Store_Self_Density']\n",
    "\n",
    "    Roots, labels, Tree_Structure = Correcting_wrong_clustering(labels, cluster_centers, data1, Roots, Tree_Structure)\n",
    "\n",
    "    OutlierPostProcessing(LLE_lookup, algo, data1, labels, percent5, alpha_weight_avg, \n",
    "                              Store_Radius_Density, distances_ddcal_updated, indices_ddcal_updated, Store_Parents, \n",
    "                          Store_Self_Density, Tree_Structure)\n",
    "\n",
    "\n",
    "    SM['Roots'] = Roots\n",
    "    SM['labels'] = labels\n",
    "    SM['Tree_Structure'] = Tree_Structure\n",
    "    SM['Name: Outlier Post Processing Last Time'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsn7Fb_rKrLQ"
   },
   "outputs": [],
   "source": [
    "# Name: Display7\n",
    "\n",
    "if 'Name: Display7' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "    Roots = SM['Roots']\n",
    "    train_end = SM['train_end']\n",
    "    train_start = SM['train_start']\n",
    "\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "        if cluster_num <= len(color):\n",
    "            for i in labels:\n",
    "                # print(\"i: \", i, \"i-limit_end: \", i-limit_end, \"i+train_start: \", i+train_start, \"i-(train_end-train_start)\", i-(train_end-train_start))\n",
    "                if labels[i] != -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1], c=color[labels[i]-1])\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except IndexError:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in labels:\n",
    "                if labels[i] == -1:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0],Incoming_points[i-(train_end-train_start)][1],c='green',marker='*',s=150)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "                    except:\n",
    "                        plt.scatter(Incoming_points[i-(train_end-train_start)][0], Incoming_points[i-(train_end-train_start)][1],c='blue', label='Datapoints')\n",
    "\n",
    "\n",
    "        # for root in Roots:\n",
    "        #     plt.scatter(Roots[root][0][0], Roots[root][0][1], c='gold', marker = 'o', s=200 )\n",
    "\n",
    "\n",
    "\n",
    "        plt.title(\"Datapoints Visualization For Incoming Data Points after Processing\")\n",
    "\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        plt.close()\n",
    "\n",
    "        SM['Name: Display7'] = True\n",
    "        SM.sync()\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYwwXjTOoPP-"
   },
   "outputs": [],
   "source": [
    "# Roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lpZ0qinGt5z",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nbrs_ddcal = NearestNeighbors(n_neighbors=len(data1), algorithm=algo).fit(data1)\n",
    "# distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "# for i in range(len(data1)):\n",
    "    \n",
    "#     if labels[i] != -1:\n",
    "#         parent_of_curr_pt = GetParentfromTree_F1(Tree_Structure, i)\n",
    "#         Orig_Parent_Dp = data1[parent_of_curr_pt]\n",
    "#         Inc_Dp = data1[i]\n",
    "#         opt_k = LLE_lookup[i]\n",
    "#         neighbour_idxs = indices_ddcal[i][1:opt_k]\n",
    "\n",
    "#         for neighbour_idx in neighbour_idxs:\n",
    "#             if labels[neighbour_idx] != labels[i]:\n",
    "#                 neighbour_dp = data1[neighbour_idx]\n",
    "                \n",
    "                \n",
    "#                 if parent_of_curr_pt == None:\n",
    "#                     Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "#                     Density_Inc_Dp = Store_Self_Density[i]\n",
    "#                     list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx)\n",
    "#                     data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "#                     weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "#                     numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "#                     denominator = weighted_density\n",
    "#                     cardinality_curr_cluster = GetCardinality_F(labels, i)\n",
    "#                     if cardinality_curr_cluster <= 8 and numerator/denominator <= thresh:\n",
    "#                         print(\"CHANGE DESPITE BEING ROOT\")\n",
    "                    \n",
    "#                 else:\n",
    "#                     # if parent_of_curr_pt is not None:\n",
    "#                     # Find distances with the present parent and the probable parent.\n",
    "#                     dist_orig_parent = np.linalg.norm(Inc_Dp - Orig_Parent_Dp)\n",
    "#                     dist_prob_parent = np.linalg.norm(Inc_Dp - neighbour_dp)\n",
    "                    \n",
    "#                     Density_neighbour = Store_Self_Density[neighbour_idx]\n",
    "#                     Density_Inc_Dp = Store_Self_Density[i]\n",
    "#                     list_of_parents = GetListofDensityofParents(Store_Self_Density, Tree_Structure, neighbour_idx)\n",
    "#                     data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "#                     weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "#                     numerator = abs(weighted_density - Density_Inc_Dp)\n",
    "#                     denominator = weighted_density\n",
    "\n",
    "#                     if (dist_orig_parent)*0.3 >= dist_prob_parent and numerator/denominator <= thresh:\n",
    "#                         print(\"INC_Dp: \", Inc_Dp, \"Orig_Parent_Dp: \", Orig_Parent_Dp)\n",
    "#                         print(\"(dist_orig_parent)*0.3: \", (dist_orig_parent)*0.3, \"dist_prob_parent: \", dist_prob_parent,\\\n",
    "#                               \"frac: \", numerator/denominator)\n",
    "#                         PlotF2(Inc_Dp, Orig_Parent_Dp, neighbour_dp, Dimension, cluster_num, data1, \\\n",
    "#                                labels, Incoming_points, train_end, train_start, Roots)\n",
    "#                         if neighbour_idx in Tree_Structure:\n",
    "#                             Tree_Structure[neighbour_idx].append(i)\n",
    "#                         else:\n",
    "#                             Tree_Structure[neighbour_idx] = [i]\n",
    "#                         if parent_of_curr_pt != None:\n",
    "#                             Tree_Structure[parent_of_curr_pt].remove(i)\n",
    "\n",
    "#                         ChangeAllLabels_F(labels, Tree_Structure, i, labels[neighbour_idx])\n",
    "#                         RemoveNodesWithNoChild(Tree_Structure)\n",
    "#             #             print(\"\")\n",
    "#             #             labels[i] = labels[neighbour_idx]\n",
    "#                         break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lIbgAaOSGt-D"
   },
   "outputs": [],
   "source": [
    "# Name: Show2\n",
    "\n",
    "if 'Name: Show2' not in SM:\n",
    "    data_all = SM['data_all']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    \n",
    "    print(len(data_all))\n",
    "    print(len(data1))\n",
    "    print(len(labels))\n",
    "\n",
    "    # for i in range(len(data_all)):\n",
    "    #     if i not in labels:\n",
    "    #         print(\"F\", i)\n",
    "            \n",
    "    SM['Name: Show2'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wvC8ZhVoPO3"
   },
   "outputs": [],
   "source": [
    "# Name: Outlier Detection\n",
    "\n",
    "if 'Name: Outlier Detection' not in SM:\n",
    "    Outlier_Detection = SM['Outlier_Detection']\n",
    "    \n",
    "    if Outlier_Detection:\n",
    "        def OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs):\n",
    "            # confusion matrix in sklearn\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            from sklearn.metrics import classification_report\n",
    "\n",
    "            # actual values\n",
    "            actual = actual_labels #[1,0,0,1,0,0,1,0,0,1]\n",
    "            # predicted values\n",
    "            predicted = pred_labels #[1,0,0,1,0,0,0,1,0,0]\n",
    "\n",
    "            if len(actual) != len(predicted):\n",
    "                trim = len(predicted)\n",
    "                actual = actual[:trim]\n",
    "\n",
    "            # confusion matrix\n",
    "            matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "            #print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "            # outcome values order in sklearn\n",
    "            tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "            #print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "            # classification report for precision, recall f1-score and accuracy\n",
    "            matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "            print('Classification report : \\n',matrix)\n",
    "\n",
    "\n",
    "            print(\"\\n\")\n",
    "            DB_Index = davies_bouldin_score(data1, labels_Idxs)\n",
    "            print(\"DB Index =\", DB_Index)\n",
    "            distances = pairwise_distances(data1)\n",
    "            Dunn_Index = dunn(distances, labels_Idxs)\n",
    "            print(\"Dunn Index =\", Dunn_Index)\n",
    "            Sil_Score = silhouette_score(data1, labels_Idxs)\n",
    "            print('Silhouette Score =', Sil_Score)\n",
    "            Cal_Har_Score = metrics.calinski_harabasz_score(data1, labels_Idxs)\n",
    "            print('Calinski-Harabasz Index =', Cal_Har_Score)\n",
    "            \n",
    "            SM['Classification report'] = matrix\n",
    "            SM['DB_Index'] = DB_Index\n",
    "            SM['Dunn_Index'] = Dunn_Index\n",
    "            SM['Sil_Score'] = Sil_Score\n",
    "            SM['Cal_Har_Score'] = Cal_Har_Score\n",
    "\n",
    "        def Actual_labels(Ground_Truth_data_path):\n",
    "            with open(Ground_Truth_data_path, 'r') as f:\n",
    "                actual_labels = np.genfromtxt(f, delimiter=',')\n",
    "            return actual_labels\n",
    "\n",
    "        def GetLabelsForOutlierDet(labels):\n",
    "            get_labels_OutlierDet = []\n",
    "            for key in labels:\n",
    "                if labels[key] != -1:\n",
    "                    get_labels_OutlierDet.append(0)\n",
    "                else:\n",
    "                    get_labels_OutlierDet.append(1)\n",
    "            return get_labels_OutlierDet\n",
    "\n",
    "        # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "        def GetLabelsForIdxs(labels):\n",
    "            get_labels_DB = []\n",
    "            for key in labels:\n",
    "                if labels[key] > 0:\n",
    "                    get_labels_DB.append(labels[key]-1)\n",
    "                else:\n",
    "                    get_labels_DB.append(labels[key])\n",
    "            return get_labels_DB\n",
    "        \n",
    "        Ground_Truth_data_path = SM['Ground_Truth_data_path']\n",
    "        labels = SM['labels']\n",
    "        data1 = SM['data1']\n",
    "        \n",
    "        actual_labels = Actual_labels(Ground_Truth_data_path)\n",
    "        pred_labels = GetLabelsForOutlierDet(labels)\n",
    "        get_labels_DB = GetLabelsForIdxs(labels)\n",
    "        labels_Idxs = np.array(get_labels_DB)\n",
    "        OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs)\n",
    "        \n",
    "    SM['Name: Outlier Detection'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7AeAbMiGuHa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEQRTv_toPP_"
   },
   "outputs": [],
   "source": [
    "# Name: Print Time\n",
    "\n",
    "if 'Name: Print Time' not in SM:\n",
    "    Total_Time = SM['Total_Time']\n",
    "    Incoming_points = SM['Incoming_points']\n",
    "\n",
    "    Avg_Time_Taken = Total_Time/len(Incoming_points)\n",
    "    print(\"Average Time Taken is {} seconds \".format(Avg_Time_Taken))\n",
    "    #print(\"Average density =\", sum(density)/len(data1))\n",
    "    \n",
    "    SM['Avg_Time_Taken'] = Avg_Time_Taken\n",
    "    \n",
    "    SM['Name: Print Time'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"distances_ddcal.p\")\n",
    "os.remove(\"distances_ddcal_updated.p\")\n",
    "os.remove(\"indices_ddcal.p\")\n",
    "os.remove(\"indices_ddcal_updated.p\")\n",
    "# print(\"File Removed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHc6GO_GoPP_"
   },
   "outputs": [],
   "source": [
    "# list(SM.items())\n",
    "SM.clear()\n",
    "print(\"Current Status of Shelve Dictionaries: \", list(SM.items()))\n",
    "SM.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAgJ-LuPoPP_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ISIPROJECTWITHSTREAMINGLATESTCOLAB2(For large data).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
