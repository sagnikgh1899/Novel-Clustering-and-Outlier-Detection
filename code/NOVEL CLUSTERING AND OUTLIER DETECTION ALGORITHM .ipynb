{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARAMETERS USED:\n",
    "<b>Dataset and K varies</b> <br>\n",
    "\n",
    "##### These parameters are tuned to best possible extent\n",
    "1. thresh (opt = 0.3)\n",
    "2. alpha_weight_avg (opt = 0.6)\n",
    "3. per in LLE (opt = 0.5)\n",
    "4. percent in GetCutOffDistMod (opt = 0.7)\n",
    "5. val2 in Correcting_wrong_clustering (opt = 20)\n",
    "6. val3 in Correcting_wrong_clustering (opt = 15)\n",
    "7. percent1 in Correcting_wrong_clustering (opt = 0.8)\n",
    "8. percent2 in Correcting_wrong_clustering (opt = 0.16)\n",
    "9. percent3 in Correcting_wrong_clustering (opt = 0.10)\n",
    "10. percent4 in Correcting_wrong_clustering (opt = 0.01)\n",
    "11. algo for NN = kd_tree\n",
    "13. KVal in Outlier Postprocessing (opt = 3% of dataset size)\n",
    "14. percent5 in Outlier Postprocessing (opt = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import csv\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "from validclust import dunn\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn .metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import shelve\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already_present = False\n",
    "# if os.path.isfile('Static_Model.db.bak') or os.path.isfile('Static_Model.db.dat') or os.path.isfile('Static_Model.db.dir'):\n",
    "#     print(\"File Already Present. Taking Previously Stored Contents!!\")\n",
    "#     already_present = True\n",
    "\n",
    "########################## USER INPUTS ##########################\n",
    "data_path = r\"E:\\JUPYTER NOTEBOOK\\Corners(F).csv\"\n",
    "Ground_Truth_data_path=r\"E:\\ISI\\GROUND TRUTHS CLUSTERING\\Corners(F)_gt.csv\"\n",
    "Dataset_Name_Only = \"Corners\"\n",
    "K = 150            ## Actually K-1 neighbors are considered. Remember              #### Imp\n",
    "#25 #20 #10\n",
    "Outlier_Detection = False #True \n",
    "Clustering = True # False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if os.path.isfile('Static_Model.db.bak') or os.path.isfile('Static_Model.db.dat') or os.path.isfile('Static_Model.db.dir'):\n",
    "#     print(\"Current Status of Shelve Dictionaries: \", list(SM.items()))\n",
    "# else:\n",
    "name_storage_file = \"Static_Model_\" + Dataset_Name_Only + \".db\"\n",
    "SM = shelve.open(name_storage_file)\n",
    "print(\"Current Status of Shelve Dictionaries: \", list(SM.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Reading Data and Thresholds\n",
    "\n",
    "if 'Name: Reading Data and Thresholds' not in SM:\n",
    "    with open(data_path, 'r') as f:\n",
    "        data1 = np.genfromtxt(f, delimiter=',')   \n",
    "    Dimension = data1.shape[1]\n",
    "    #data1=data1/np.linalg.norm(data1)\n",
    "    print(len(data1))\n",
    "    print(\"Dimension: \",Dimension)\n",
    "    thresh = 0.3                                                                       #### Imp\n",
    "    alpha_weight_avg = 0.6\n",
    "    algo = 'ball_tree' #'kd_tree' \n",
    "\n",
    "    ############### SAVING ###############\n",
    "    SM['data_path'] = data_path\n",
    "    SM['Ground_Truth_data_path'] = Ground_Truth_data_path\n",
    "    SM['K'] = K\n",
    "    SM['Dimension'] = Dimension\n",
    "    SM['thresh'] = thresh\n",
    "    SM['alpha_weight_avg'] = alpha_weight_avg\n",
    "    SM['algo'] = algo\n",
    "    SM['Outlier_Detection'] = Outlier_Detection\n",
    "    SM['Clustering'] = Clustering\n",
    "    SM['data1'] = data1\n",
    "\n",
    "    SM['Name: Reading Data and Thresholds'] = True\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Calculate NN and Density\n",
    "\n",
    "if 'Name: Calculate NN and Density' not in SM:\n",
    "    # Calculate the nearest neighbors, the density of point_i and density of its K neighbors\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "    # print(indices_ddcal)\n",
    "\n",
    "    Hash_Map = {i:0 for i in range(len(data1))}\n",
    "    labels = {i:0 for i in range(len(data1))}\n",
    "    density = []\n",
    "    for i in range(len(data1)):\n",
    "        density.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "        Hash_Map[i] = density[i]\n",
    "\n",
    "\n",
    "    # SM['nbrs_ddcal'] = nbrs_ddcal\n",
    "    \n",
    "    with open('distances_ddcal.p', 'wb') as pfile1:\n",
    "        pickle.dump(distances_ddcal, pfile1, protocol=4)\n",
    "    # SM['distances_ddcal'] = distances_ddcal\n",
    "    with open('indices_ddcal.p', 'wb') as pfile2:\n",
    "        pickle.dump(indices_ddcal, pfile2, protocol=4)\n",
    "    # SM['indices_ddcal'] = indices_ddcal\n",
    "    SM['Hash_Map'] = Hash_Map\n",
    "    SM['labels'] = labels\n",
    "    \n",
    "    SM['Name: Calculate NN and Density'] = True\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Density Calculation\n",
    "\n",
    "if 'Name: Density Calculation' not in SM:\n",
    "    # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "    # 0: 0.45; (25,5); 0.2; [1,2,3,4,5]; 0\n",
    "\n",
    "    density.sort(key = itemgetter(0), reverse = False)\n",
    "    #print(density[:3])\n",
    "\n",
    "    SM['density'] = density\n",
    "    \n",
    "    SM['Name: Density Calculation'] = True\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the weighted moving average\n",
    "def numpy_ewma_vectorized_v2(values, alpha):\n",
    "    #values = np.array(values)\n",
    "    span = (2/alpha) - 1\n",
    "    df = pd.DataFrame(values)\n",
    "    return df.ewm(span=span).mean().iloc[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLE: Locally linear Embedding\n",
    "def LLE(data, K): #, data_point_Idx):\n",
    "    row=len(data)\n",
    "    #print(row)\n",
    "    dimension=len(data[0])\n",
    "    #print(dimension)\n",
    "    neighbors= K\n",
    "    # W=np.zeros((row, K)) # W=np.zeros((row, row))\n",
    "    count_pts = 0\n",
    "\n",
    "    hash_map = {value:np.array([]) for value in range(len(data))}\n",
    "\n",
    "    for i in range(row):\n",
    "        D_i=np.array(data-data[i, :])\n",
    "        #print(D_i)\n",
    "        distance=(D_i**2).sum(1)\n",
    "        #print(distance)\n",
    "        nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "        #print(nearest_neighbor)\n",
    "        D_nbrs=D_i[nearest_neighbor, :]\n",
    "        #print(D_nbrs)\n",
    "        ##Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "        Q=np.matmul(D_nbrs, D_nbrs.T)\n",
    "        #print(Q)\n",
    "#         t=np.trace(Q)\n",
    "#         r=0.001*t\n",
    "        r = 0.001 * float(Q.trace())\n",
    "        if(neighbors>=dimension):\n",
    "#             sig2 = (np.linalg.svd(D_i,compute_uv=0))**2\n",
    "#             r = np.sum(sig2[dimension:])\n",
    "            Q=Q+(r*np.identity(neighbors))\n",
    "#             Q.flat[::neighbors+1] += r\n",
    "        ##w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "        w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "        \n",
    "        w = w/sum(w)\n",
    "        ##w=w/sum(w)\n",
    "        #print(i, w)\n",
    "        # print(\"P1\", W[i, nearest_neighbor])\n",
    "        # print(\"P2\", w)\n",
    "        # W[i, nearest_neighbor]=w\n",
    "\n",
    "        # My code\n",
    "        temp = []\n",
    "        for ele in w:\n",
    "            #print(ele)\n",
    "            #if ele != 0:\n",
    "            temp.append(ele)\n",
    "            temp.sort(reverse = True)\n",
    "            hash_map[i] = np.array(temp) \n",
    "            \n",
    "#     I=np.identity(row)\n",
    "#     M=I-W\n",
    "        if (count_pts+1) % 100 == 0:\n",
    "            print(\"****************Computed LLE for {0} points****************\".format(count_pts+1))\n",
    "        count_pts += 1\n",
    "\n",
    "    per = 0.5 #0.6 #0.3 #0.5 #0.2 #0.05 #0.1\n",
    "    store_K = defaultdict()\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        count = 0\n",
    "        for i in range(1, len(hash_map[j])):\n",
    "            #print(((1/neighbors) - (dec_per*(1/neighbors))), ((1/neighbors) + (dec_per*(1/neighbors))))\n",
    "            if round((abs(hash_map[j][i-1] - hash_map[j][i])/abs(hash_map[j][i-1]))*100,1) <= per:\n",
    "                count += 1\n",
    "        store_K[j] = count\n",
    "        \n",
    "    # print(store_K)\n",
    "    for key in store_K:\n",
    "        store_K[key] += 1\n",
    "    return store_K\n",
    "\n",
    "    # return store_K[data_point_Idx] + 1  # +1 because all neighbouring algorithms consider the datapoint itself to be a part\n",
    "                                          # of its neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: LLE_lookup Calculation\n",
    "\n",
    "if 'Name: LLE_lookup Calculation' not in SM:\n",
    "    \n",
    "    data1 = SM['data1']\n",
    "    K = SM['K']\n",
    "    start4 = time.time()\n",
    "    # Optimization: Instead of calling the LLE for each point, we make it constant time\n",
    "    LLE_lookup = {}\n",
    "    LLE_lookup = LLE(data1, K)\n",
    "\n",
    "    SM['LLE_lookup'] = LLE_lookup\n",
    "\n",
    "    print(\"time taken = \",time.time()-start4,\" sec\")\n",
    "    \n",
    "    SM['Name: LLE_lookup Calculation'] = True\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# DFS Based Approach:\n",
    "\n",
    "# However due to the maximum recursion depth constraint in Python Programming language\n",
    "# this is not a very good approach. The BFS one above is a better solution.\n",
    "# Typically it will fail for large datasets. \n",
    "# \"\"\"\n",
    "\n",
    "# def NovelClus(Parent, Hash_Map, idx, dataset, K, list_of_parents, labels, thresh, cluster_num, can_form_cluster):\n",
    "#     #print(Parent[1], idx)\n",
    "    \n",
    "#     print(\"Cluster =\", cluster_num)\n",
    "\n",
    "#     while idx <= K-2:\n",
    "\n",
    "#         Density_Parent = Parent[0]\n",
    "#         Radius = Parent[2]\n",
    "#         Child_Idx = Parent[3][idx] \n",
    "#         Child = Hash_Map[Child_Idx]\n",
    "#         Child_Datapoint = Child[1]\n",
    "\n",
    "#         # if child is already labelled then return\n",
    "#         if labels[Child_Idx] > 0:\n",
    "#             idx += 1\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "#         neigh = NearestNeighbors(radius = Radius)\n",
    "#         neigh.fit(dataset)\n",
    "#         rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "#         #print(rng[0]) \n",
    "\n",
    "#         Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "\n",
    "#         # if dis-density threshold criteria is satisfied\n",
    "#         if Density_Parent not in list_of_parents:\n",
    "#             list_of_parents.append(Density_Parent)\n",
    "#         data_parent = np.array(list_of_parents, dtype=np.float64)\n",
    "#         weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)\n",
    "#         print(\"Density of Parent =\", weighted_density)\n",
    "#         print(\"Density of Child = \", Density_Child)\n",
    "#         numerator = abs(weighted_density - Density_Child)\n",
    "#         denominator = weighted_density\n",
    "#         print(\"Weighted Density =\", numerator/denominator)\n",
    "        \n",
    "#         if numerator/denominator <= thresh:\n",
    "#             labels[Child_Idx] = cluster_num\n",
    "#             can_form_cluster[0] = [True]\n",
    "#             NovelClus(Child, Hash_Map, 0, dataset, K, list_of_parents, labels, thresh, cluster_num, can_form_cluster)\n",
    "\n",
    "#         # else term Child as \"Anomaly\" and return\n",
    "#         else:\n",
    "#             #print(\"\\n\")\n",
    "#             labels[Child_Idx] = -1\n",
    "#             idx += 1\n",
    "#             continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BFS based Approach: \n",
    "\n",
    "It takes care of the maximum recursion depth constraint in Python programming Language.\n",
    "This however may take more time than DFS\n",
    "\"\"\"\n",
    "\n",
    "def NovelClus_FF(Parent, Hash_Map, dataset, K, list_of_parents_queue, labels, thresh, cluster_num, can_form_cluster, \n",
    "                 alpha_weight_avg, Tree_Structure, Roots, Store_Radius_Density, Store_Parents):\n",
    "    ##print(\"Cluster =\", cluster_num)\n",
    "    \n",
    "    # Adding only the root\n",
    "    Roots[Parent[4]] = Parent[1]\n",
    "    Store_Radius_Density[Parent[4]] = [Parent[2], Parent[0]]\n",
    "\n",
    "    # We make use of a Deque for both side operation\n",
    "    Queue = deque()\n",
    "\n",
    "    # Append from left the Parent Node\n",
    "    Queue.appendleft(Parent)\n",
    "\n",
    "\n",
    "    # While Queue is not Null:\n",
    "        # Queue extracts the first Node\n",
    "        # We check if k children of Node satisfies the density criterion\n",
    "        # if a child satisfies, it is pushed into the Queue from the end; else it is termed -1 and left\n",
    "        \n",
    "    \n",
    "    while Queue:\n",
    "        # Pop from the right end\n",
    "        Parent = Queue.pop()\n",
    "        \n",
    "        # Getting important information from Parent\n",
    "        Density_Parent = Parent[0]\n",
    "        Radius = Parent[2]\n",
    "        #Child_Idx = Parent[3][idx] \n",
    "        Child_Idx_array = Parent[3]   # [94,  32, 117, .... 100, 118]\n",
    "        #Child = Hash_Map[Child_Idx]\n",
    "        #Child_Datapoint = Child[1]\n",
    "        \n",
    "        # Update the list_of_parents for the weighted moving average\n",
    "        list_of_parents = list_of_parents_queue.pop() + [Density_Parent] \n",
    "        Store_Parents[Parent[4]] = list_of_parents\n",
    "        #print(\"LoP =\", list_of_parents)\n",
    "        \n",
    "        \n",
    "        Child_array = []\n",
    "        Child_Datapoint_array = []\n",
    "        for Child_Idx in Child_Idx_array:\n",
    "            Child_array.append(Hash_Map[Child_Idx])\n",
    "            Child_Datapoint_array.append(Hash_Map[Child_Idx][1])  \n",
    "        \n",
    "        \n",
    "        # For each Parent find which child satiesfies the density threshold criterion\n",
    "        ##print(\"Number of Children Nodes =\", len(Child_Idx_array), \"\\n\")\n",
    "        for i in range(len(Child_Idx_array)):\n",
    "            \n",
    "            Child = Child_array[i]\n",
    "            Child_Idx = Child[-1]             ##Child_Idx_array[i]\n",
    "            Child_Datapoint = Child[1]        ##Child_Datapoint_array[i]\n",
    "            \n",
    "            # Base case: if child is already labelled in cluster then ignore\n",
    "            if labels[Child_Idx] > 0:\n",
    "                #print(\"Child is already labelled!!\")\n",
    "                continue\n",
    "                \n",
    "            # print(\"Radius: \", Radius)\n",
    "            neigh = NearestNeighbors(radius = Radius)\n",
    "            neigh.fit(dataset)\n",
    "            rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "            #print(rng[0])\n",
    "            \n",
    "            if len(rng[0][0]) == 0:\n",
    "                Density_Child = 0\n",
    "            else:\n",
    "                Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "            \n",
    "            #print(\"No. of Parent =\",len(list_of_parents))\n",
    "            \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "            #print(\"Density of Parent =\", weighted_density)\n",
    "            #print(\"Density of Child = \", Density_Child)\n",
    "            numerator = abs(weighted_density - Density_Child)\n",
    "            denominator = weighted_density\n",
    "            #print(\"Weighted Density =\", numerator/denominator)\n",
    "            \n",
    "            \n",
    "            if numerator/denominator <= thresh:\n",
    "                labels[Child_Idx] = cluster_num\n",
    "                \n",
    "                list_of_parents_queue.appendleft(list_of_parents)\n",
    "                       \n",
    "                # Update the information for this child\n",
    "                Child[0] = Density_Child\n",
    "                if len(rng[0][0]) == 0:\n",
    "                    Child[2] = 0\n",
    "                else:\n",
    "                    Child[2] = max(rng[0][0])\n",
    "                Child[3] = rng[1][0]\n",
    "                \n",
    "                \n",
    "                # Store Adaptive Radius and New Density of the Parent Node for latter usage\n",
    "                list_of_parents_1 = list_of_parents + [Density_Child]\n",
    "                data_parent_1 = np.array(list_of_parents_1) #, dtype=np.float64)\n",
    "                weighted_density_1 = numpy_ewma_vectorized_v2(data_parent_1, alpha_weight_avg)  #O()??\n",
    "                Store_Radius_Density[Child[4]] = [Child[2], weighted_density_1]\n",
    "                \n",
    "                \n",
    "                # Add Child to the Tree Structure\n",
    "                if Parent[4] not in Tree_Structure:\n",
    "                    Tree_Structure[Parent[4]] = [Child[4]]\n",
    "                else:\n",
    "                    Tree_Structure[Parent[4]].append(Child[4])\n",
    "                \n",
    "                Queue.appendleft(Child)\n",
    "                \n",
    "                # Update the hashmap OLD\n",
    "                ##Hash_Map[Child[-1]] = Child\n",
    "                \n",
    "                # Update the hashmap NEW\n",
    "                temp_dict = {Child[-1]:Child}\n",
    "                Hash_Map.update(temp_dict)\n",
    "                ##################################\n",
    "                \n",
    "            \n",
    "                # The Parent can form a cluster of its own\n",
    "                can_form_cluster[0] = [True]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #print(\"Possible Anomaly!!\")\n",
    "                labels[Child_Idx] = -1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nbrs(data1, idx, K, algo): # 70\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "    density_mod = []\n",
    "    density_mod.append([sum(distances_ddcal[idx])/(K-1), data1[idx], max(distances_ddcal[idx]), indices_ddcal[idx][1:], idx])\n",
    "    \n",
    "    # density_mod.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    # density_cpy.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    \n",
    "    return density_mod[0] #, density_mod[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_idx = 0\n",
    "# cluster_num = 0\n",
    "# cluster_centers = []\n",
    "# hold_cluster_val = cluster_num ####\n",
    "\n",
    "# # For Storing Tree Structure\n",
    "# Tree_Structure = dict()\n",
    "# # Only Roots\n",
    "# Roots = dict()\n",
    "# # For Storing Radius, Density of each point\n",
    "# Store_Radius_Density = dict()\n",
    "# # For Storing parents\n",
    "# Store_Parents = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Clustering Function\n",
    "\n",
    "if 'Name: Clustering Function' not in SM:\n",
    "\n",
    "    if 'data_idx' not in SM:\n",
    "        SM['data_idx'] = 0\n",
    "\n",
    "    if 'cluster_num' not in SM:\n",
    "        SM['cluster_num'] = 0\n",
    "\n",
    "    if 'cluster_centers' not in SM:\n",
    "        SM['cluster_centers'] = []\n",
    "\n",
    "    if 'hold_cluster_val' not in SM:\n",
    "        SM['hold_cluster_val'] = SM['cluster_num']\n",
    "\n",
    "    if 'Tree_Structure' not in SM:\n",
    "        SM['Tree_Structure'] = dict()\n",
    "\n",
    "    if 'Roots' not in SM:\n",
    "        SM['Roots'] = dict()\n",
    "\n",
    "    if 'Store_Radius_Density' not in SM:\n",
    "        SM['Store_Radius_Density'] = dict()\n",
    "\n",
    "    if 'Store_Parents' not in SM:\n",
    "        SM['Store_Parents'] = dict()\n",
    "\n",
    "\n",
    "    start_at_data_idx = SM['data_idx']\n",
    "    data1 = SM['data1']\n",
    "    for data_idx in range(start_at_data_idx, len(data1)):  #O(n)\n",
    "\n",
    "        if (data_idx + 1) % 100 == 0:\n",
    "            print(\"**************************Computed {} Datapoints**************************\".format(data_idx+1))\n",
    "        idx = 0\n",
    "        list_of_parents_queue = deque()\n",
    "        list_of_parents_queue.appendleft([])\n",
    "\n",
    "        labels = SM['labels']\n",
    "        cluster_num = SM['cluster_num']\n",
    "        cluster_centers = SM['cluster_centers']\n",
    "        hold_cluster_val = SM['hold_cluster_val']\n",
    "        density = SM['density']\n",
    "\n",
    "        if labels[density[data_idx][-1]] == 0: #<= 0:\n",
    "            cluster_num += 1\n",
    "            labels[density[data_idx][-1]] = cluster_num\n",
    "            can_form_cluster = [False]\n",
    "\n",
    "\n",
    "            # Update information for the Parent node\n",
    "            #   0                         1         2        3                    4\n",
    "            # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "\n",
    "\n",
    "            # print(density[data_idx][-1])\n",
    "            # break\n",
    "\n",
    "            ## Modification ##############\n",
    "            # print(density[data_idx])\n",
    "            # print(\"\\n\\n\")\n",
    "\n",
    "            LLE_lookup = SM['LLE_lookup']\n",
    "\n",
    "            ##K_mod = LLE(data1, K, density[data_idx][-1])   #O(n)\n",
    "            K_mod = LLE_lookup[density[data_idx][-1]]      #O(1) Optimized One\n",
    "            #print(\"K modified: \",K_mod) # Can be uncommented for better intuition\n",
    "\n",
    "            # Old One\n",
    "            ##density[data_idx], Hash_Map[density[data_idx][-1]] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "            # print(density[data_idx])\n",
    "            # break\n",
    "            #########################################\n",
    "\n",
    "            SM['list_of_parents_queue'] = list_of_parents_queue\n",
    "            SM['density'] = density\n",
    "            Hash_Map = SM['Hash_Map']\n",
    "            data1 = SM['data1']\n",
    "            thresh = SM['thresh']\n",
    "            alpha_weight_avg = SM['alpha_weight_avg']\n",
    "            Store_Radius_Density = SM['Store_Radius_Density']\n",
    "            Store_Parents = SM['Store_Parents']\n",
    "            Roots = SM['Roots']\n",
    "            Tree_Structure = SM['Tree_Structure']\n",
    "            algo = SM['algo']\n",
    "            \n",
    "            # New One\n",
    "            density[data_idx] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "            temp_dict = {density[data_idx][-1]:density[data_idx]}\n",
    "            #########################################\n",
    "\n",
    "            # print(\"\\nNEW ROOT!!\")\n",
    "            NovelClus_FF(density[data_idx], Hash_Map, data1, K_mod, list_of_parents_queue, labels, thresh, cluster_num, \n",
    "                         can_form_cluster, alpha_weight_avg, Tree_Structure, Roots, Store_Radius_Density, Store_Parents)\n",
    "            # print(Tree_Structure)\n",
    "\n",
    "\n",
    "            # Visualization: For higher dimensions this time complexity can be ignored\n",
    "            \"\"\"\n",
    "            if Dimension <= 2:\n",
    "                plt.figure(figsize=(8,8))\n",
    "                for i in range(len(data1)):\n",
    "                    if labels[i] == cluster_num:\n",
    "                        plt.scatter(data1[i][0],data1[i][1],c='pink')\n",
    "                    elif labels[i] == -1:\n",
    "                        plt.scatter(data1[i][0],data1[i][1], c='yellow')\n",
    "                    else:\n",
    "                        plt.scatter(data1[i][0], data1[i][1],c='blue')\n",
    "                plt.scatter(density[data_idx][1][0], density[data_idx][1][1], c='red',s=200, marker = '+')\n",
    "                plt.show()\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            # If the last Parent Node cannot form a clusterof its own then it is a part of a cluster with only 1 datapoint\n",
    "            # which means \" a possible Anomaly\"\n",
    "            # then change its label to -1 and reduce the cluster num by 1\n",
    "            if can_form_cluster[0] == False:\n",
    "                labels[density[data_idx][-1]] = -1\n",
    "                cluster_num = hold_cluster_val  ####\n",
    "            else:\n",
    "                cluster_centers.append([density[data_idx][1].tolist(), cluster_num])\n",
    "                hold_cluster_val += 1  ####\n",
    "                \n",
    "            SM['Tree_Structure'] = Tree_Structure\n",
    "            SM['Store_Radius_Density'] = Store_Radius_Density\n",
    "            SM['Store_Parents'] = Store_Parents\n",
    "            SM['Hash_Map'] = Hash_Map\n",
    "\n",
    "        SM['labels'] = labels\n",
    "        SM['cluster_num'] = cluster_num\n",
    "        SM['cluster_centers'] = cluster_centers\n",
    "        SM['hold_cluster_val'] = hold_cluster_val\n",
    "        SM['data_idx'] = data_idx\n",
    "        SM.sync()\n",
    "\n",
    "    SM['Name: Clustering Function'] = True\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following print statements can be uncommented for better intuition\n",
    "# print(cluster_centers)\n",
    "# print(\"\\n\", len(cluster_centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function corrects the wrong clustering\n",
    "def Correcting_wrong_clustering(labels, cluster_centers, data1):  \n",
    "    cluster_labels = []\n",
    "    for key in labels:\n",
    "        cluster_labels.append(labels[key])\n",
    "    \n",
    "    Hash_Map_2 = dict()\n",
    "    X_label = dict()\n",
    "    \n",
    "    for i in range(len(data1)): \n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        if labels[i] in Hash_Map_2:\n",
    "            Hash_Map_2[labels[i]] += 1\n",
    "            X_label[labels[i]] = X_label[labels[i]] + [data1[i].tolist()]\n",
    "        else:\n",
    "            Hash_Map_2[labels[i]] = 1\n",
    "            X_label[labels[i]] = [data1[i].tolist()]\n",
    "            \n",
    "    #print(\"\\nHASH MAP\\n\", Hash_Map_2)\n",
    "\n",
    "    for key in Hash_Map_2:\n",
    "        temp = Hash_Map_2[key]\n",
    "        for idx in range(len(cluster_centers)):\n",
    "            if cluster_centers[idx][1] == key:\n",
    "                break\n",
    "        Hash_Map_2[key] = [temp, cluster_centers[idx][0]]\n",
    "        \n",
    "        \n",
    "    # Function to find Intra Cluster Distance    \n",
    "    def IntraClusterMetric(X):\n",
    "        Intra_Clus_Dist = 0\n",
    "        for i in range(len(X)):\n",
    "            Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "        return Intra_Clus_Dist/(len(X)-1)\n",
    "           \n",
    "    \n",
    "    for key in X_label:\n",
    "        X = X_label[key]\n",
    "        Intra_metric = IntraClusterMetric(X)\n",
    "        Hash_Map_2[key] = Hash_Map_2[key] + [Intra_metric]  \n",
    "    \n",
    "    #print(\"Hash_Map_2\\n\",Hash_Map_2,\"\\n\")\n",
    "    \n",
    "    clusnum_card = sorted(Hash_Map_2.items(), key=lambda kv:(kv[1], kv[0]), reverse = False)\n",
    "    cluster_center_new = []\n",
    "    for ele in clusnum_card:\n",
    "        cluster_center_new.append(np.array(ele[1][1]))   \n",
    "\n",
    "    ## Following print statements can be uncommented for better intuition\n",
    "    # print(Hash_Map_2)\n",
    "    # print(\"\\nAfter Sorting \")\n",
    "    # print(clusnum_card)\n",
    "    # print(\"\\nOnly cluster centers\")\n",
    "    # print(cluster_center_new)\n",
    "    \n",
    "    \n",
    "    # print(len(cluster_centers))\n",
    "    # print(len(cluster_center_new))\n",
    "    if len(cluster_centers) > len(cluster_center_new):\n",
    "        cluster_centers = cluster_center_new\n",
    "    K1 = len(cluster_centers)\n",
    "    nbrs = NearestNeighbors(n_neighbors=K1, algorithm='auto').fit(cluster_center_new)\n",
    "    distances, indices = nbrs.kneighbors(cluster_center_new)\n",
    "    #print(indices,\"\\n\")\n",
    "    #print(distances)\n",
    "\n",
    "    \n",
    "    def GetCutoffDistMod(cluster_centers):\n",
    "        percent = 0.7\n",
    "        i = 0\n",
    "        j = 1\n",
    "        Count = 0\n",
    "        Sum = 0\n",
    "        while i < len(cluster_centers)-1:\n",
    "            while j < len(cluster_centers):\n",
    "                Sum += math.dist(cluster_centers[i], cluster_centers[j])\n",
    "                j += 1\n",
    "                Count += 1\n",
    "            i += 1\n",
    "            j = i + 1\n",
    "        Count = max(Count, 1)\n",
    "        return math.ceil(Sum/Count)*percent\n",
    "    \n",
    "    \n",
    "    #print(clusnum_card)\n",
    "    \n",
    "    #print(\"\\n\\n\")\n",
    "    dist_val = GetCutoffDistMod(cluster_center_new)\n",
    "    val2 = 20\n",
    "    val3 = 15\n",
    "    percent1 = 0.8\n",
    "    percent2 = 0.16\n",
    "    percent3 = 0.10\n",
    "    percent4 = 0.02\n",
    "\n",
    "    # Format of clus_to_change: [curr_clus, clus_to_be]\n",
    "    clus_to_change = []\n",
    "    for i in range(len(clusnum_card)-1):\n",
    "        # We are not considering till the last term since,\n",
    "        # last term is the most dense and biggest among all \n",
    "        # the other clusters. It cannot be merged with others\n",
    "        \n",
    "        j = 1\n",
    "        cardinality_of_self = Hash_Map_2[clusnum_card[i][0]][0]\n",
    "        Intra_Clus_Self = Hash_Map_2[clusnum_card[i][0]][2]\n",
    "        cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "        \n",
    "        #print(cardinality_of_self, cardinality_of_neighbor)\n",
    "        while cardinality_of_self >= cardinality_of_neighbor and j < len(clusnum_card)-1:\n",
    "            #print(\"YES\")\n",
    "            j += 1\n",
    "            #print(\"j=\",j)\n",
    "            cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]  #-1\n",
    "            Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "\n",
    "        # cardinality_of_self = Hash_Map_2[clusnum_card[i][0]]\n",
    "        # cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        # print(cardinality_of_neighbor)\n",
    "    \n",
    "        ## Following print statements can be uncommented for better intuition\n",
    "        # print(\"Cluster number\", clusnum_card[i][0], \"Cardinality \", Hash_Map_2[clusnum_card[i][0]][0])\n",
    "        # print(\"Neighbor Cluster number\", clusnum_card[indices[i][j]][0], \"Cardinality \", Hash_Map_2[clusnum_card[indices[i][j]][0]][0])\n",
    "        # print(\"Intra_Clus_Self\", Intra_Clus_Self, \"Intra_Clus_Neigh \", Intra_Clus_Neigh)\n",
    "        # print(\"Intra Val Lower =\", (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh), \"Intra Val Higher =\", (Intra_Clus_Neigh + percent1*Intra_Clus_Neigh))\n",
    "        # print(\"Percent1*CNeigh\", int(percent1*cardinality_of_neighbor))\n",
    "        # print(\"Percent2 =\", (cardinality_of_self/cardinality_of_neighbor))\n",
    "        # print(\"val3\", val3)\n",
    "        # print(\"Or condition\", abs(cardinality_of_self - cardinality_of_neighbor), \"val2\", val2)\n",
    "        # print(\"Distances between self and neighbor\", int(round(distances[i][j],0)), \"Dist_val\", dist_val)\n",
    "        \n",
    "        \n",
    "        # Using a boolean merge to keep a track if the self cluster is merged or not\n",
    "        # Unless and until the cluster is merged, it is processed through every if condition\n",
    "        merged = False\n",
    "        \n",
    "        # If cardinality of self and cardinality of neighbor are both less than a given threshold\n",
    "        # where the threshold is very very small; say 10\n",
    "        if merged == False:\n",
    "            if cardinality_of_self <= val2 and cardinality_of_neighbor <= val2:\n",
    "                # Distance checking here isn't necessary\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                #print(\"Merged 1st condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions\n",
    "        \n",
    "        # If cardinality of self is less than 16% of cardinality of neighbor \n",
    "        if merged == False:\n",
    "            if (cardinality_of_self/cardinality_of_neighbor) <= percent2 and cardinality_of_self <= val3:\n",
    "                if int(round(distances[i][j],0)) <= dist_val:\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 2nd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions\n",
    "                \n",
    "        # If Cardinality of self > val3 but is less than percent6 of the cardinality of neighbor\n",
    "        if merged == False:\n",
    "            if val3 < cardinality_of_self <= val3*2.7 and cardinality_of_self/cardinality_of_neighbor <= percent3:\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 3rd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions          \n",
    "        \n",
    "        # If Cardinality of self is less than percent4 of the cardinality of neighbor; where percent4 ~ 2 percent\n",
    "        if merged == False:\n",
    "            if cardinality_of_self/cardinality_of_neighbor <= percent4:\n",
    "                # Here we don't need to check the distance and intra cluster density;\n",
    "                # The self cluster is very very small compared to the neighbor cluster.\n",
    "                # So just merge them\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                # print(\"Merged 4th condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "        \n",
    "        if merged == False: # Some merges by this logic as well\n",
    "            if (cardinality_of_self <= cardinality_of_neighbor and cardinality_of_self <= val3) or (abs(cardinality_of_self - cardinality_of_neighbor) <= val2 and cardinality_of_self <= val3):\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    # print(\"Merged 5th condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "\n",
    "        # print(\"\\n\")\n",
    "    print(\"\\nChanged Clusters are: [initial cluster --> New Cluster]\")\n",
    "    print(clus_to_change)\n",
    "    \n",
    "    \n",
    "    for key in labels:\n",
    "        for item in clus_to_change:\n",
    "            if labels[key] == item[0]:\n",
    "                labels[key] = item[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Correcting Wrong Clustering\n",
    "\n",
    "if 'Name: Correcting Wrong Clustering' not in SM:\n",
    "    labels = SM['labels']\n",
    "    cluster_centers = SM['cluster_centers']\n",
    "    data1 = SM['data1']\n",
    "\n",
    "    # Correct the wrong clusters\n",
    "    Correcting_wrong_clustering(labels, cluster_centers, data1)\n",
    "\n",
    "    SM['labels'] = labels\n",
    "    SM['cluster_centers'] = cluster_centers\n",
    "    \n",
    "    SM['Name: Correcting Wrong Clustering'] = True\n",
    "\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Correcting Labels2\n",
    "\n",
    "if 'Name: Correcting Labels2' not in SM:\n",
    "\n",
    "    # This returns cluster numbers in a non-missing sorted order\n",
    "    # Before this function many clusters may be removed in the previous step\n",
    "    # So, to keep everything ordered, this function makes sure all clusters have a number\n",
    "    # and the number starts from 1 all the way to n without any missing number in between\n",
    "    def get_count_in_label(labels):\n",
    "        table = {value:0 for value in labels}\n",
    "        for ele in labels:\n",
    "            table[ele] += 1\n",
    "        key_list = table.keys()\n",
    "        #sorted(key_list, reverse = False)\n",
    "        for ele in sorted(key_list, reverse = False):\n",
    "            print(ele,\":\",table[ele])\n",
    "\n",
    "    def get_label_list(hash_map):\n",
    "        label_list = []\n",
    "        for key in hash_map.keys():\n",
    "            if hash_map[key] > 0:\n",
    "                label_list.append(hash_map[key])\n",
    "            #else:\n",
    "            #    get_labels.append(hash_map[key])\n",
    "        #print(get_labels)\n",
    "        return label_list    \n",
    "\n",
    "\n",
    "    def find_missing_idx(hash_map):\n",
    "        max_idx = float(\"-inf\")\n",
    "        for key in hash_map:\n",
    "            if hash_map[key] > max_idx:\n",
    "                max_idx = hash_map[key]\n",
    "        #max_idx = max(hash_map.keys())\n",
    "\n",
    "        min_idx = float(\"inf\")\n",
    "        for key in hash_map:\n",
    "            if hash_map[key] < min_idx and hash_map[key] != -1:\n",
    "                min_idx = hash_map[key]\n",
    "        #min_idx = 0\n",
    "\n",
    "        ########### print(min_idx, max_idx)                     \n",
    "\n",
    "        label_list = get_label_list(hash_map)\n",
    "\n",
    "        missing_idx = []\n",
    "        count = 1\n",
    "        while count <= max_idx:\n",
    "            if count not in label_list:\n",
    "                missing_idx.append(count)\n",
    "            count += 1\n",
    "\n",
    "        ########## print(missing_idx)  \n",
    "        return missing_idx\n",
    "\n",
    "    def correct_the_idx(hash_map): #labels):\n",
    "        missing_idx = find_missing_idx(hash_map) #labels)\n",
    "\n",
    "        # Can be uncommented for better intuition\n",
    "        # print(missing_idx)\n",
    "\n",
    "        while missing_idx:\n",
    "            for i in missing_idx:\n",
    "                for key in hash_map:# in range(len(labels)):\n",
    "                    if hash_map[key] == i + 1: #labels[j] == i+1:\n",
    "                        hash_map[key] = hash_map[key] - 1 #labels[j] = labels[j] - 1\n",
    "\n",
    "            #get_count_in_label(labels)\n",
    "\n",
    "            #print(\"\\n\")\n",
    "            #coorect_the_idx(labels)\n",
    "\n",
    "            missing_idx = find_missing_idx(hash_map) #labels)\n",
    "\n",
    "\n",
    "    #missing_idx = correct_the_label_idx(labels)\n",
    "    #print(missing_idx)\n",
    "\n",
    "    #get_count_in_label(labels)\n",
    "    # print(\"\\n\\n\")\n",
    "    labels = SM['labels']\n",
    "    correct_the_idx(labels)\n",
    "    #get_count_in_label(labels)\n",
    "    #find_missing_idx(hash_map)\n",
    "    #print(hash_map)\n",
    "\n",
    "    #label_l = get_label_list(hash_map)\n",
    "    #get_count_in_label(label_l)\n",
    "\n",
    "    SM['labels'] = labels\n",
    "    SM['Name: Correcting Labels2'] = True\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Before Post Processing Display\n",
    "\n",
    "if 'Name: Before Post Processing Display' not in SM:\n",
    "    \n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "\n",
    "    label_list = []\n",
    "    anomaly_list = []\n",
    "    max_val = float(\"-inf\")\n",
    "    for key in labels:\n",
    "        if max_val < labels[key]:\n",
    "            max_val = labels[key]\n",
    "        if labels[key] == -1:\n",
    "            anomaly_list.append(data1[key])\n",
    "        else:\n",
    "            label_list.append(labels[key])\n",
    "    cluster_num = max_val\n",
    "    #print(label_list)\n",
    "    print(\"\\n------------ Before Outlier Post Processing ------------\")\n",
    "    print(\"Total number of datapoints =\", len(data1))\n",
    "    print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "    print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "    print(\"Number of Cluster(s) =\", cluster_num)\n",
    "\n",
    "    SM['data1'] = data1\n",
    "    SM['label_list'] = label_list\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    \n",
    "    SM['Name: Before Post Processing Display'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Name: Display1\n",
    "\n",
    "if 'Name: Display1' not in SM:\n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization Before Outlier Post Processing')\n",
    "        plt.show()\n",
    "        \n",
    "    SM['Name: Display1'] = True\n",
    "    SM.sync()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Outlier Post Processing\n",
    "\n",
    "if 'Name: Outlier Post Processing' not in SM:\n",
    "\n",
    "    def OutlierPostProcessing(LLE_lookup, algo, data1, labels, percent5, alpha_weight_avg, Store_Radius_Density, Tree_Structure):\n",
    "        # Select a predetermined value of K\n",
    "        # Find the K nearest Neighbors of an outlier\n",
    "        # For each of those neighbors:\n",
    "        #     Find their Intracluster distance with the same K value\n",
    "        #     If the Intracluster distance of outlier is within +-5% of the intracuster distance of the neighbor,\n",
    "        #     and the neighbor point is not an outlier, \n",
    "        #     then consider the outlier to be a part of the smae cluster that the neighbor point belongs.\n",
    "\n",
    "        ##nbrs = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "        ##distances, indices = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] == -1:\n",
    "                # Predicted Outlier, so find its K nearest neighbors\n",
    "                #############################################################\n",
    "                Opt_K = LLE_lookup[i]\n",
    "                nbrs = NearestNeighbors(n_neighbors=Opt_K, algorithm=algo).fit(data1)\n",
    "                _, indices = nbrs.kneighbors(data1)\n",
    "                #############################################################\n",
    "\n",
    "                PredOutlierNNidx = indices[i]\n",
    "                # print(\"IDXs: \", PredOutlierNNidx)\n",
    "                PredOutlierNN = []\n",
    "                for idx in PredOutlierNNidx:\n",
    "                    PredOutlierNN.append(data1[idx])\n",
    "                # Find the intracluster distance for the Predicted Outlier\n",
    "                PredOutlierICD = IntraClusterDistance(PredOutlierNN)\n",
    "                # print(\"Outlier\", i, PredOutlierICD)\n",
    "\n",
    "\n",
    "                # For each of the outlier's K neighbors\n",
    "                for idx in PredOutlierNNidx[1:]:\n",
    "\n",
    "                    # Check if the neighbor is an outlier or inlier\n",
    "                    if labels[idx] != -1:\n",
    "                        # print(\"Not an Outlier\")\n",
    "                        NNeighboridx = indices[idx]\n",
    "                        NNeighbor = []\n",
    "                        for idx1 in NNeighboridx:\n",
    "                            NNeighbor.append(data1[idx1])\n",
    "\n",
    "                        # Find each neighbor's IntraClusterDistance(ICD)\n",
    "                        NNICD = IntraClusterDistance(NNeighbor)\n",
    "                        #print(\"NNICD: \", NNICD)\n",
    "\n",
    "                        # Check if outlier's ICD is about +-5% of neighbor's ICD\n",
    "                        # print(\"Lower: \",(NNICD-percent5*NNICD), \"Upper: \",(NNICD+percent5*NNICD), \"Self: \",PredOutlierICD)\n",
    "                        if (NNICD-percent5*NNICD) <= PredOutlierICD <= (NNICD+percent5*NNICD):\n",
    "                            # Change the label of the Predicted Outlier 'i' to the \n",
    "                            # label of the 1st Nearest Neighbor which satifies the condition\n",
    "                            labels[i] = labels[idx]\n",
    "                            # print(\"Changed\\n\")\n",
    "                            if idx not in Tree_Structure:\n",
    "                                Tree_Structure[idx] = [i]\n",
    "                            else:\n",
    "                                Tree_Structure[idx].append(i)\n",
    "\n",
    "                            # Update its Store_Radius_Density Value\n",
    "                            # print(\"IDX:\",idx)\n",
    "                            Radius = Store_Radius_Density[idx][0]\n",
    "                            neigh = NearestNeighbors(radius = Radius)\n",
    "                            neigh.fit(data1)\n",
    "                            rng = neigh.radius_neighbors([data1[i]])\n",
    "                            # Check if there is noneighbour within the radius\n",
    "                            # If yes then the density of the incoming datapoint is 0\n",
    "                            if len(rng[0][0]) == 0:\n",
    "                                Density_Parent = 0\n",
    "                            else:\n",
    "                                Density_Parent = sum(rng[0][0])/len(rng[0][0])\n",
    "\n",
    "                            list_of_parents = Store_Parents[idx] + [Density_Parent] \n",
    "                            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "                            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)  #O()??\n",
    "                            if len(rng[0][0]) == 0:\n",
    "                                Radius_new = 0\n",
    "                            else:\n",
    "                                Radius_new = max(rng[0][0])\n",
    "                            Store_Radius_Density[i] = [Radius_new, weighted_density]\n",
    "                            Store_Parents[i] = list_of_parents\n",
    "\n",
    "                            break\n",
    "\n",
    "\n",
    "    def IntraClusterDistance(X):\n",
    "            Intra_Clus_Dist = 0\n",
    "            for i in range(len(X)):\n",
    "                Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "            return Intra_Clus_Dist/(len(X)-1)\n",
    "\n",
    "\n",
    "\n",
    "    # KVal = int(0.01*len(data1)) #5 #0.03 \n",
    "    percent5 = 0.01 # 5\n",
    "    LLE_lookup = SM['LLE_lookup']\n",
    "    algo = SM['algo']\n",
    "    data1 = SM['data1']\n",
    "    labels = SM['labels']\n",
    "    alpha_weight_avg = SM['alpha_weight_avg']\n",
    "    Store_Radius_Density = SM['Store_Radius_Density']\n",
    "    Tree_Structure = SM['Tree_Structure']\n",
    "    Store_Parents = SM['Store_Parents']\n",
    "    OutlierPostProcessing(LLE_lookup, algo, data1, labels, percent5, alpha_weight_avg, Store_Radius_Density, Tree_Structure)\n",
    "    \n",
    "    SM['Name: Outlier Post Processing'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: After Post Processing Display\n",
    "\n",
    "if 'Name: After Post Processing Display' not in SM:\n",
    "    \n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "\n",
    "    label_list = []\n",
    "    anomaly_list = []\n",
    "    max_val = float(\"-inf\")\n",
    "    for key in labels:\n",
    "        if max_val < labels[key]:\n",
    "            max_val = labels[key]\n",
    "        if labels[key] == -1:\n",
    "            anomaly_list.append(data1[key])\n",
    "        else:\n",
    "            label_list.append(labels[key])\n",
    "    cluster_num = max_val\n",
    "    #print(label_list)\n",
    "    print(\"\\n------------ After Outlier Post Processing ------------\")\n",
    "    print(\"Total number of datapoints =\", len(data1))\n",
    "    print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "    print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "    print(\"Number of Cluster(s) =\", cluster_num)\n",
    "\n",
    "    SM['data1'] = data1\n",
    "    SM['label_list'] = label_list\n",
    "    SM['anomaly_list'] = anomaly_list\n",
    "    SM['cluster_num'] = cluster_num\n",
    "    \n",
    "    SM['Name: After Post Processing Display'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Name: Display2\n",
    "\n",
    "if 'Name: Display2' not in SM:\n",
    "    \n",
    "    Dimension = SM['Dimension']\n",
    "    labels = SM['labels']\n",
    "    data1 = SM['data1']\n",
    "    cluster_num = SM['cluster_num']\n",
    "\n",
    "    if Dimension <= 2:\n",
    "        plt.figure(figsize=(8,8))\n",
    "\n",
    "        color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "        if cluster_num <= len(color):\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] != -1:\n",
    "                    plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "        else:\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "        plt.xlabel('X-Coordinates')\n",
    "        plt.ylabel('Y-Coordinates')\n",
    "        plt.title('Datapoints Visualization After Outlier Post Processing')\n",
    "        plt.show()\n",
    "        \n",
    "    SM['Name: Display2'] = True\n",
    "    \n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Clustering Results\n",
    "\n",
    "if 'Name: Clustering Results' not in SM:\n",
    "    \n",
    "    Clustering = SM['Clustering']\n",
    "    if Clustering:\n",
    "        # Performance Evaluation\n",
    "        # Ground_Truth_data_path=r\"E:\\ISI\\GROUND TRUTHS CLUSTERING\\a2_gt.csv\"\n",
    "        Ground_Truth_data_path = SM['Ground_Truth_data_path']\n",
    "        with open(Ground_Truth_data_path, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            data_f = list(reader)\n",
    "\n",
    "        # Clusters from Ground Truth Data\n",
    "        clustersFromGT = []\n",
    "        clustersFromGT.append(0)\n",
    "\n",
    "        count = 1\n",
    "        temp = []\n",
    "        temp.append(int(data_f[0][-1]))\n",
    "\n",
    "        for i in range(1, len(data_f)):\n",
    "            if int(data_f[i][-1]) != int(data_f[i-1][-1]) and int(data_f[i][-1]) not in temp:\n",
    "                temp.append(int(data_f[i][-1]))\n",
    "                clustersFromGT.append(i)\n",
    "                count += 1\n",
    "        clustersFromGT.append(i+1)\n",
    "\n",
    "\n",
    "        def EvaluateClustering(labels_PC, n_e_val_0, data_path, clustersFromGT, data1, labels_Idxs):\n",
    "            with open(data_path, 'r') as f:\n",
    "                reader = csv.reader(f, delimiter=',')\n",
    "                data = list(reader)\n",
    "                data = np.array(data).astype(float)\n",
    "\n",
    "            d=2\n",
    "            row=len(data)\n",
    "            dimension=len(data[0])\n",
    "            neighbors=4\n",
    "            W=np.zeros((row, row))\n",
    "\n",
    "            for i in range(row):\n",
    "                D_i=np.array(data-data[i, :])\n",
    "                distance=(D_i**2).sum(1)\n",
    "                nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "                D_nbrs=D_i[nearest_neighbor, :]\n",
    "                Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "                t=np.trace(Q)\n",
    "                r=0.001*t\n",
    "                if(neighbors>=dimension):\n",
    "                    Q=Q+(r*np.identity(neighbors))\n",
    "                w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "                w=w/sum(w)\n",
    "                W[i, nearest_neighbor]=w\n",
    "\n",
    "            I=np.identity(row)\n",
    "            M=I-W\n",
    "\n",
    "            U, S, Vt = np.linalg.svd(M)\n",
    "            e_val=S**2\n",
    "            e_val_0=np.array(e_val<10**-20)\n",
    "\n",
    "\n",
    "            uniq=np.array(np.unique(labels_PC))\n",
    "            confusion_mat=np.zeros((n_e_val_0,n_e_val_0))\n",
    "            status=np.zeros((len(uniq)))\n",
    "            # print(\"\\nSTATUS_INIT\",status,\"\\n\")                                                                    \n",
    "\n",
    "            x = 0\n",
    "            y = 1\n",
    "            f = 0                       \n",
    "\n",
    "            get_cluster_from_ground_truth = clustersFromGT\n",
    "\n",
    "\n",
    "            while y < len(get_cluster_from_ground_truth):\n",
    "                cm_k=np.zeros((n_e_val_0))\n",
    "                start = get_cluster_from_ground_truth[x]\n",
    "                end = get_cluster_from_ground_truth[y]\n",
    "                pred_labels=labels_PC[start:end]\n",
    "                f += 1\n",
    "                #print(f)\n",
    "                while True:\n",
    "                    maj_label=np.argmax(np.bincount(pred_labels))\n",
    "                    if(status[maj_label]!=1):\n",
    "                        status[maj_label]=1\n",
    "                        break\n",
    "                    pred_labels=pred_labels[pred_labels!=maj_label]\n",
    "                    if len(pred_labels) == 0:\n",
    "                        break\n",
    "                cl_k_k=np.count_nonzero(np.array(pred_labels==maj_label))  # True Positive\n",
    "                cm_k[maj_label]=cl_k_k  \n",
    "                for i in range((n_e_val_0)): # n_e_val_0 = no.of clusters in dataset ? Till now 'NO'  \n",
    "                    if (i != maj_label):\n",
    "                        cl=np.count_nonzero(np.array(pred_labels==uniq[i]))\n",
    "                        cm_k[i]=cl\n",
    "                confusion_mat[maj_label, :]=cm_k\n",
    "                x += 1\n",
    "                y += 1\n",
    "\n",
    "            # print(status,\"\\n\")\n",
    "\n",
    "            def precision(label, confusion_mat):\n",
    "                col=confusion_mat[:,label]\n",
    "                return confusion_mat[label, label]/col.sum()\n",
    "\n",
    "            def recall(label, confusion_mat):\n",
    "                row=confusion_mat[label,:]\n",
    "                return confusion_mat[label,label]/row.sum()\n",
    "\n",
    "            precision_arr=[]\n",
    "            recall_arr=[]\n",
    "            f_score=[]\n",
    "            g_mean=[]\n",
    "            for i in range(n_e_val_0):\n",
    "                p=precision(i, confusion_mat)\n",
    "                r=recall(i, confusion_mat)\n",
    "                f=2*p*r/(p+r)\n",
    "                g=np.sqrt(p*r)\n",
    "                precision_arr=np.append(precision_arr, p)\n",
    "                recall_arr=np.append(recall_arr, r)\n",
    "                f_score=np.append(f_score, f)\n",
    "                g_mean=np.append(g_mean, g)\n",
    "\n",
    "            f_score = f_score[np.logical_not(np.isnan(f_score))]\n",
    "            g_mean = g_mean[np.logical_not(np.isnan(g_mean))]\n",
    "\n",
    "            f_score=np.mean(f_score)\n",
    "            g_mean=np.mean(g_mean)\n",
    "            accuracy=(np.trace(confusion_mat)/len(data))*100\n",
    "\n",
    "            print(\"\\n***************************** PERFORMANCE MEASURE *****************************\")\n",
    "            print(\"Accuracy: \",accuracy,\"--------- f score:\",f_score,\"--------- g mean:\",g_mean)\n",
    "            DB = davies_bouldin_score(data1, labels_Idxs)\n",
    "            print(\"DB Index =\", DB)\n",
    "            distances = pairwise_distances(data1)\n",
    "            DN = dunn(distances, labels_Idxs)\n",
    "            print(\"Dunn Index =\", DN)\n",
    "            SiL = silhouette_score(data1, labels_Idxs)\n",
    "            print('Silhouette Score =', SiL)\n",
    "            CHar = metrics.calinski_harabasz_score(data1, labels_Idxs)\n",
    "            print('Calinski-Harabasz Index =', CHar)\n",
    "\n",
    "            SM['accuracy'] = accuracy\n",
    "            SM['f_score'] = f_score\n",
    "            SM['g_mean'] = g_mean\n",
    "            SM['DB'] = DB\n",
    "            SM['Dunn'] = DN \n",
    "            SM['SiL'] = SiL \n",
    "            SM['CHar'] = CHar \n",
    "\n",
    "\n",
    "        # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "        def GetLabelsForIdxs(labels):\n",
    "            get_labels_DB = []\n",
    "            for key in labels:\n",
    "                if labels[key] > 0:\n",
    "                    get_labels_DB.append(labels[key]-1)\n",
    "                else:\n",
    "                    get_labels_DB.append(labels[key])\n",
    "            return get_labels_DB\n",
    "\n",
    "        # Function to generate labels for getting the accurcay, f score and g mean\n",
    "        def GetLabelsForPC(labels):\n",
    "            get_labels_PC = []\n",
    "            for key in labels:\n",
    "                if labels[key] > 0:\n",
    "                    get_labels_PC.append(labels[key]-1)\n",
    "            return get_labels_PC\n",
    "\n",
    "\n",
    "        labels = SM['labels']\n",
    "        data1 = SM['data1']\n",
    "        cluster_num = SM['cluster_num']\n",
    "\n",
    "        get_labels_PC = GetLabelsForPC(labels)\n",
    "        labels_PC = np.array(get_labels_PC)\n",
    "        get_labels_DB = GetLabelsForIdxs(labels)\n",
    "        labels_Idxs = np.array(get_labels_DB)\n",
    "        EvaluateClustering(labels_PC, cluster_num, Ground_Truth_data_path, clustersFromGT, data1, labels_Idxs)\n",
    "    \n",
    "    SM['Name: Clustering Results'] = True\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Outlier Detection Results\n",
    "\n",
    "if '' not in SM:\n",
    "    \n",
    "    Outlier_Detection = SM['Outlier_Detection']\n",
    "    if Outlier_Detection:\n",
    "        def OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs):\n",
    "            # confusion matrix in sklearn\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            from sklearn.metrics import classification_report\n",
    "\n",
    "            # actual values\n",
    "            actual = actual_labels #[1,0,0,1,0,0,1,0,0,1]\n",
    "            # predicted values\n",
    "            predicted = pred_labels #[1,0,0,1,0,0,0,1,0,0]\n",
    "\n",
    "            # confusion matrix\n",
    "            matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "            #print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "            # outcome values order in sklearn\n",
    "            tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "            #print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "            # classification report for precision, recall f1-score and accuracy\n",
    "            matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "            print('Classification report : \\n',matrix)\n",
    "\n",
    "\n",
    "            print(\"\\n\")\n",
    "            DB = davies_bouldin_score(data1, labels_Idxs)\n",
    "            print(\"DB Index =\", DB)\n",
    "            distances = pairwise_distances(data1)\n",
    "            Dunn = dunn(distances, labels_Idxs)\n",
    "            print(\"Dunn Index =\", Dunn)\n",
    "            SiL = silhouette_score(data1, labels_Idxs)\n",
    "            print('Silhouette Score =', SiL)\n",
    "            CHar = metrics.calinski_harabasz_score(data1, labels_Idxs)\n",
    "            print('Calinski-Harabasz Index =', CHar)\n",
    "            SM['DB'] = DB\n",
    "            SM['Dunn'] = DN \n",
    "            SM['SiL'] = SiL \n",
    "            SM['CHar'] = CHar \n",
    "\n",
    "\n",
    "        def Actual_labels(Ground_Truth_data_path):\n",
    "            with open(Ground_Truth_data_path, 'r') as f:\n",
    "                actual_labels = np.genfromtxt(f, delimiter=',')\n",
    "            return actual_labels\n",
    "\n",
    "        def GetLabelsForOutlierDet(labels):\n",
    "            get_labels_OutlierDet = []\n",
    "            for key in labels:\n",
    "                if labels[key] != -1:\n",
    "                    get_labels_OutlierDet.append(0)\n",
    "                else:\n",
    "                    get_labels_OutlierDet.append(1)\n",
    "            return get_labels_OutlierDet\n",
    "\n",
    "        # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "        def GetLabelsForIdxs(labels):\n",
    "            get_labels_DB = []\n",
    "            for key in labels:\n",
    "                if labels[key] > 0:\n",
    "                    get_labels_DB.append(labels[key]-1)\n",
    "                else:\n",
    "                    get_labels_DB.append(labels[key])\n",
    "            return get_labels_DB\n",
    "\n",
    "\n",
    "        Ground_Truth_data_path = SM['Ground_Truth_data_path']\n",
    "        labels = SM['labels']\n",
    "        data1 = SM['data1']\n",
    "\n",
    "        actual_labels = Actual_labels(Ground_Truth_data_path)\n",
    "        pred_labels = GetLabelsForOutlierDet(labels)\n",
    "        get_labels_DB = GetLabelsForIdxs(labels)\n",
    "        labels_Idxs = np.array(get_labels_DB)\n",
    "        OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs)\n",
    "    \n",
    "    SM['Name: Outlier Detection Results'] = True\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"Skipped as Already Stored in Existing File!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list(SM.items())\n",
    "SM.clear()\n",
    "print(\"Current Status of Shelve Dictionaries: \", list(SM.items()))\n",
    "SM.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
